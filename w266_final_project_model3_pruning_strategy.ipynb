{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSYTybU1PKoU"
      },
      "outputs": [],
      "source": [
        "!pip install pydot --quiet\n",
        "!pip install tensorflow-datasets --quiet\n",
        "!pip install transformers --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CRy8dCCecWv",
        "outputId": "46475ddb-9ba9-47c7-e69b-71106ae22180"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi -L"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lcy6biPZeclf",
        "outputId": "752d768c-863b-401d-d21b-28e2305df3f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-039504dc-70da-dce7-8155-835b0eb72471)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Embedding, Input, Dense, Lambda, Dropout, Conv1D, GlobalMaxPooling1D, Concatenate, Activation\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow_datasets as tfds\n",
        "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel, GPT2Config\n",
        "import sklearn as sk\n",
        "import os\n",
        "from nltk.data import find\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "LGwRSY7Wec0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data = tfds.load(\n",
        "    name=\"imdb_reviews\",\n",
        "    split=('train[:80%]', 'test[80%:]'),\n",
        "    as_supervised=True)\n",
        "\n",
        "train_examples, train_labels = next(iter(train_data.batch(20000)))\n",
        "val_examples, val_labels = next(iter(train_data.batch(5000)))\n",
        "test_examples, test_labels = next(iter(test_data.batch(1000)))"
      ],
      "metadata": {
        "id": "TVn3Fda3ec2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "config = GPT2Config.from_pretrained(\"gpt2\", output_hidden_states=True)\n",
        "gpt2_model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\", config=config)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "MAX_SEQUENCE_LENGTH = 100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9T2hjMcgec48",
        "outputId": "f51d98e1-bf3a-41df-f375-c566c7d4afb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Embedding size of GPT2 tokenizer: 768\n",
        "#Dictionary size of GPT2 tokenizer: 50257\n",
        "\n",
        "\n",
        "train_examples_str = [x.decode('utf-8') for x in train_examples.numpy()]\n",
        "val_examples_str = [x.decode('utf-8') for x in val_examples.numpy()]\n",
        "test_examples_str = [x.decode('utf-8') for x in test_examples.numpy()]\n",
        "\n",
        "#training data\n",
        "gpt_train_tokenized = tokenizer(train_examples_str,\n",
        "              max_length=MAX_SEQUENCE_LENGTH,\n",
        "              truncation=True,\n",
        "              padding='max_length',\n",
        "              return_tensors='tf')\n",
        "\n",
        "gpt_train_inputs = {\n",
        "    'input_ids': gpt_train_tokenized['input_ids'],\n",
        "    'attention_mask': gpt_train_tokenized['attention_mask']\n",
        "}\n",
        "\n",
        "gpt_train_labels = np.array(train_labels)\n",
        "\n",
        "\n",
        "#validation data\n",
        "gpt_val_tokenized = tokenizer(val_examples_str,\n",
        "              max_length=MAX_SEQUENCE_LENGTH,\n",
        "              truncation=True,\n",
        "              padding='max_length',\n",
        "              return_tensors='tf')\n",
        "\n",
        "gpt_val_inputs = {\n",
        "    'input_ids': gpt_val_tokenized['input_ids'],\n",
        "    'attention_mask': gpt_val_tokenized['attention_mask']\n",
        "}\n",
        "\n",
        "gpt_val_labels = np.array(val_labels)\n",
        "\n",
        "#testing data\n",
        "gpt_test_tokenized = tokenizer(test_examples_str,\n",
        "              max_length=MAX_SEQUENCE_LENGTH,\n",
        "              truncation=True,\n",
        "              padding='max_length',\n",
        "              return_tensors='tf')\n",
        "\n",
        "gpt_test_inputs = {\n",
        "    'input_ids': gpt_test_tokenized['input_ids'],\n",
        "    'attention_mask': gpt_test_tokenized['attention_mask']\n",
        "}\n",
        "\n",
        "gpt_test_labels = np.array(test_labels)\n"
      ],
      "metadata": {
        "id": "TaXKoaHwec7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#24 layers of transformer\n",
        "#A drop out layer + dense layer with 100 hidden layer size on top + final layer with sigmoid as activation function\n",
        "\n",
        "\n",
        "def create_gpt_last_model(gpt_model,\n",
        "                          max_sequence_length=MAX_SEQUENCE_LENGTH,\n",
        "                          hidden_size = 100,\n",
        "                          dropout=0.3,\n",
        "                          learning_rate=0.00005):\n",
        "    \"\"\"\n",
        "    Build a simple classification model with gpt. Use the last token output for classification purposes.\n",
        "    \"\"\"\n",
        "\n",
        "    gpt_model.trainable = True #True\n",
        "\n",
        "    #input layers of gpt, shape (batch, max_sequence_length), model will be fit with gpt_train_inputs\n",
        "    input_ids = Input(shape=(max_sequence_length,), dtype=tf.int32, name='input_ids')\n",
        "    attention_mask = Input(shape=(max_sequence_length,), dtype=tf.int32, name='attention_mask')\n",
        "\n",
        "    # GPT-2 model\n",
        "    #model.generate() for iteratively generating (autoregressive)\n",
        "    #we only do it one time.\n",
        "    gpt2_outputs = gpt_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "    # Use the last hidden state of the last transformer layer for classification, ingore linear layer and softmax layer\n",
        "    # Select the last token of hidden state\n",
        "    last_hidden_state_last_token = gpt2_outputs.logits[:, -1, :]\n",
        "\n",
        "    #Add a dropout layer\n",
        "    x = Dropout(dropout)(last_hidden_state_last_token)\n",
        "\n",
        "    #Add a fully connected layer for classification\n",
        "    x = Dense(hidden_size, activation='relu')(x)\n",
        "\n",
        "    #Final output layer for classification, assuming it's binary task\n",
        "    output = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "\n",
        "    # Create the model\n",
        "    classification_model = Model(inputs=[input_ids, attention_mask], outputs=output)\n",
        "\n",
        "\n",
        "    #Model complie\n",
        "    classification_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                                 loss='binary_crossentropy',\n",
        "                                 metrics=['accuracy'])\n",
        "\n",
        "    return classification_model\n"
      ],
      "metadata": {
        "id": "o82-2fQrec9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()"
      ],
      "metadata": {
        "id": "jqM3TFB_1tqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt_logit_model_classification = create_gpt_last_model(gpt2_model)\n",
        "\n",
        "history = gpt_logit_model_classification.fit(gpt_train_inputs,\n",
        "                    gpt_train_labels,\n",
        "                    epochs=2, #2\n",
        "                    batch_size=8,\n",
        "                    validation_data=(gpt_val_inputs, gpt_val_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THmRPlvgenw-",
        "outputId": "a778a1e9-356a-480d-c77e-0d13da2a1bf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "2500/2500 [==============================] - 762s 294ms/step - loss: 2.0575 - accuracy: 0.6130 - val_loss: 0.4004 - val_accuracy: 0.8250\n",
            "Epoch 2/2\n",
            "2500/2500 [==============================] - 734s 294ms/step - loss: 0.4860 - accuracy: 0.7948 - val_loss: 0.3947 - val_accuracy: 0.8634\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Model before zeroing out\n",
        "gpt_logit_model_classification.evaluate(gpt_test_inputs, gpt_test_labels)"
      ],
      "metadata": {
        "id": "ynD38eQgfgUD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29b9f605-9923-46c4-e3f6-9460b3a09846"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 13s 334ms/step - loss: 0.4284 - accuracy: 0.8490\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.42840564250946045, 0.8489999771118164]"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "prediction = gpt_logit_model_classification.predict(gpt_test_inputs)\n",
        "end_time = time.time()\n",
        "\n",
        "elapsed_time = end_time - start_time\n",
        "print(\"Elapsed time: {:.2f} seconds\".format(elapsed_time))"
      ],
      "metadata": {
        "id": "lxU5NxX6fglK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "966eca95-bd52-4596-d23e-2988e7892423"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 14s 347ms/step\n",
            "Elapsed time: 23.38 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example test reviews\n",
        "\"\"\"\n",
        "1. Identifying Emotional Tone\n",
        "Sub-Task: Determine the emotional tone of the review (e.g., positive, negative, neutral).\n",
        "Test Reviews:\n",
        "    EX1 \"The movie's breathtaking scenery and exceptional soundtrack added depth to its rich storytelling.\" -> Positive Tone\n",
        "    EX2 \"The film was a letdown with its lackluster plot and uninspired performances.\" -> Negative Tone\n",
        "2. Analyzing Subjective Statements\n",
        "Sub-Task: Detect subjective statements or opinions in the review.\n",
        "Test Reviews:\n",
        "    EX3 \"In my opinion, the film's portrayal of historical events was highly inaccurate.\" -> Subjective\n",
        "    EX4 \"The movie won three Academy Awards, including Best Picture.\" -> Objective\n",
        "3. Evaluating Specific Aspects (Acting, Plot, Cinematography)\n",
        "Sub-Task: Assess specific aspects of the movie like acting quality, plot development, and cinematography.\n",
        "Test Reviews:\n",
        "    EX5 \"The acting was superb, with each character bringing depth and emotion to the screen.\" -> Positive Acting\n",
        "    EX6 \"The plot was predictable and lacked originality, making the movie quite boring.\" -> Negative Plot\n",
        "4. Recognizing Extremes in Sentiment\n",
        "Sub-Task: Identify reviews with extremely positive or negative sentiments.\n",
        "Test Reviews:\n",
        "    EX7 \"This is possibly the worst movie ever made, with no redeeming qualities whatsoever.\" -> Extremely Negative\n",
        "    EX8 \"An absolute masterpiece, every moment was captivating and a joy to watch.\" -> Extremely Positive\n",
        "5. Detecting Sarcasm or Irony\n",
        "Sub-Task: Detect sarcasm or irony, which can often invert the apparent sentiment of a statement.\n",
        "Test Reviews:\n",
        "    EX9 \"Oh great, another predictable rom-com, just what the world needs.\" -> Sarcasm\n",
        "    EX10 \"I loved how the movie ended abruptly without resolving any plot points.\" -> Irony\n",
        "\"\"\"\n",
        "\n",
        "# First reivews is positive tone and the second is negative tone\n",
        "test_reviews = [\n",
        "    \"In my opinion, the film's portrayal of historical events was highly inaccurate.\",\n",
        "    \"The movie won three Academy Awards, including Best Picture.\",\n",
        "    \"In my opinion, the film's portrayal of historical events was highly inaccurate.\",\n",
        "    \"The movie won three Academy Awards, including Best Picture.\",\n",
        "    \"The acting was superb, with each character bringing depth and emotion to the screen.\",\n",
        "    \"The plot was predictable and lacked originality, making the movie quite boring.\",\n",
        "    \"This is possibly the worst movie ever made, with no redeeming qualities whatsoever.\",\n",
        "    \"An absolute masterpiece, every moment was captivating and a joy to watch.\",\n",
        "    \"Oh great, another predictable rom-com, just what the world needs.\",\n",
        "    \"I loved how the movie ended abruptly without resolving any plot points.\"\n",
        "]"
      ],
      "metadata": {
        "id": "VJqyt1v_enzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_transformer_outputs(model, tokenizer, text, max_length=MAX_SEQUENCE_LENGTH):\n",
        "    # Tokenize the input text\n",
        "    inputs = tokenizer(text, return_tensors=\"tf\", max_length=max_length, padding=True, truncation=True)\n",
        "\n",
        "    # Get outputs from the model\n",
        "    outputs = model(**inputs, output_hidden_states=True)\n",
        "\n",
        "    # Extract the hidden states from each layer\n",
        "    hidden_states = outputs.hidden_states\n",
        "\n",
        "    return hidden_states\n",
        "\n",
        "\n",
        "# Example usage\n",
        "hidden_states = extract_transformer_outputs(gpt2_model, tokenizer, test_reviews)\n",
        "print(hidden_states)\n",
        "print(len(hidden_states))"
      ],
      "metadata": {
        "id": "9c4kATXAen7h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f88aba1-000b-42ff-9468-c3297643eee7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(<tf.Tensor: shape=(10, 16, 768), dtype=float32, numpy=\n",
            "array([[[-3.31244692e-02, -2.75187343e-01,  1.08748525e-01, ...,\n",
            "          1.99037895e-01, -3.70798334e-02, -1.05327368e-02],\n",
            "        [ 1.79998502e-01,  5.36328852e-02, -2.54170746e-02, ...,\n",
            "          8.41936842e-02,  1.15722775e-01,  2.74957810e-02],\n",
            "        [ 1.66623279e-01, -6.95171207e-03,  3.70554477e-02, ...,\n",
            "         -1.32766098e-01,  1.76915135e-02, -8.86920542e-02],\n",
            "        ...,\n",
            "        [-8.80430080e-03,  9.04437825e-02,  1.97137147e-01, ...,\n",
            "         -1.49239480e-01,  2.43238714e-02,  8.35666135e-02],\n",
            "        [ 4.75492217e-02,  8.79946630e-03,  1.23632759e-01, ...,\n",
            "         -7.13714734e-02,  4.89826016e-02,  9.65108499e-02],\n",
            "        [ 5.40797748e-02, -6.06381893e-03,  1.34249732e-01, ...,\n",
            "          7.08856620e-04,  1.50670081e-01,  1.15978584e-01]],\n",
            "\n",
            "       [[-8.45964774e-02, -2.17638597e-01,  7.07410499e-02, ...,\n",
            "          2.03571208e-02,  2.10232083e-02,  5.84937856e-02],\n",
            "        [ 4.72831130e-02, -9.31925997e-02,  1.25816941e-01, ...,\n",
            "          3.33622098e-04,  2.81056147e-02,  1.53020084e-01],\n",
            "        [-1.05175348e-02, -4.35663648e-02,  2.03982666e-01, ...,\n",
            "         -1.06670395e-01, -4.62482944e-02, -7.08451495e-02],\n",
            "        ...,\n",
            "        [ 5.55776618e-02, -6.26162812e-03,  1.53515086e-01, ...,\n",
            "          5.50115015e-04,  1.54995173e-01,  1.18648365e-01],\n",
            "        [ 5.29530011e-02, -6.69118762e-03,  1.44179910e-01, ...,\n",
            "          6.15083985e-03,  1.52012497e-01,  1.17389351e-01],\n",
            "        [ 5.40797748e-02, -6.06381893e-03,  1.34249732e-01, ...,\n",
            "          7.08856620e-04,  1.50670081e-01,  1.15978584e-01]],\n",
            "\n",
            "       [[-3.31244692e-02, -2.75187343e-01,  1.08748525e-01, ...,\n",
            "          1.99037895e-01, -3.70798334e-02, -1.05327368e-02],\n",
            "        [ 1.79998502e-01,  5.36328852e-02, -2.54170746e-02, ...,\n",
            "          8.41936842e-02,  1.15722775e-01,  2.74957810e-02],\n",
            "        [ 1.66623279e-01, -6.95171207e-03,  3.70554477e-02, ...,\n",
            "         -1.32766098e-01,  1.76915135e-02, -8.86920542e-02],\n",
            "        ...,\n",
            "        [-8.80430080e-03,  9.04437825e-02,  1.97137147e-01, ...,\n",
            "         -1.49239480e-01,  2.43238714e-02,  8.35666135e-02],\n",
            "        [ 4.75492217e-02,  8.79946630e-03,  1.23632759e-01, ...,\n",
            "         -7.13714734e-02,  4.89826016e-02,  9.65108499e-02],\n",
            "        [ 5.40797748e-02, -6.06381893e-03,  1.34249732e-01, ...,\n",
            "          7.08856620e-04,  1.50670081e-01,  1.15978584e-01]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[-5.12117893e-02, -2.88077950e-01,  9.14916098e-02, ...,\n",
            "         -1.04047962e-01,  1.30256996e-01, -7.52932206e-02],\n",
            "        [-1.04853623e-02, -2.05361471e-01, -7.68863484e-02, ...,\n",
            "          1.00317299e-01, -8.21984857e-02, -8.02216455e-02],\n",
            "        [-7.35573918e-02, -1.62000090e-01,  1.81950808e-01, ...,\n",
            "          9.63318162e-03,  1.01791672e-01,  7.52716511e-02],\n",
            "        ...,\n",
            "        [ 1.76225349e-01,  1.08130559e-01,  3.45140189e-01, ...,\n",
            "         -4.43092510e-02,  9.54540595e-02, -8.75783488e-02],\n",
            "        [ 4.75492217e-02,  8.79946630e-03,  1.23632759e-01, ...,\n",
            "         -7.13714734e-02,  4.89826016e-02,  9.65108499e-02],\n",
            "        [ 5.40797748e-02, -6.06381893e-03,  1.34249732e-01, ...,\n",
            "          7.08856620e-04,  1.50670081e-01,  1.15978584e-01]],\n",
            "\n",
            "       [[-7.61833563e-02, -1.98370636e-01,  1.36642039e-01, ...,\n",
            "          4.37679403e-02,  4.16190699e-02,  1.03194579e-01],\n",
            "        [-2.19568200e-02, -6.07463159e-02, -4.13032025e-02, ...,\n",
            "         -4.76484969e-02, -1.02011219e-01, -1.68657616e-01],\n",
            "        [ 1.58435274e-02, -9.34401527e-02,  9.03966874e-02, ...,\n",
            "          4.58517149e-02, -7.91732408e-03, -7.91082159e-02],\n",
            "        ...,\n",
            "        [-1.49190322e-01,  2.83963419e-02,  5.99570051e-02, ...,\n",
            "         -2.75349617e-01, -1.12442851e-01,  5.32631353e-02],\n",
            "        [ 4.75492217e-02,  8.79946630e-03,  1.23632759e-01, ...,\n",
            "         -7.13714734e-02,  4.89826016e-02,  9.65108499e-02],\n",
            "        [ 5.40797748e-02, -6.06381893e-03,  1.34249732e-01, ...,\n",
            "          7.08856620e-04,  1.50670081e-01,  1.15978584e-01]],\n",
            "\n",
            "       [[ 1.28250167e-01, -2.92469949e-01,  1.47395357e-01, ...,\n",
            "          5.94852902e-02, -3.23293433e-02, -5.96103072e-02],\n",
            "        [-1.13851339e-01, -3.00314218e-01, -1.45657569e-01, ...,\n",
            "         -1.62306696e-01,  1.85807999e-02, -1.25143886e-01],\n",
            "        [-2.50100344e-02, -1.14368349e-01,  8.92449319e-02, ...,\n",
            "          1.65267318e-01,  1.83230266e-01,  4.78969924e-02],\n",
            "        ...,\n",
            "        [ 5.55776618e-02, -6.26162812e-03,  1.53515086e-01, ...,\n",
            "          5.50115015e-04,  1.54995173e-01,  1.18648365e-01],\n",
            "        [ 5.29530011e-02, -6.69118762e-03,  1.44179910e-01, ...,\n",
            "          6.15083985e-03,  1.52012497e-01,  1.17389351e-01],\n",
            "        [ 5.40797748e-02, -6.06381893e-03,  1.34249732e-01, ...,\n",
            "          7.08856620e-04,  1.50670081e-01,  1.15978584e-01]]],\n",
            "      dtype=float32)>, <tf.Tensor: shape=(10, 16, 768), dtype=float32, numpy=\n",
            "array([[[ 1.12483656e+00, -1.62417924e+00,  7.62920976e-01, ...,\n",
            "         -2.10730124e+00, -3.03627968e-01,  3.73379558e-01],\n",
            "        [-1.07703435e+00, -5.26524305e-01, -2.98386574e-01, ...,\n",
            "          8.79189014e-01,  2.06185126e+00,  6.85546920e-02],\n",
            "        [ 1.55493185e-01, -9.44221020e-02, -2.02947378e+00, ...,\n",
            "         -2.06613111e+00, -1.53402877e+00, -7.30120778e-01],\n",
            "        ...,\n",
            "        [ 3.20052075e+00,  1.10132742e+00,  8.55507135e-01, ...,\n",
            "         -2.28929138e+00, -1.10025835e+00,  1.78876913e+00],\n",
            "        [ 1.10186422e+00,  2.71928370e-01, -9.16477144e-02, ...,\n",
            "         -6.35475993e-01, -5.71511507e-01,  4.88082767e-01],\n",
            "        [ 1.01805878e+00,  5.89449167e-01,  8.07747245e-01, ...,\n",
            "          2.19483709e+00,  3.11760187e-01,  7.86484599e-01]],\n",
            "\n",
            "       [[ 2.92615831e-01, -1.19749153e+00,  2.92439729e-01, ...,\n",
            "         -2.06114745e+00, -2.79548287e-01,  1.44075230e-01],\n",
            "        [-6.00889325e-02, -4.58442271e-02, -9.49371219e-01, ...,\n",
            "          5.88199437e-01,  2.68194818e+00, -3.00626397e-01],\n",
            "        [ 9.66626644e-01, -1.75391436e+00,  1.19799256e+00, ...,\n",
            "         -3.85272413e-01,  6.04724109e-01, -1.00857735e+00],\n",
            "        ...,\n",
            "        [ 7.64915347e-01,  5.15133381e-01,  7.25472212e-01, ...,\n",
            "          1.99150872e+00,  4.93311465e-01,  9.45846558e-01],\n",
            "        [ 7.99360871e-01,  5.15510023e-01,  7.22831368e-01, ...,\n",
            "          1.95483756e+00,  4.70739424e-01,  8.92576277e-01],\n",
            "        [ 8.36013496e-01,  5.04765570e-01,  7.44405389e-01, ...,\n",
            "          1.93804014e+00,  4.61187690e-01,  8.56656432e-01]],\n",
            "\n",
            "       [[ 1.12483656e+00, -1.62417924e+00,  7.62920976e-01, ...,\n",
            "         -2.10730124e+00, -3.03627968e-01,  3.73379558e-01],\n",
            "        [-1.07703435e+00, -5.26524305e-01, -2.98386574e-01, ...,\n",
            "          8.79189014e-01,  2.06185126e+00,  6.85546920e-02],\n",
            "        [ 1.55493185e-01, -9.44221020e-02, -2.02947378e+00, ...,\n",
            "         -2.06613111e+00, -1.53402877e+00, -7.30120778e-01],\n",
            "        ...,\n",
            "        [ 3.20052075e+00,  1.10132742e+00,  8.55507135e-01, ...,\n",
            "         -2.28929138e+00, -1.10025835e+00,  1.78876913e+00],\n",
            "        [ 1.10186422e+00,  2.71928370e-01, -9.16477144e-02, ...,\n",
            "         -6.35475993e-01, -5.71511507e-01,  4.88082767e-01],\n",
            "        [ 1.01805878e+00,  5.89449167e-01,  8.07747245e-01, ...,\n",
            "          2.19483709e+00,  3.11760187e-01,  7.86484599e-01]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[ 7.91066289e-01, -2.36591101e+00, -4.41972911e-02, ...,\n",
            "         -2.21860600e+00, -1.36581063e+00, -2.16307473e+00],\n",
            "        [-2.37108874e+00, -3.06009054e+00, -2.04760242e+00, ...,\n",
            "          8.25313449e-01, -6.25851810e-01,  4.27061200e-01],\n",
            "        [ 4.39974695e-01,  6.62369728e-01,  8.20842683e-01, ...,\n",
            "          5.99379361e-01,  2.03152633e+00,  4.83598828e-01],\n",
            "        ...,\n",
            "        [ 1.71249270e+00,  3.93624020e+00, -1.74022150e+00, ...,\n",
            "         -1.70538798e-02,  2.11553025e+00, -2.16712698e-01],\n",
            "        [ 1.52859807e+00,  3.50084782e-01, -1.60813212e-01, ...,\n",
            "         -5.68621516e-01, -7.70007968e-01,  1.54160827e-01],\n",
            "        [ 1.03864431e+00,  5.11691689e-01,  6.54167891e-01, ...,\n",
            "          2.33145905e+00,  2.50971884e-01,  4.79803741e-01]],\n",
            "\n",
            "       [[ 1.77077711e-01, -1.33195937e+00,  2.09827006e-01, ...,\n",
            "         -1.98035157e+00, -6.99696600e-01,  8.28207284e-03],\n",
            "        [-1.08232367e+00, -2.78927541e+00, -1.84109449e+00, ...,\n",
            "          4.54071820e-01, -5.03872097e-01, -7.33545423e-01],\n",
            "        [-1.60944521e-01, -6.90903544e-01, -5.62823892e-01, ...,\n",
            "          9.44630384e-01,  4.45293933e-02,  4.59959269e-01],\n",
            "        ...,\n",
            "        [-3.88962030e-01, -9.20539320e-01,  1.11881483e+00, ...,\n",
            "         -2.11424971e+00,  8.46332908e-01,  1.70398235e+00],\n",
            "        [ 3.97969961e-01, -4.43007052e-02, -1.85430795e-03, ...,\n",
            "         -4.76972997e-01, -4.71869409e-01,  5.93195558e-01],\n",
            "        [ 4.16547716e-01,  3.45990211e-01,  8.32480788e-01, ...,\n",
            "          2.02234077e+00,  3.09715986e-01,  5.96184731e-01]],\n",
            "\n",
            "       [[ 8.43806714e-02, -9.99227524e-01,  4.39836442e-01, ...,\n",
            "         -1.49865949e+00,  2.16516227e-01,  6.70298874e-01],\n",
            "        [-8.55261207e-01, -1.22604108e+00, -4.27338094e-01, ...,\n",
            "          1.19691506e-01,  5.61460733e-01,  2.73142785e-01],\n",
            "        [ 6.24581099e-01,  9.04139578e-02, -1.58307171e+00, ...,\n",
            "         -7.89848804e-01,  3.36408675e-01,  4.47631240e-01],\n",
            "        ...,\n",
            "        [ 9.22896028e-01,  4.22909617e-01,  3.87399942e-01, ...,\n",
            "          2.04462123e+00,  2.87224978e-01,  5.80761254e-01],\n",
            "        [ 9.83245373e-01,  4.41580504e-01,  3.84551585e-01, ...,\n",
            "          1.99580002e+00,  2.58533061e-01,  5.39146602e-01],\n",
            "        [ 1.02712917e+00,  4.41168427e-01,  3.90794277e-01, ...,\n",
            "          1.96083581e+00,  2.58605689e-01,  5.11837006e-01]]],\n",
            "      dtype=float32)>, <tf.Tensor: shape=(10, 16, 768), dtype=float32, numpy=\n",
            "array([[[ 0.7318206 , -1.482572  ,  1.6358979 , ..., -2.1294734 ,\n",
            "         -0.03792974, -0.25375763],\n",
            "        [-0.6280757 ,  0.01140586,  0.39958054, ...,  0.9675616 ,\n",
            "          2.6257927 , -0.03838331],\n",
            "        [ 0.8914701 , -0.2558442 , -2.195327  , ..., -2.0980015 ,\n",
            "         -1.9002808 , -1.7838688 ],\n",
            "        ...,\n",
            "        [ 5.520056  ,  1.1809033 ,  0.8351027 , ..., -2.3458152 ,\n",
            "         -1.3671815 ,  2.600769  ],\n",
            "        [ 1.0906597 ,  0.05386651, -0.7277812 , ..., -1.0199536 ,\n",
            "         -0.01321131,  0.9820426 ],\n",
            "        [ 0.4905256 ,  0.7247497 ,  1.3373966 , ...,  2.6597564 ,\n",
            "          1.9238117 , -0.24866629]],\n",
            "\n",
            "       [[-0.39443204, -1.6424708 ,  1.428513  , ..., -1.4604715 ,\n",
            "         -0.3256194 , -0.5199361 ],\n",
            "        [-0.6844889 , -0.34775132, -1.342756  , ...,  0.7109426 ,\n",
            "          3.5432518 , -0.6820302 ],\n",
            "        [ 1.2018949 , -1.4973783 ,  1.1990703 , ...,  0.31626305,\n",
            "          0.0180003 , -1.2621038 ],\n",
            "        ...,\n",
            "        [-0.5324538 ,  1.2848096 ,  1.5998353 , ...,  2.6468952 ,\n",
            "          1.864167  ,  0.08975148],\n",
            "        [-0.46714374,  1.3074652 ,  1.5797282 , ...,  2.613049  ,\n",
            "          1.8629277 ,  0.03062892],\n",
            "        [-0.4187694 ,  1.3062503 ,  1.5729575 , ...,  2.5965204 ,\n",
            "          1.8737891 ,  0.02667093]],\n",
            "\n",
            "       [[ 0.7318206 , -1.482572  ,  1.6358979 , ..., -2.1294734 ,\n",
            "         -0.03792974, -0.25375763],\n",
            "        [-0.6280757 ,  0.01140586,  0.39958054, ...,  0.9675616 ,\n",
            "          2.6257927 , -0.03838331],\n",
            "        [ 0.8914701 , -0.2558442 , -2.195327  , ..., -2.0980015 ,\n",
            "         -1.9002808 , -1.7838688 ],\n",
            "        ...,\n",
            "        [ 5.520056  ,  1.1809033 ,  0.8351027 , ..., -2.3458152 ,\n",
            "         -1.3671815 ,  2.600769  ],\n",
            "        [ 1.0906597 ,  0.05386651, -0.7277812 , ..., -1.0199536 ,\n",
            "         -0.01321131,  0.9820426 ],\n",
            "        [ 0.4905256 ,  0.7247497 ,  1.3373966 , ...,  2.6597564 ,\n",
            "          1.9238117 , -0.24866629]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[ 0.37321955, -2.9103043 ,  0.69112134, ..., -1.2344602 ,\n",
            "         -0.6634262 , -2.3855274 ],\n",
            "        [-1.2658806 , -2.3336682 , -2.3566184 , ...,  1.4681739 ,\n",
            "         -0.41228402,  0.3639303 ],\n",
            "        [ 0.76244134,  1.1523829 ,  0.39035094, ...,  1.321846  ,\n",
            "          2.514193  ,  0.88993615],\n",
            "        ...,\n",
            "        [ 2.0633445 ,  5.230774  , -2.947007  , ...,  0.0844433 ,\n",
            "          3.04013   ,  0.9521025 ],\n",
            "        [ 1.0620847 ,  0.09958184, -0.70715845, ..., -0.65039045,\n",
            "          0.0123136 ,  0.7358959 ],\n",
            "        [ 0.01677668,  0.50501925,  1.2960024 , ...,  2.5325298 ,\n",
            "          1.6143858 , -1.0836672 ]],\n",
            "\n",
            "       [[-0.28755033, -1.6977253 ,  1.3472316 , ..., -1.7316943 ,\n",
            "         -0.22063929, -0.9998107 ],\n",
            "        [-0.59453475, -2.618242  , -2.7502677 , ...,  0.4875992 ,\n",
            "         -0.09603201, -1.2033727 ],\n",
            "        [-0.87092423, -0.818873  , -0.80804753, ...,  1.2423136 ,\n",
            "          0.917293  ,  0.30617383],\n",
            "        ...,\n",
            "        [-1.5009327 , -0.962685  ,  0.6504488 , ..., -2.5358145 ,\n",
            "          1.6824256 ,  2.7292178 ],\n",
            "        [ 0.23901314, -0.39350525, -0.7526095 , ..., -0.64342415,\n",
            "          0.5214256 ,  0.91119194],\n",
            "        [-0.35806948,  0.5667976 ,  0.79942775, ...,  2.4127986 ,\n",
            "          1.4903808 , -0.43515807]],\n",
            "\n",
            "       [[-0.46130466, -1.9356225 ,  1.8173482 , ..., -1.0117975 ,\n",
            "          0.17292881, -0.04096377],\n",
            "        [-0.42413476, -1.259699  , -0.58285856, ...,  0.3082703 ,\n",
            "          0.77468157,  0.5670409 ],\n",
            "        [-0.3717286 , -0.04868673, -2.1129863 , ..., -1.4567914 ,\n",
            "          0.48529947,  1.0547452 ],\n",
            "        ...,\n",
            "        [ 0.5138856 ,  0.7821135 ,  1.1172583 , ...,  2.5737972 ,\n",
            "          1.5330336 , -0.7811112 ],\n",
            "        [ 0.60180855,  0.7882966 ,  1.0924008 , ...,  2.5403955 ,\n",
            "          1.5128663 , -0.8255679 ],\n",
            "        [ 0.6688851 ,  0.7887355 ,  1.0794537 , ...,  2.5169663 ,\n",
            "          1.4936376 , -0.82758737]]], dtype=float32)>, <tf.Tensor: shape=(10, 16, 768), dtype=float32, numpy=\n",
            "array([[[ 2.9181629e-01, -2.1560545e+00,  1.5063007e+00, ...,\n",
            "         -2.3487947e+00,  3.6543120e-02, -6.6601920e-01],\n",
            "        [-2.0196301e-01,  3.6922443e-01,  1.1016635e+00, ...,\n",
            "          9.4653416e-01,  2.3969100e+00, -5.7311547e-01],\n",
            "        [ 6.5639919e-01, -8.5310507e-01, -1.6224427e+00, ...,\n",
            "         -2.7732859e+00, -2.8887255e+00, -1.7204592e+00],\n",
            "        ...,\n",
            "        [ 5.2596903e+00,  6.2232029e-01,  1.4523438e+00, ...,\n",
            "         -2.5674942e+00, -1.9628985e+00,  2.4881282e+00],\n",
            "        [ 7.6530594e-01, -1.4111984e-01, -6.8075562e-01, ...,\n",
            "         -1.0986269e+00, -1.1783241e-01,  9.4250101e-01],\n",
            "        [ 2.2948536e-01,  6.1695457e-02,  1.0757401e+00, ...,\n",
            "          2.0354071e+00,  1.0718184e+00, -1.1389475e+00]],\n",
            "\n",
            "       [[-6.2138963e-01, -2.2794120e+00,  1.4772369e+00, ...,\n",
            "         -1.5715593e+00, -1.6881855e-01, -8.2675374e-01],\n",
            "        [-1.8917322e-03,  4.4931555e-01, -9.9505913e-01, ...,\n",
            "          5.8007848e-01,  3.9673476e+00, -1.4347347e+00],\n",
            "        [ 1.1685703e+00, -3.3903229e-01,  2.1073849e+00, ...,\n",
            "         -5.5638540e-01,  4.2918140e-01, -1.2639847e+00],\n",
            "        ...,\n",
            "        [-5.6377047e-01, -1.6912341e-02,  1.6433990e+00, ...,\n",
            "          2.8102305e+00,  1.6956103e+00, -4.8560780e-01],\n",
            "        [-5.0994188e-01,  2.1627903e-02,  1.6131576e+00, ...,\n",
            "          2.7903183e+00,  1.6999397e+00, -5.1869488e-01],\n",
            "        [-4.8475754e-01,  3.0534387e-02,  1.6106904e+00, ...,\n",
            "          2.8052263e+00,  1.7224243e+00, -5.1015663e-01]],\n",
            "\n",
            "       [[ 2.9181629e-01, -2.1560545e+00,  1.5063007e+00, ...,\n",
            "         -2.3487947e+00,  3.6543120e-02, -6.6601920e-01],\n",
            "        [-2.0196301e-01,  3.6922443e-01,  1.1016635e+00, ...,\n",
            "          9.4653416e-01,  2.3969100e+00, -5.7311547e-01],\n",
            "        [ 6.5639919e-01, -8.5310507e-01, -1.6224427e+00, ...,\n",
            "         -2.7732859e+00, -2.8887255e+00, -1.7204592e+00],\n",
            "        ...,\n",
            "        [ 5.2596903e+00,  6.2232029e-01,  1.4523438e+00, ...,\n",
            "         -2.5674942e+00, -1.9628985e+00,  2.4881282e+00],\n",
            "        [ 7.6530594e-01, -1.4111984e-01, -6.8075562e-01, ...,\n",
            "         -1.0986269e+00, -1.1783241e-01,  9.4250101e-01],\n",
            "        [ 2.2948536e-01,  6.1695457e-02,  1.0757401e+00, ...,\n",
            "          2.0354071e+00,  1.0718184e+00, -1.1389475e+00]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[-6.1287880e-03, -3.4133296e+00,  7.2562999e-01, ...,\n",
            "         -1.4585305e+00, -5.2427447e-01, -2.5538380e+00],\n",
            "        [-1.3887999e+00, -2.9921603e+00, -2.5544209e+00, ...,\n",
            "         -5.0855851e-01, -6.9498098e-01,  1.3829004e+00],\n",
            "        [ 4.8645556e-01,  1.9669335e+00, -1.9297764e-01, ...,\n",
            "          1.3617268e+00,  6.2510026e-01,  6.4225549e-01],\n",
            "        ...,\n",
            "        [ 1.9833205e+00,  6.2961164e+00, -3.9625721e+00, ...,\n",
            "         -1.2375054e-01,  2.4117284e+00, -5.8623552e-03],\n",
            "        [ 1.9222569e-01, -4.3391913e-02, -6.1287719e-01, ...,\n",
            "         -5.5425507e-01, -2.7376527e-01,  6.5601599e-01],\n",
            "        [-5.8031642e-01,  3.6087039e-01,  1.0819513e+00, ...,\n",
            "          2.4874606e+00,  1.0492706e+00, -1.6855813e+00]],\n",
            "\n",
            "       [[-7.5872278e-01, -2.3416009e+00,  1.5738316e+00, ...,\n",
            "         -1.7680333e+00, -1.7744992e-02, -1.3146392e+00],\n",
            "        [ 1.4576694e-01, -2.7967627e+00, -1.7807281e+00, ...,\n",
            "          7.8766352e-01, -6.9097722e-01, -1.8385611e+00],\n",
            "        [-1.5900537e+00, -5.7964247e-01,  2.8182971e-01, ...,\n",
            "          9.7521210e-01,  9.6178705e-01,  1.5332024e-01],\n",
            "        ...,\n",
            "        [-2.5298636e+00, -1.6649764e+00,  5.9890372e-01, ...,\n",
            "         -2.5689278e+00,  2.2433007e+00,  2.6792502e+00],\n",
            "        [-2.3882046e-01, -7.7009511e-01, -5.5879188e-01, ...,\n",
            "         -2.3980647e-01,  6.5287602e-01,  7.7685869e-01],\n",
            "        [-8.2795149e-01,  2.4360755e-01,  9.0552086e-01, ...,\n",
            "          2.3182719e+00,  9.3020481e-01, -1.1094940e+00]],\n",
            "\n",
            "       [[-6.8600208e-01, -2.6976020e+00,  1.8833282e+00, ...,\n",
            "         -1.1892468e+00,  3.2584441e-01, -4.7182909e-01],\n",
            "        [-7.9697394e-01, -5.5542940e-01, -1.2796245e+00, ...,\n",
            "         -5.7303780e-01,  3.3803701e-02,  1.1399926e+00],\n",
            "        [-7.5696260e-03,  1.8076459e-01, -2.1982589e+00, ...,\n",
            "         -1.5194951e+00,  5.1430142e-01,  8.9584732e-01],\n",
            "        ...,\n",
            "        [-2.8380936e-01,  1.1662048e-01,  7.0530981e-01, ...,\n",
            "          2.4428389e+00,  1.2857429e+00, -1.4357321e+00],\n",
            "        [-2.3861521e-01,  1.3675946e-01,  6.8508524e-01, ...,\n",
            "          2.4450955e+00,  1.2579600e+00, -1.4391745e+00],\n",
            "        [-2.0863938e-01,  1.3477644e-01,  6.8296790e-01, ...,\n",
            "          2.4769893e+00,  1.2312148e+00, -1.4164882e+00]]], dtype=float32)>, <tf.Tensor: shape=(10, 16, 768), dtype=float32, numpy=\n",
            "array([[[ 0.4505155 , -2.1593788 ,  1.7453074 , ..., -2.2455192 ,\n",
            "          0.11528455, -0.82481146],\n",
            "        [-0.9317304 , -0.19827461,  1.4077438 , ...,  0.60115784,\n",
            "          3.2981136 , -0.28406593],\n",
            "        [-0.50275195, -1.8788259 , -2.1300426 , ..., -1.0787238 ,\n",
            "         -3.7237902 , -2.5572853 ],\n",
            "        ...,\n",
            "        [ 4.846286  ,  1.2066461 ,  1.1226794 , ..., -2.634227  ,\n",
            "         -2.238427  ,  3.4166484 ],\n",
            "        [ 1.5032451 , -0.4626664 , -1.5466509 , ..., -0.8902969 ,\n",
            "         -0.39283767,  0.9454706 ],\n",
            "        [ 0.4321381 , -0.6098546 ,  1.240835  , ...,  2.0933044 ,\n",
            "          0.99716705, -0.84391886]],\n",
            "\n",
            "       [[-0.4885279 , -2.2635741 ,  1.7190493 , ..., -1.4601161 ,\n",
            "         -0.09728993, -0.9803438 ],\n",
            "        [ 0.06124112,  0.70248103, -0.83624625, ...,  0.5838051 ,\n",
            "          3.207512  , -0.8269866 ],\n",
            "        [ 0.86003155,  0.3805493 ,  2.1712945 , ...,  0.05551064,\n",
            "         -0.4804197 , -2.011247  ],\n",
            "        ...,\n",
            "        [-0.13582355, -0.7756379 ,  1.6924174 , ...,  2.773095  ,\n",
            "          1.1068618 , -0.19683164],\n",
            "        [-0.11451545, -0.7440829 ,  1.67093   , ...,  2.742877  ,\n",
            "          1.1329491 , -0.24177364],\n",
            "        [-0.1140736 , -0.7362382 ,  1.6680118 , ...,  2.7570329 ,\n",
            "          1.1599952 , -0.24297497]],\n",
            "\n",
            "       [[ 0.4505155 , -2.1593788 ,  1.7453074 , ..., -2.2455192 ,\n",
            "          0.11528455, -0.82481146],\n",
            "        [-0.9317304 , -0.19827461,  1.4077438 , ...,  0.60115784,\n",
            "          3.2981136 , -0.28406593],\n",
            "        [-0.50275195, -1.8788259 , -2.1300426 , ..., -1.0787238 ,\n",
            "         -3.7237902 , -2.5572853 ],\n",
            "        ...,\n",
            "        [ 4.846286  ,  1.2066461 ,  1.1226794 , ..., -2.634227  ,\n",
            "         -2.238427  ,  3.4166484 ],\n",
            "        [ 1.5032451 , -0.4626664 , -1.5466509 , ..., -0.8902969 ,\n",
            "         -0.39283767,  0.9454706 ],\n",
            "        [ 0.4321381 , -0.6098546 ,  1.240835  , ...,  2.0933044 ,\n",
            "          0.99716705, -0.84391886]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[ 0.15824357, -3.3974886 ,  0.9748409 , ..., -1.3675418 ,\n",
            "         -0.41911772, -2.6921964 ],\n",
            "        [-2.2825298 , -2.5545    , -1.8743258 , ..., -0.17694426,\n",
            "         -1.6035423 ,  0.7677733 ],\n",
            "        [ 0.2378637 ,  2.203817  ,  0.39736813, ...,  0.7197873 ,\n",
            "          1.6380475 , -0.36641145],\n",
            "        ...,\n",
            "        [ 1.7385564 ,  6.7690415 , -4.973229  , ...,  0.36047682,\n",
            "          1.4012567 ,  0.41298628],\n",
            "        [ 0.6094721 , -0.91803026, -0.77971697, ..., -0.58591306,\n",
            "         -0.5925817 ,  0.9285107 ],\n",
            "        [-0.13634592, -0.75836706,  1.5972857 , ...,  2.2594135 ,\n",
            "          0.59725356, -1.2350945 ]],\n",
            "\n",
            "       [[-0.62871593, -2.317739  ,  1.8412106 , ..., -1.6557925 ,\n",
            "          0.05506579, -1.485039  ],\n",
            "        [-0.51145756, -2.3908749 , -1.6682656 , ...,  1.333826  ,\n",
            "         -1.3234    , -1.9503694 ],\n",
            "        [-1.7213928 , -0.22639903,  0.2128727 , ...,  0.5275122 ,\n",
            "          0.5306163 ,  0.87741256],\n",
            "        ...,\n",
            "        [-2.152576  , -2.089201  ,  1.1182101 , ..., -2.423297  ,\n",
            "          2.171433  ,  3.7774668 ],\n",
            "        [-0.27871448, -0.73940635, -0.721092  , ..., -0.65241635,\n",
            "          0.5757784 ,  1.2512708 ],\n",
            "        [-0.45579818, -0.37444344,  1.1234455 , ...,  1.8221037 ,\n",
            "          0.95137703, -1.0088483 ]],\n",
            "\n",
            "       [[-0.53556424, -2.6661942 ,  2.1247704 , ..., -1.0557935 ,\n",
            "          0.39683878, -0.6400582 ],\n",
            "        [-0.45317248,  0.2429992 , -1.8371555 , ..., -1.2935495 ,\n",
            "         -0.6969243 ,  0.09769058],\n",
            "        [ 0.30414486,  0.01136512, -2.8221657 , ..., -1.04014   ,\n",
            "         -0.11937416,  0.40264535],\n",
            "        ...,\n",
            "        [ 0.07783906, -1.0864335 ,  0.94536036, ...,  1.8312411 ,\n",
            "          1.0328684 , -1.2860153 ],\n",
            "        [ 0.09045474, -1.0592945 ,  0.94625455, ...,  1.8138554 ,\n",
            "          1.0146276 , -1.3155217 ],\n",
            "        [ 0.0956361 , -1.0537072 ,  0.9558594 , ...,  1.8223025 ,\n",
            "          0.99434257, -1.3148273 ]]], dtype=float32)>, <tf.Tensor: shape=(10, 16, 768), dtype=float32, numpy=\n",
            "array([[[ 3.48161340e-01, -2.27241421e+00,  1.76268041e+00, ...,\n",
            "         -2.59062719e+00, -3.14949453e-03, -8.40936303e-01],\n",
            "        [-3.81774902e-02,  4.53270674e-01,  1.40229499e+00, ...,\n",
            "          5.78879714e-02,  2.51212621e+00,  1.24674648e-01],\n",
            "        [ 7.12225497e-01, -9.88423526e-01, -9.15026784e-01, ...,\n",
            "         -1.07527781e+00, -3.95282984e+00, -2.71395397e+00],\n",
            "        ...,\n",
            "        [ 5.98036957e+00,  2.81443405e+00,  2.85885572e+00, ...,\n",
            "         -2.40783763e+00, -1.70311701e+00,  3.88370562e+00],\n",
            "        [ 1.30593038e+00,  2.51061440e-01, -1.73436582e+00, ...,\n",
            "         -1.34254456e+00, -1.21318793e+00,  2.85972416e-01],\n",
            "        [-5.30979037e-01, -4.49271560e-01,  1.31000924e+00, ...,\n",
            "          1.87209749e+00,  1.19028699e+00, -1.36394000e+00]],\n",
            "\n",
            "       [[-5.80290020e-01, -2.40023017e+00,  1.71612763e+00, ...,\n",
            "         -1.82167602e+00, -2.43554622e-01, -9.80196774e-01],\n",
            "        [-2.41405576e-01,  1.48677516e+00, -9.67798293e-01, ...,\n",
            "          5.82034826e-01,  2.71345639e+00, -2.32794940e-01],\n",
            "        [ 8.16439986e-01,  1.18700624e+00,  2.49977398e+00, ...,\n",
            "          3.31424981e-01, -9.05167937e-01, -2.72206712e+00],\n",
            "        ...,\n",
            "        [-1.60340452e+00, -1.74760044e-01,  2.19404387e+00, ...,\n",
            "          2.34651542e+00,  1.06989241e+00, -3.74595106e-01],\n",
            "        [-1.60428727e+00, -1.58417106e-01,  2.18611050e+00, ...,\n",
            "          2.33175659e+00,  1.10561383e+00, -4.30702388e-01],\n",
            "        [-1.60791552e+00, -1.77806139e-01,  2.19480014e+00, ...,\n",
            "          2.35592699e+00,  1.14228451e+00, -4.44720685e-01]],\n",
            "\n",
            "       [[ 3.48161340e-01, -2.27241421e+00,  1.76268041e+00, ...,\n",
            "         -2.59062719e+00, -3.14949453e-03, -8.40936303e-01],\n",
            "        [-3.81774902e-02,  4.53270674e-01,  1.40229499e+00, ...,\n",
            "          5.78879714e-02,  2.51212621e+00,  1.24674648e-01],\n",
            "        [ 7.12225497e-01, -9.88423526e-01, -9.15026784e-01, ...,\n",
            "         -1.07527781e+00, -3.95282984e+00, -2.71395397e+00],\n",
            "        ...,\n",
            "        [ 5.98036957e+00,  2.81443405e+00,  2.85885572e+00, ...,\n",
            "         -2.40783763e+00, -1.70311701e+00,  3.88370562e+00],\n",
            "        [ 1.30593038e+00,  2.51061440e-01, -1.73436582e+00, ...,\n",
            "         -1.34254456e+00, -1.21318793e+00,  2.85972416e-01],\n",
            "        [-5.30979037e-01, -4.49271560e-01,  1.31000924e+00, ...,\n",
            "          1.87209749e+00,  1.19028699e+00, -1.36394000e+00]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[ 6.80595413e-02, -3.54983711e+00,  1.00737238e+00, ...,\n",
            "         -1.71640348e+00, -5.54104269e-01, -2.70144582e+00],\n",
            "        [-1.77854490e+00, -1.43408501e+00, -1.95269060e+00, ...,\n",
            "         -1.69215739e+00, -1.82418203e+00,  8.37772965e-01],\n",
            "        [ 7.51117587e-01,  1.68301105e+00, -1.46419317e-01, ...,\n",
            "          1.54758775e+00,  3.03645110e+00, -1.27139783e+00],\n",
            "        ...,\n",
            "        [ 1.29874587e+00,  7.23738098e+00, -3.13232470e+00, ...,\n",
            "          1.26284266e+00,  4.55494672e-01,  1.34221017e+00],\n",
            "        [ 1.49354088e+00,  1.71425283e-01,  2.99490064e-01, ...,\n",
            "         -1.13812768e+00, -1.07053673e+00, -1.84627533e-01],\n",
            "        [-7.02644408e-01, -4.94147778e-01,  1.63262486e+00, ...,\n",
            "          1.18885493e+00,  7.53930092e-01, -1.37528563e+00]],\n",
            "\n",
            "       [[-7.35396445e-01, -2.46561074e+00,  1.87498903e+00, ...,\n",
            "         -2.02491093e+00, -1.09955601e-01, -1.49004781e+00],\n",
            "        [ 1.01380348e-02, -1.96106124e+00, -1.82088125e+00, ...,\n",
            "          7.52662361e-01, -1.02363825e-01, -7.52269149e-01],\n",
            "        [-1.63227201e+00,  4.10710096e-01,  1.35838509e+00, ...,\n",
            "         -1.80351436e+00, -1.14026189e+00,  1.41210675e-01],\n",
            "        ...,\n",
            "        [-1.68183708e+00, -2.73073936e+00,  1.43124914e+00, ...,\n",
            "         -2.13169789e+00,  2.50418997e+00,  3.94026494e+00],\n",
            "        [-2.39211500e-01,  2.13601470e-01, -9.24020290e-01, ...,\n",
            "         -6.27322197e-01,  5.94909966e-01,  1.07653165e+00],\n",
            "        [-9.52696264e-01, -3.14790785e-01,  1.34545207e+00, ...,\n",
            "          1.09986889e+00,  1.28061080e+00, -1.33953059e+00]],\n",
            "\n",
            "       [[-6.16789639e-01, -2.84480572e+00,  2.14444947e+00, ...,\n",
            "         -1.46593642e+00,  2.24559709e-01, -6.03517294e-01],\n",
            "        [-1.43402815e-02,  1.24580431e+00, -2.16888118e+00, ...,\n",
            "         -1.56945121e+00, -1.00084901e+00,  3.95485431e-01],\n",
            "        [-4.23255980e-01,  2.27563906e+00, -1.76608896e+00, ...,\n",
            "         -1.22099864e+00, -1.11672771e+00,  5.09349525e-01],\n",
            "        ...,\n",
            "        [-1.03524542e+00, -1.37420928e+00,  9.60700035e-01, ...,\n",
            "          1.26592374e+00,  1.17819607e+00, -1.87586713e+00],\n",
            "        [-1.03408933e+00, -1.35780907e+00,  9.51996326e-01, ...,\n",
            "          1.24707532e+00,  1.18733227e+00, -1.90779221e+00],\n",
            "        [-1.04045737e+00, -1.36485326e+00,  9.57798362e-01, ...,\n",
            "          1.25195897e+00,  1.19846809e+00, -1.93030989e+00]]],\n",
            "      dtype=float32)>, <tf.Tensor: shape=(10, 16, 768), dtype=float32, numpy=\n",
            "array([[[ 2.66893983e-01, -2.54901004e+00,  1.92442226e+00, ...,\n",
            "         -2.86048651e+00, -3.23970541e-02, -8.10702682e-01],\n",
            "        [-4.71213102e-01, -1.11813426e-01,  1.64552569e+00, ...,\n",
            "         -8.28050226e-02,  1.97854102e+00, -8.93543363e-01],\n",
            "        [ 1.16339064e+00, -9.40951109e-02, -2.12818861e+00, ...,\n",
            "         -2.07642293e+00, -2.46230316e+00, -2.48801160e+00],\n",
            "        ...,\n",
            "        [ 6.61468744e+00,  2.37495208e+00,  4.32147884e+00, ...,\n",
            "         -1.55820191e+00, -2.05024338e+00,  3.04534745e+00],\n",
            "        [ 1.82801628e+00,  6.57142520e-01, -1.35956919e+00, ...,\n",
            "         -1.79732382e+00, -1.47519016e+00,  4.94871140e-01],\n",
            "        [-5.79753876e-01, -8.17482233e-01,  2.19815850e+00, ...,\n",
            "          1.32477212e+00,  2.39100367e-01, -1.45632410e+00]],\n",
            "\n",
            "       [[-6.50349438e-01, -2.68044448e+00,  1.89378607e+00, ...,\n",
            "         -2.09922671e+00, -2.40876824e-01, -9.54286098e-01],\n",
            "        [ 7.84652233e-01,  2.08309627e+00, -9.13362622e-01, ...,\n",
            "          1.67738497e-01,  2.92583132e+00, -8.63401055e-01],\n",
            "        [ 1.12634373e+00,  1.68647456e+00,  2.31181240e+00, ...,\n",
            "          3.35458159e-01, -1.04153264e+00, -1.73418546e+00],\n",
            "        ...,\n",
            "        [-1.88014722e+00, -7.55044073e-02,  2.60977054e+00, ...,\n",
            "          1.42607594e+00, -1.19229972e-01, -1.04700953e-01],\n",
            "        [-1.88102472e+00, -4.09819931e-02,  2.61526656e+00, ...,\n",
            "          1.41339707e+00, -9.49674249e-02, -1.39376521e-01],\n",
            "        [-1.88158834e+00, -4.77932543e-02,  2.62454367e+00, ...,\n",
            "          1.43149519e+00, -6.72198534e-02, -1.51949108e-01]],\n",
            "\n",
            "       [[ 2.66893983e-01, -2.54901004e+00,  1.92442226e+00, ...,\n",
            "         -2.86048651e+00, -3.23970541e-02, -8.10702682e-01],\n",
            "        [-4.71213102e-01, -1.11813426e-01,  1.64552569e+00, ...,\n",
            "         -8.28050226e-02,  1.97854102e+00, -8.93543363e-01],\n",
            "        [ 1.16339064e+00, -9.40951109e-02, -2.12818861e+00, ...,\n",
            "         -2.07642293e+00, -2.46230316e+00, -2.48801160e+00],\n",
            "        ...,\n",
            "        [ 6.61468744e+00,  2.37495208e+00,  4.32147884e+00, ...,\n",
            "         -1.55820191e+00, -2.05024338e+00,  3.04534745e+00],\n",
            "        [ 1.82801628e+00,  6.57142520e-01, -1.35956919e+00, ...,\n",
            "         -1.79732382e+00, -1.47519016e+00,  4.94871140e-01],\n",
            "        [-5.79753876e-01, -8.17482233e-01,  2.19815850e+00, ...,\n",
            "          1.32477212e+00,  2.39100367e-01, -1.45632410e+00]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[ 8.24723393e-03, -3.84178329e+00,  1.17131376e+00, ...,\n",
            "         -2.05112219e+00, -5.83213508e-01, -2.66044044e+00],\n",
            "        [-3.50085115e+00, -5.33019662e-01, -2.37781167e+00, ...,\n",
            "         -1.84065008e+00, -1.80073559e+00,  1.68426359e+00],\n",
            "        [ 1.60548925e+00,  2.53916740e+00, -1.11623764e+00, ...,\n",
            "          2.55814028e+00,  4.08759785e+00,  1.68919683e-01],\n",
            "        ...,\n",
            "        [ 1.54923069e+00,  7.52068996e+00, -2.28085446e+00, ...,\n",
            "          4.44309950e-01,  1.27731919e+00,  1.47873294e+00],\n",
            "        [ 2.03605294e+00,  8.69706273e-03,  8.48544419e-01, ...,\n",
            "         -1.72034574e+00, -1.39148617e+00,  3.12607765e-01],\n",
            "        [-1.14756775e+00, -3.20452750e-01,  1.78048575e+00, ...,\n",
            "          1.85996175e-01, -3.35769057e-02, -1.19250607e+00]],\n",
            "\n",
            "       [[-8.20025027e-01, -2.71920443e+00,  2.05626488e+00, ...,\n",
            "         -2.32908487e+00, -1.02389418e-01, -1.47211874e+00],\n",
            "        [ 9.01377499e-01, -1.01494014e+00, -1.59308815e+00, ...,\n",
            "          1.37244713e+00,  4.78599906e-01, -6.58626318e-01],\n",
            "        [-1.59329569e+00,  8.42237473e-01,  1.53464258e+00, ...,\n",
            "         -2.54082084e+00, -2.26347983e-01, -2.12117136e-01],\n",
            "        ...,\n",
            "        [-1.54585767e+00, -1.63446879e+00,  2.23326635e+00, ...,\n",
            "         -2.82008505e+00,  2.86821508e+00,  3.88959765e+00],\n",
            "        [-1.69217587e-03,  1.08607531e+00,  1.53931290e-01, ...,\n",
            "         -6.65888071e-01, -1.64783686e-01,  1.12038708e+00],\n",
            "        [-1.66204429e+00,  8.20943713e-02,  2.18814063e+00, ...,\n",
            "          2.94756293e-01,  5.39881289e-01, -9.44707155e-01]],\n",
            "\n",
            "       [[-7.31310427e-01, -3.11671329e+00,  2.34521604e+00, ...,\n",
            "         -1.74577057e+00,  2.04627171e-01, -5.56027353e-01],\n",
            "        [ 5.97137213e-03,  1.15728390e+00, -1.47512424e+00, ...,\n",
            "         -1.70163310e+00, -1.82052255e+00, -8.50129873e-02],\n",
            "        [ 6.10916018e-02,  2.03754544e+00, -1.58998597e+00, ...,\n",
            "         -1.31725252e+00, -1.88462019e+00,  1.30000591e+00],\n",
            "        ...,\n",
            "        [-1.52425969e+00, -1.16440272e+00,  2.06503534e+00, ...,\n",
            "          6.79672360e-01,  8.13405991e-01, -1.50137782e+00],\n",
            "        [-1.53187478e+00, -1.14186525e+00,  2.08009362e+00, ...,\n",
            "          6.66476846e-01,  8.27666223e-01, -1.53946352e+00],\n",
            "        [-1.54982829e+00, -1.14399290e+00,  2.10263276e+00, ...,\n",
            "          6.70922399e-01,  8.46532822e-01, -1.58340633e+00]]],\n",
            "      dtype=float32)>, <tf.Tensor: shape=(10, 16, 768), dtype=float32, numpy=\n",
            "array([[[ 0.28823444, -2.7399235 ,  2.1913962 , ..., -3.0156438 ,\n",
            "         -0.05920002, -0.700223  ],\n",
            "        [-0.41002798,  0.23444319,  2.5121791 , ..., -0.9468087 ,\n",
            "          3.2070973 ,  0.22425371],\n",
            "        [ 2.3728552 , -0.87843037, -3.6378672 , ..., -2.495627  ,\n",
            "         -3.4079792 , -3.0941672 ],\n",
            "        ...,\n",
            "        [ 8.275396  ,  2.3884916 ,  4.8783984 , ..., -2.6269875 ,\n",
            "         -1.9125099 ,  3.7659822 ],\n",
            "        [ 3.8915195 , -0.6581744 , -1.3012393 , ..., -2.195851  ,\n",
            "         -2.0948145 ,  2.2297373 ],\n",
            "        [-0.0460037 , -0.45033848,  3.5634613 , ...,  0.83693725,\n",
            "         -0.17346227,  0.23922294]],\n",
            "\n",
            "       [[-0.6236698 , -2.8990324 ,  2.1835477 , ..., -2.2600026 ,\n",
            "         -0.26361912, -0.84858423],\n",
            "        [ 1.0026112 ,  3.1201768 , -0.57262915, ...,  0.04419979,\n",
            "          3.3095365 , -0.14583737],\n",
            "        [ 1.6635362 ,  2.3737192 ,  2.49756   , ..., -0.4543537 ,\n",
            "          0.8182677 , -1.8236369 ],\n",
            "        ...,\n",
            "        [-2.3968122 , -0.53702724,  4.805647  , ...,  0.12922323,\n",
            "          0.80618644,  0.30822292],\n",
            "        [-2.4149253 , -0.5140545 ,  4.8243117 , ...,  0.08542192,\n",
            "          0.8381671 ,  0.28200877],\n",
            "        [-2.4210143 , -0.53110594,  4.8239474 , ...,  0.10386837,\n",
            "          0.86064357,  0.24787724]],\n",
            "\n",
            "       [[ 0.28823444, -2.7399235 ,  2.1913962 , ..., -3.0156438 ,\n",
            "         -0.05920002, -0.700223  ],\n",
            "        [-0.41002798,  0.23444319,  2.5121791 , ..., -0.9468087 ,\n",
            "          3.2070973 ,  0.22425371],\n",
            "        [ 2.3728552 , -0.87843037, -3.6378672 , ..., -2.495627  ,\n",
            "         -3.4079792 , -3.0941672 ],\n",
            "        ...,\n",
            "        [ 8.275396  ,  2.3884916 ,  4.8783984 , ..., -2.6269875 ,\n",
            "         -1.9125099 ,  3.7659822 ],\n",
            "        [ 3.8915195 , -0.6581744 , -1.3012393 , ..., -2.195851  ,\n",
            "         -2.0948145 ,  2.2297373 ],\n",
            "        [-0.0460037 , -0.45033848,  3.5634613 , ...,  0.83693725,\n",
            "         -0.17346227,  0.23922294]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[ 0.0426501 , -4.0317297 ,  1.4001924 , ..., -2.2554672 ,\n",
            "         -0.6445695 , -2.5455737 ],\n",
            "        [-2.4668803 , -1.9713147 , -2.6955492 , ..., -2.4239087 ,\n",
            "         -2.838624  ,  3.4150834 ],\n",
            "        [ 3.9732995 ,  1.9963841 , -1.0197024 , ...,  1.399943  ,\n",
            "          2.9897132 ,  1.5962255 ],\n",
            "        ...,\n",
            "        [ 2.7585113 ,  5.041998  , -1.8616287 , ...,  0.45431408,\n",
            "          0.89470124,  1.7304732 ],\n",
            "        [ 2.8002877 , -2.3526716 ,  2.582198  , ..., -3.9653735 ,\n",
            "         -2.2849102 ,  1.704111  ],\n",
            "        [-0.4729886 , -2.0336561 ,  3.34129   , ..., -1.5029097 ,\n",
            "         -0.5032579 , -0.6744115 ]],\n",
            "\n",
            "       [[-0.8201804 , -2.9187324 ,  2.3151517 , ..., -2.549948  ,\n",
            "         -0.18707865, -1.3539917 ],\n",
            "        [ 1.0878632 , -1.450842  , -0.7768947 , ...,  0.5804314 ,\n",
            "          1.6240278 , -0.28248107],\n",
            "        [-0.8353643 ,  0.84931326,  2.1320968 , ..., -2.5303395 ,\n",
            "          0.88484645,  0.60160875],\n",
            "        ...,\n",
            "        [-1.3666496 , -1.1442015 ,  4.6940804 , ..., -3.1025734 ,\n",
            "          4.6458216 ,  6.0623264 ],\n",
            "        [ 1.452203  ,  1.1056192 ,  3.08069   , ..., -3.2989273 ,\n",
            "          2.9688911 ,  4.416791  ],\n",
            "        [-0.93424994, -0.30674472,  5.336105  , ..., -2.2829587 ,\n",
            "          2.937881  ,  1.6942949 ]],\n",
            "\n",
            "       [[-0.7092386 , -3.3100636 ,  2.6137502 , ..., -1.9489658 ,\n",
            "          0.14873812, -0.42795527],\n",
            "        [ 1.2590287 ,  0.2944395 , -0.8800914 , ..., -2.4017162 ,\n",
            "         -0.5193794 ,  0.64996827],\n",
            "        [ 0.7046462 ,  0.93993235, -1.6489506 , ..., -2.3797126 ,\n",
            "         -1.1008301 ,  0.8838972 ],\n",
            "        ...,\n",
            "        [-0.28687644, -1.5206753 ,  4.6616654 , ..., -1.5865461 ,\n",
            "          2.3841033 ,  0.07908595],\n",
            "        [-0.2959132 , -1.4943745 ,  4.6783085 , ..., -1.5847573 ,\n",
            "          2.4067261 ,  0.04891926],\n",
            "        [-0.303686  , -1.5028735 ,  4.7071204 , ..., -1.5917807 ,\n",
            "          2.4329152 ,  0.01600409]]], dtype=float32)>, <tf.Tensor: shape=(10, 16, 768), dtype=float32, numpy=\n",
            "array([[[ 0.2832954 , -2.689885  ,  2.268529  , ..., -2.9864852 ,\n",
            "         -0.18480195, -0.7988681 ],\n",
            "        [ 0.22579756,  0.88942516,  3.207315  , ..., -2.3708167 ,\n",
            "          4.9290543 ,  0.46226364],\n",
            "        [ 2.6516502 , -0.6420326 , -3.3857744 , ..., -3.1582632 ,\n",
            "         -5.080944  , -2.9706528 ],\n",
            "        ...,\n",
            "        [ 8.704066  ,  0.16776037,  4.940382  , ..., -3.128853  ,\n",
            "         -3.701959  ,  5.3053904 ],\n",
            "        [ 5.93836   , -3.2127104 , -0.44361556, ..., -4.185788  ,\n",
            "         -5.484767  ,  4.3195515 ],\n",
            "        [ 1.1315551 , -1.1707283 ,  3.9958277 , ..., -1.6726973 ,\n",
            "         -4.180164  ,  1.2977389 ]],\n",
            "\n",
            "       [[-0.6195046 , -2.8828819 ,  2.3081336 , ..., -2.2292755 ,\n",
            "         -0.4141615 , -0.9109028 ],\n",
            "        [ 1.2299569 ,  2.6922188 , -0.92289853, ..., -0.7058824 ,\n",
            "          4.071291  ,  0.13618368],\n",
            "        [ 1.2227621 ,  1.9095237 ,  3.769999  , ..., -1.6945823 ,\n",
            "          1.2586565 , -1.4555333 ],\n",
            "        ...,\n",
            "        [-1.7792277 , -1.0455401 ,  5.393571  , ..., -1.8384204 ,\n",
            "         -1.3198051 ,  1.9786861 ],\n",
            "        [-1.8036808 , -1.013043  ,  5.4381704 , ..., -1.8899909 ,\n",
            "         -1.3049483 ,  1.9718907 ],\n",
            "        [-1.8159528 , -1.0375718 ,  5.4488463 , ..., -1.8664858 ,\n",
            "         -1.2807322 ,  1.9384103 ]],\n",
            "\n",
            "       [[ 0.2832954 , -2.689885  ,  2.268529  , ..., -2.9864852 ,\n",
            "         -0.18480195, -0.7988681 ],\n",
            "        [ 0.22579756,  0.88942516,  3.207315  , ..., -2.3708167 ,\n",
            "          4.9290543 ,  0.46226364],\n",
            "        [ 2.6516502 , -0.6420326 , -3.3857744 , ..., -3.1582632 ,\n",
            "         -5.080944  , -2.9706528 ],\n",
            "        ...,\n",
            "        [ 8.704066  ,  0.16776037,  4.940382  , ..., -3.128853  ,\n",
            "         -3.701959  ,  5.3053904 ],\n",
            "        [ 5.93836   , -3.2127104 , -0.44361556, ..., -4.185788  ,\n",
            "         -5.484767  ,  4.3195515 ],\n",
            "        [ 1.1315551 , -1.1707283 ,  3.9958277 , ..., -1.6726973 ,\n",
            "         -4.180164  ,  1.2977389 ]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[ 0.01585924, -4.0349565 ,  1.5064983 , ..., -2.266261  ,\n",
            "         -0.77999353, -2.5872443 ],\n",
            "        [-2.6661484 , -2.231053  , -2.61462   , ..., -3.1785884 ,\n",
            "         -4.198864  ,  5.0235605 ],\n",
            "        [ 5.240298  ,  1.9922352 , -0.3612391 , ...,  1.7063167 ,\n",
            "          0.03888321,  3.5800982 ],\n",
            "        ...,\n",
            "        [ 4.711527  ,  6.0061646 , -2.36909   , ...,  0.21744972,\n",
            "         -1.2008752 ,  1.0698023 ],\n",
            "        [ 4.6676908 , -2.0413039 ,  3.177493  , ..., -5.3782325 ,\n",
            "         -5.707382  ,  2.427572  ],\n",
            "        [ 1.7600362 , -1.4920024 ,  4.2291884 , ..., -3.3073823 ,\n",
            "         -4.501449  , -0.0749768 ]],\n",
            "\n",
            "       [[-0.8470488 , -2.8817255 ,  2.4165697 , ..., -2.5020044 ,\n",
            "         -0.3186549 , -1.4063444 ],\n",
            "        [ 0.0235306 , -2.0891788 , -0.20409733, ..., -0.19852734,\n",
            "         -0.4218241 , -0.21364799],\n",
            "        [-0.6528327 ,  0.2880934 ,  2.9324102 , ..., -3.185481  ,\n",
            "          1.2144803 ,  1.7659016 ],\n",
            "        ...,\n",
            "        [-0.46037665, -3.1272378 ,  7.7596292 , ..., -4.647449  ,\n",
            "          6.2894444 ,  8.29262   ],\n",
            "        [ 4.069711  ,  0.72691256,  4.435911  , ..., -6.403121  ,\n",
            "          2.8840806 ,  6.72381   ],\n",
            "        [ 1.7668388 , -0.28849897,  5.5475526 , ..., -5.057933  ,\n",
            "          2.467655  ,  2.5021148 ]],\n",
            "\n",
            "       [[-0.70459443, -3.264804  ,  2.7035902 , ..., -1.9193426 ,\n",
            "          0.02724721, -0.47617483],\n",
            "        [ 1.1209515 , -0.27710283, -0.06300449, ..., -3.4211786 ,\n",
            "         -0.4183997 ,  0.98404   ],\n",
            "        [ 0.95643675,  0.6752474 , -1.1388315 , ..., -3.8247313 ,\n",
            "         -2.0946193 ,  1.316601  ],\n",
            "        ...,\n",
            "        [ 0.5331423 , -2.4824588 ,  4.8046684 , ..., -6.011614  ,\n",
            "         -0.96559834,  0.953942  ],\n",
            "        [ 0.52258956, -2.4606771 ,  4.821089  , ..., -6.0204287 ,\n",
            "         -0.9330399 ,  0.9478364 ],\n",
            "        [ 0.5086756 , -2.4812107 ,  4.8572645 , ..., -6.0330515 ,\n",
            "         -0.8965231 ,  0.94428736]]], dtype=float32)>, <tf.Tensor: shape=(10, 16, 768), dtype=float32, numpy=\n",
            "array([[[ 0.38378763, -2.5920424 ,  2.2271886 , ..., -2.950476  ,\n",
            "         -0.09940623, -0.87189317],\n",
            "        [-0.19316444,  3.1065207 ,  3.4970617 , ..., -1.3279448 ,\n",
            "          6.3473673 ,  0.509076  ],\n",
            "        [ 2.7191684 , -0.8314458 , -4.265356  , ..., -3.0760617 ,\n",
            "         -6.2419424 , -3.2966027 ],\n",
            "        ...,\n",
            "        [ 9.31414   ,  1.3996289 ,  3.3901687 , ..., -2.937407  ,\n",
            "         -5.7652082 ,  5.950598  ],\n",
            "        [ 4.465349  , -1.1145324 , -0.98361486, ..., -3.2942588 ,\n",
            "         -7.579904  ,  5.3525867 ],\n",
            "        [-1.6522379 ,  2.7219658 ,  2.7189856 , ..., -1.2381501 ,\n",
            "         -7.0553513 ,  1.6860945 ]],\n",
            "\n",
            "       [[-0.52106714, -2.7604096 ,  2.284996  , ..., -2.1615384 ,\n",
            "         -0.3921463 , -0.9972261 ],\n",
            "        [ 1.1781185 ,  2.403835  , -1.0109931 , ..., -1.1508844 ,\n",
            "          4.2191124 , -1.0766003 ],\n",
            "        [ 1.8596485 ,  1.9336926 ,  5.4031253 , ..., -4.874525  ,\n",
            "          0.13877597, -1.1568003 ],\n",
            "        ...,\n",
            "        [-3.2624583 ,  1.3574324 ,  4.130141  , ..., -0.9661387 ,\n",
            "         -4.6165395 ,  1.6935475 ],\n",
            "        [-3.2949018 ,  1.4233005 ,  4.1823554 , ..., -1.0134547 ,\n",
            "         -4.6561356 ,  1.6948953 ],\n",
            "        [-3.303639  ,  1.3943024 ,  4.2032437 , ..., -0.9901879 ,\n",
            "         -4.6302423 ,  1.6841516 ]],\n",
            "\n",
            "       [[ 0.38378763, -2.5920424 ,  2.2271886 , ..., -2.950476  ,\n",
            "         -0.09940623, -0.87189317],\n",
            "        [-0.19316444,  3.1065207 ,  3.4970617 , ..., -1.3279448 ,\n",
            "          6.3473673 ,  0.509076  ],\n",
            "        [ 2.7191684 , -0.8314458 , -4.265356  , ..., -3.0760617 ,\n",
            "         -6.2419424 , -3.2966027 ],\n",
            "        ...,\n",
            "        [ 9.31414   ,  1.3996289 ,  3.3901687 , ..., -2.937407  ,\n",
            "         -5.7652082 ,  5.950598  ],\n",
            "        [ 4.465349  , -1.1145324 , -0.98361486, ..., -3.2942588 ,\n",
            "         -7.579904  ,  5.3525867 ],\n",
            "        [-1.6522379 ,  2.7219658 ,  2.7189856 , ..., -1.2381501 ,\n",
            "         -7.0553513 ,  1.6860945 ]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[ 0.13154776, -3.9646378 ,  1.4977258 , ..., -2.227442  ,\n",
            "         -0.6988599 , -2.66546   ],\n",
            "        [-2.2793217 , -3.8300202 , -2.9220667 , ..., -3.1633582 ,\n",
            "         -4.244186  ,  6.3781195 ],\n",
            "        [ 6.6992292 , -1.4955547 , -1.739999  , ...,  1.1322224 ,\n",
            "         -2.5995488 ,  4.1861362 ],\n",
            "        ...,\n",
            "        [ 4.2555604 ,  6.6637077 , -2.473155  , ..., -1.3458581 ,\n",
            "         -4.792507  ,  1.7427186 ],\n",
            "        [ 4.460588  , -2.525846  ,  3.2811382 , ..., -4.6388626 ,\n",
            "         -9.94589   ,  3.1641903 ],\n",
            "        [ 1.3744321 , -0.98627245,  4.5116005 , ..., -3.5458803 ,\n",
            "         -9.091578  ,  0.9367672 ]],\n",
            "\n",
            "       [[-0.7303915 , -2.7685006 ,  2.3566105 , ..., -2.4808948 ,\n",
            "         -0.34490892, -1.4789646 ],\n",
            "        [ 0.4290429 , -4.395918  ,  0.44143274, ..., -1.3878542 ,\n",
            "         -1.6901829 ,  0.17275159],\n",
            "        [-2.1480713 , -0.87860996,  5.0741568 , ..., -5.239466  ,\n",
            "          0.09571892,  1.2882624 ],\n",
            "        ...,\n",
            "        [-1.7906128 ,  0.60967696,  8.361551  , ..., -6.3506217 ,\n",
            "          4.630135  ,  9.048391  ],\n",
            "        [ 1.8639226 ,  6.1503506 ,  5.8843184 , ..., -7.4817576 ,\n",
            "          0.4289032 ,  8.0978985 ],\n",
            "        [-0.5533825 ,  5.4248834 ,  6.982635  , ..., -5.6449614 ,\n",
            "         -0.6816269 ,  4.2768326 ]],\n",
            "\n",
            "       [[-0.57390624, -3.1149938 ,  2.6599214 , ..., -1.9136158 ,\n",
            "          0.05272692, -0.56645995],\n",
            "        [ 2.5071564 ,  0.2015621 , -0.61922014, ..., -3.891086  ,\n",
            "          1.3350137 ,  0.48548517],\n",
            "        [ 1.4038632 ,  0.74087983,  0.3636707 , ..., -5.0161605 ,\n",
            "         -3.1042666 ,  0.8816886 ],\n",
            "        ...,\n",
            "        [-0.81981146,  1.7560246 ,  3.7465696 , ..., -7.037284  ,\n",
            "         -2.175591  ,  1.9816874 ],\n",
            "        [-0.83347446,  1.8222548 ,  3.7773345 , ..., -7.0329046 ,\n",
            "         -2.1520963 ,  2.0030942 ],\n",
            "        [-0.85293674,  1.8464963 ,  3.8176641 , ..., -7.0277185 ,\n",
            "         -2.1263232 ,  2.0229218 ]]], dtype=float32)>, <tf.Tensor: shape=(10, 16, 768), dtype=float32, numpy=\n",
            "array([[[ 0.585009  , -2.4734354 ,  1.97301   , ..., -3.086895  ,\n",
            "          0.09998207, -0.91219294],\n",
            "        [ 0.7623514 ,  3.628211  ,  2.6606398 , ..., -2.2605739 ,\n",
            "          8.558371  ,  0.42611974],\n",
            "        [ 3.2549877 , -2.3524895 , -7.125227  , ..., -4.5740933 ,\n",
            "         -6.848298  , -3.30887   ],\n",
            "        ...,\n",
            "        [11.922141  ,  0.8132173 ,  3.814556  , ..., -3.9399576 ,\n",
            "         -5.2228813 ,  7.6409197 ],\n",
            "        [ 8.398603  , -0.7580939 , -0.10478371, ..., -5.3453784 ,\n",
            "         -6.9614816 ,  6.4869747 ],\n",
            "        [ 1.6797948 ,  2.3509622 ,  2.749406  , ..., -2.3941278 ,\n",
            "         -6.9568195 ,  2.6900547 ]],\n",
            "\n",
            "       [[-0.35189322, -2.6702776 ,  2.0524096 , ..., -2.2337563 ,\n",
            "         -0.23152097, -1.1028107 ],\n",
            "        [ 2.059031  ,  3.0315864 , -0.93791664, ...,  0.06597066,\n",
            "          5.342987  , -1.0839016 ],\n",
            "        [ 1.5390933 ,  1.6777931 ,  6.700643  , ..., -5.8872566 ,\n",
            "          1.1746137 , -0.13348304],\n",
            "        ...,\n",
            "        [-2.0076258 ,  0.20577997,  4.9323883 , ..., -1.8201531 ,\n",
            "         -2.6256118 ,  1.4941819 ],\n",
            "        [-2.0140924 ,  0.2561934 ,  4.9809146 , ..., -1.8687582 ,\n",
            "         -2.6694973 ,  1.4965886 ],\n",
            "        [-2.0195503 ,  0.22161925,  5.0006    , ..., -1.8416542 ,\n",
            "         -2.649986  ,  1.4929737 ]],\n",
            "\n",
            "       [[ 0.585009  , -2.4734354 ,  1.97301   , ..., -3.086895  ,\n",
            "          0.09998207, -0.91219294],\n",
            "        [ 0.7623514 ,  3.628211  ,  2.6606398 , ..., -2.2605739 ,\n",
            "          8.558371  ,  0.42611974],\n",
            "        [ 3.2549877 , -2.3524895 , -7.125227  , ..., -4.5740933 ,\n",
            "         -6.848298  , -3.30887   ],\n",
            "        ...,\n",
            "        [11.922141  ,  0.8132173 ,  3.814556  , ..., -3.9399576 ,\n",
            "         -5.2228813 ,  7.6409197 ],\n",
            "        [ 8.398603  , -0.7580939 , -0.10478371, ..., -5.3453784 ,\n",
            "         -6.9614816 ,  6.4869747 ],\n",
            "        [ 1.6797948 ,  2.3509622 ,  2.749406  , ..., -2.3941278 ,\n",
            "         -6.9568195 ,  2.6900547 ]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[ 0.39202788, -3.8400118 ,  1.2610755 , ..., -2.3680325 ,\n",
            "         -0.5871509 , -2.6623003 ],\n",
            "        [-3.8889694 , -4.1862464 , -3.7646675 , ..., -1.7945054 ,\n",
            "         -4.1977897 ,  6.8060317 ],\n",
            "        [ 5.7602496 , -2.9066994 , -6.245408  , ...,  1.5947781 ,\n",
            "         -3.102061  ,  5.1118736 ],\n",
            "        ...,\n",
            "        [ 6.6370792 , -0.0930419 , -4.4285727 , ...,  2.6224947 ,\n",
            "         -5.7342725 ,  3.7823987 ],\n",
            "        [ 8.791572  , -6.009706  ,  0.07285047, ...,  0.4461167 ,\n",
            "         -9.362883  ,  4.95374   ],\n",
            "        [ 4.600875  , -3.9882014 ,  0.8459401 , ...,  0.6439016 ,\n",
            "         -8.46828   ,  3.0650477 ]],\n",
            "\n",
            "       [[-0.5685972 , -2.5923765 ,  2.101343  , ..., -2.6089585 ,\n",
            "         -0.22440372, -1.5228438 ],\n",
            "        [ 0.08265358, -5.73705   , -0.13283528, ..., -1.1426098 ,\n",
            "         -1.5589823 ,  0.05752599],\n",
            "        [-0.3079235 , -3.203504  ,  5.6066165 , ..., -3.7334638 ,\n",
            "          0.46479172,  1.0764624 ],\n",
            "        ...,\n",
            "        [ 3.1720152 ,  1.6978079 ,  7.6780167 , ..., -6.7331977 ,\n",
            "          8.961229  ,  9.776953  ],\n",
            "        [ 8.168188  ,  7.888269  ,  5.588588  , ..., -7.788849  ,\n",
            "          5.0412545 ,  8.947709  ],\n",
            "        [ 4.9802585 ,  6.759989  ,  7.144209  , ..., -6.100991  ,\n",
            "          2.4863567 ,  4.71949   ]],\n",
            "\n",
            "       [[-0.38837922, -2.9774132 ,  2.421862  , ..., -2.033844  ,\n",
            "          0.21043146, -0.6097916 ],\n",
            "        [ 3.2184699 , -1.3854517 , -1.6013114 , ..., -3.849234  ,\n",
            "          1.2395613 ,  0.03649747],\n",
            "        [ 1.8232433 , -2.0324647 , -0.9766853 , ..., -4.9076633 ,\n",
            "         -2.2385094 , -0.79192674],\n",
            "        ...,\n",
            "        [ 1.8066941 ,  0.296842  ,  2.7398746 , ..., -9.568648  ,\n",
            "          1.1517854 ,  1.209085  ],\n",
            "        [ 1.8087558 ,  0.34500495,  2.763398  , ..., -9.581504  ,\n",
            "          1.1760464 ,  1.2126738 ],\n",
            "        [ 1.7979585 ,  0.36544034,  2.7975528 , ..., -9.594811  ,\n",
            "          1.2054722 ,  1.220914  ]]], dtype=float32)>, <tf.Tensor: shape=(10, 16, 768), dtype=float32, numpy=\n",
            "array([[[ 0.64783585, -2.1949368 ,  1.5195543 , ..., -3.2789083 ,\n",
            "          0.37916034, -1.162971  ],\n",
            "        [ 1.8349378 ,  3.4724996 ,  0.63457906, ..., -2.6435888 ,\n",
            "          9.829746  ,  0.7461405 ],\n",
            "        [ 6.2542787 , -1.8540199 , -8.203183  , ..., -4.684268  ,\n",
            "         -6.8922954 , -4.3145237 ],\n",
            "        ...,\n",
            "        [16.400387  , -1.507174  ,  1.7212989 , ..., -1.9334204 ,\n",
            "         -4.0122247 ,  4.998542  ],\n",
            "        [13.2145195 , -3.676982  , -0.81767493, ..., -2.5324104 ,\n",
            "         -7.051134  ,  4.2301    ],\n",
            "        [ 6.196108  , -1.0481284 ,  1.5696356 , ..., -0.13038874,\n",
            "         -6.657055  ,  0.6421755 ]],\n",
            "\n",
            "       [[-0.31011045, -2.351571  ,  1.6524956 , ..., -2.4566646 ,\n",
            "          0.1166521 , -1.2985746 ],\n",
            "        [ 2.7887185 ,  1.9461178 , -1.4997874 , ..., -0.3773234 ,\n",
            "          5.8506613 ,  0.49276286],\n",
            "        [ 2.9035413 ,  2.8006835 ,  8.051298  , ..., -2.5340033 ,\n",
            "          0.8370348 ,  2.887794  ],\n",
            "        ...,\n",
            "        [ 1.7351693 , -0.31422064,  6.46443   , ...,  0.26594162,\n",
            "         -2.2009442 ,  1.1550287 ],\n",
            "        [ 1.7465892 , -0.2597174 ,  6.522938  , ...,  0.24084711,\n",
            "         -2.2594235 ,  1.1617166 ],\n",
            "        [ 1.7371231 , -0.3039647 ,  6.547504  , ...,  0.26879144,\n",
            "         -2.2392595 ,  1.1552345 ]],\n",
            "\n",
            "       [[ 0.64783585, -2.1949368 ,  1.5195543 , ..., -3.2789083 ,\n",
            "          0.37916034, -1.162971  ],\n",
            "        [ 1.8349378 ,  3.4724996 ,  0.63457906, ..., -2.6435888 ,\n",
            "          9.829746  ,  0.7461405 ],\n",
            "        [ 6.2542787 , -1.8540199 , -8.203183  , ..., -4.684268  ,\n",
            "         -6.8922954 , -4.3145237 ],\n",
            "        ...,\n",
            "        [16.400387  , -1.507174  ,  1.7212989 , ..., -1.9334204 ,\n",
            "         -4.0122247 ,  4.998542  ],\n",
            "        [13.2145195 , -3.676982  , -0.81767493, ..., -2.5324104 ,\n",
            "         -7.051134  ,  4.2301    ],\n",
            "        [ 6.196108  , -1.0481284 ,  1.5696356 , ..., -0.13038874,\n",
            "         -6.657055  ,  0.6421755 ]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[ 0.44789293, -3.5401618 ,  0.75966805, ..., -2.6322522 ,\n",
            "         -0.2953487 , -2.848237  ],\n",
            "        [-3.0481918 , -3.3881383 , -5.764847  , ..., -2.6546493 ,\n",
            "         -3.4595299 ,  6.193072  ],\n",
            "        [ 7.0032263 , -1.4883919 , -8.243334  , ...,  1.0190614 ,\n",
            "         -1.1201735 ,  3.879443  ],\n",
            "        ...,\n",
            "        [ 7.998804  , -1.8691407 , -4.1793575 , ...,  4.3027215 ,\n",
            "         -3.4854178 ,  4.617146  ],\n",
            "        [10.432376  , -9.520494  , -0.8499834 , ...,  2.884961  ,\n",
            "         -7.928866  ,  3.679258  ],\n",
            "        [ 6.7235804 , -7.6304936 , -0.42624998, ...,  2.6912444 ,\n",
            "         -6.4928837 ,  1.7062625 ]],\n",
            "\n",
            "       [[-0.49286342, -2.1768417 ,  1.6244552 , ..., -2.902851  ,\n",
            "          0.01719046, -1.7651968 ],\n",
            "        [ 4.4104033 , -5.581779  , -1.8147141 , ..., -0.41637868,\n",
            "         -1.539376  ,  0.4298543 ],\n",
            "        [ 3.6106098 , -3.7477405 ,  3.3657188 , ..., -2.9885519 ,\n",
            "          0.13177016,  1.4503663 ],\n",
            "        ...,\n",
            "        [ 7.081209  ,  1.8994471 ,  7.692251  , ..., -7.1396084 ,\n",
            "         10.412426  , 11.099559  ],\n",
            "        [12.654648  ,  8.668153  ,  5.718513  , ..., -8.177036  ,\n",
            "          6.8863106 , 10.835819  ],\n",
            "        [ 9.465487  ,  7.229929  ,  6.9208875 , ..., -6.198044  ,\n",
            "          4.5950637 ,  6.4976087 ]],\n",
            "\n",
            "       [[-0.25405276, -2.6575744 ,  2.013416  , ..., -2.3068187 ,\n",
            "          0.4040246 , -0.87949824],\n",
            "        [ 5.057736  , -1.2214087 , -1.8752873 , ..., -3.558913  ,\n",
            "          2.5331674 ,  0.99065995],\n",
            "        [ 2.3726416 , -2.7158546 , -1.0424054 , ..., -3.5049872 ,\n",
            "         -1.3237598 , -1.6448756 ],\n",
            "        ...,\n",
            "        [ 6.3056374 , -0.4738843 ,  1.5271192 , ..., -9.199973  ,\n",
            "          3.8622577 ,  1.2489479 ],\n",
            "        [ 6.3159    , -0.42326978,  1.5431025 , ..., -9.216513  ,\n",
            "          3.89259   ,  1.2531924 ],\n",
            "        [ 6.313363  , -0.4005054 ,  1.5653925 , ..., -9.237839  ,\n",
            "          3.917216  ,  1.2564747 ]]], dtype=float32)>, <tf.Tensor: shape=(10, 16, 768), dtype=float32, numpy=\n",
            "array([[[-0.05380506, -0.22883826,  0.07309419, ..., -0.24215063,\n",
            "         -0.08541422, -0.06932674],\n",
            "        [ 0.14493087,  0.22115526,  0.04617205, ..., -0.06386319,\n",
            "          0.39018697,  0.05897988],\n",
            "        [ 0.30707982, -0.05818498, -0.47399157, ..., -0.12061885,\n",
            "         -0.31095067, -0.0922084 ],\n",
            "        ...,\n",
            "        [ 0.5901565 , -0.01071263,  0.05164839, ..., -0.00861153,\n",
            "         -0.16212073,  0.16365285],\n",
            "        [ 0.44195965, -0.06807346,  0.02230646, ..., -0.00551292,\n",
            "         -0.22887227,  0.13154224],\n",
            "        [ 0.24030992, -0.00710621,  0.10394002, ...,  0.04644899,\n",
            "         -0.21129583,  0.05205108]],\n",
            "\n",
            "       [[-0.06004831, -0.14102297,  0.16319063, ..., -0.12483071,\n",
            "         -0.08076319, -0.09347703],\n",
            "        [ 0.1662042 ,  0.14385645, -0.13151032, ...,  0.05118155,\n",
            "          0.14359815,  0.02764047],\n",
            "        [ 0.23380454,  0.19670065,  0.5794729 , ..., -0.1052708 ,\n",
            "          0.04844327,  0.12717253],\n",
            "        ...,\n",
            "        [ 0.20629998,  0.07535615,  0.43262076, ...,  0.07991193,\n",
            "         -0.0588664 ,  0.04937177],\n",
            "        [ 0.20709527,  0.0772348 ,  0.43558696, ...,  0.07951115,\n",
            "         -0.06015953,  0.04943265],\n",
            "        [ 0.20684724,  0.07559115,  0.43678287, ...,  0.08040171,\n",
            "         -0.05962781,  0.0494081 ]],\n",
            "\n",
            "       [[-0.05380506, -0.22883826,  0.07309419, ..., -0.24215063,\n",
            "         -0.08541422, -0.06932674],\n",
            "        [ 0.14493087,  0.22115526,  0.04617205, ..., -0.06386319,\n",
            "          0.39018697,  0.05897988],\n",
            "        [ 0.30707982, -0.05818498, -0.47399157, ..., -0.12061885,\n",
            "         -0.31095067, -0.0922084 ],\n",
            "        ...,\n",
            "        [ 0.5901565 , -0.01071263,  0.05164839, ..., -0.00861153,\n",
            "         -0.16212073,  0.16365285],\n",
            "        [ 0.44195965, -0.06807346,  0.02230646, ..., -0.00551292,\n",
            "         -0.22887227,  0.13154224],\n",
            "        [ 0.24030992, -0.00710621,  0.10394002, ...,  0.04644899,\n",
            "         -0.21129583,  0.05205108]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[ 0.0142574 , -0.30518597, -0.02511057, ..., -0.17955811,\n",
            "         -0.15418454, -0.21303129],\n",
            "        [-0.03802349, -0.19447678, -0.52952355, ..., -0.12650685,\n",
            "         -0.16050373,  0.3307149 ],\n",
            "        [ 0.5544525 , -0.11418188, -0.6903671 , ...,  0.07486912,\n",
            "         -0.00294878,  0.22927882],\n",
            "        ...,\n",
            "        [ 0.40080738, -0.1457249 , -0.18726167, ...,  0.21103941,\n",
            "         -0.11555526,  0.17001252],\n",
            "        [ 0.43573567, -0.3680677 , -0.0039873 , ...,  0.1541921 ,\n",
            "         -0.1845223 ,  0.09444386],\n",
            "        [ 0.33825862, -0.3233704 ,  0.0183672 , ...,  0.15683827,\n",
            "         -0.15549871,  0.0512347 ]],\n",
            "\n",
            "       [[-0.17896761, -0.15195303,  0.10352725, ..., -0.20506214,\n",
            "         -0.11582297, -0.12091729],\n",
            "        [ 0.39382297, -0.32236034, -0.09902975, ...,  0.05167987,\n",
            "         -0.07272494,  0.12501515],\n",
            "        [ 0.24357806, -0.11502392,  0.13944896, ..., -0.00771161,\n",
            "         -0.03122761,  0.11610219],\n",
            "        ...,\n",
            "        [ 0.22029957,  0.09265533,  0.27756813, ..., -0.13042127,\n",
            "          0.24541257,  0.35813618],\n",
            "        [ 0.37181056,  0.31020486,  0.21655677, ..., -0.12152363,\n",
            "          0.13390584,  0.29766202],\n",
            "        [ 0.29342622,  0.2732282 ,  0.25840867, ..., -0.08411527,\n",
            "          0.08636368,  0.198104  ]],\n",
            "\n",
            "       [[-0.09436099, -0.17711659,  0.18083595, ..., -0.15164998,\n",
            "         -0.07932015,  0.01069095],\n",
            "        [ 0.36856577, -0.08517455, -0.10579857, ..., -0.15074694,\n",
            "          0.09511233,  0.1358735 ],\n",
            "        [ 0.22346576, -0.12235852, -0.09622696, ..., -0.09490101,\n",
            "         -0.0870596 , -0.01061698],\n",
            "        ...,\n",
            "        [ 0.24560328, -0.0026612 ,  0.09024342, ..., -0.15597725,\n",
            "          0.07887182,  0.0598128 ],\n",
            "        [ 0.24578099, -0.0015517 ,  0.09083267, ..., -0.15631475,\n",
            "          0.07940269,  0.0597695 ],\n",
            "        [ 0.2455213 , -0.00103716,  0.09158725, ..., -0.15686642,\n",
            "          0.07997738,  0.05967391]]], dtype=float32)>)\n",
            "13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#simiarity for layers\n",
        "\n",
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "\n",
        "def calculate_similarities(hidden_states):\n",
        "\n",
        "    # Number of examples and layers\n",
        "    num_examples, num_layers = hidden_states[0].shape[0], len(hidden_states)\n",
        "\n",
        "    # Initializing arrays to store the results\n",
        "    cosine_similarities = np.zeros((num_examples, num_layers))\n",
        "    dot_products = np.zeros((num_examples, num_layers))\n",
        "\n",
        "    # Final layer's last token output\n",
        "    # (layers, examples, seq, embedding dimension)\n",
        "    final_layer_output = hidden_states[-1]\n",
        "\n",
        "    # Calculating similarities by layers\n",
        "    for i in range(num_layers):\n",
        "        for j in range(num_examples):\n",
        "            # Extracting the CLS output for the current layer and example\n",
        "            current_output = hidden_states[i][j][-1]\n",
        "\n",
        "            # Cosine Similarity\n",
        "            cosine_similarities[j, i] = np.dot(current_output, final_layer_output[j][-1]) / (norm(current_output) * norm(final_layer_output[j][-1]))\n",
        "\n",
        "    return cosine_similarities\n",
        "\n",
        "\n",
        "cosine_similarities_layer = calculate_similarities(hidden_states)\n",
        "print(cosine_similarities_layer.shape)\n",
        "#10 examples, 13 cosine similarirty of 13 layers. last one will be 1.\n",
        "#(10, 13)"
      ],
      "metadata": {
        "id": "sH2FIFo9f_2r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8e9a348-63b7-40c4-838f-ee4772a0c552"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for var in gpt2_model.variables:\n",
        "    print(f\"{var.name}: {var.shape}\")"
      ],
      "metadata": {
        "id": "8fu45IhK9Qmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_ffn_second_dense_weights(gpt2_model):\n",
        "    \"\"\"\n",
        "    Extracts the kernel weights from the second dense layer of the FFN in each transformer layer of the gpt model.\n",
        "    \"\"\"\n",
        "    ffn_weights = []\n",
        "    # Loop through each transformer layer and construct the variable name\n",
        "    for layer_num in range(gpt2_model.config.num_hidden_layers):\n",
        "        # Construct the variable name for the second dense layer weights in the current layer\n",
        "        weight_name = f\"tfgpt2lm_head_model/transformer/h_._{layer_num}/mlp/c_proj/weight:0\" #f\"tfgpt2lm_head_model_1/transformer/h_._{layer_num}/mlp/c_proj/weight:0\"\n",
        "\n",
        "        # Find and extract the variable\n",
        "        for var in gpt2_model.variables:\n",
        "            if var.name == weight_name:\n",
        "                print(var)\n",
        "                weights = var.numpy()  # Convert to numpy array\n",
        "                ffn_weights.append(weights)\n",
        "                break  # Move to the next layer once the weights are found\n",
        "\n",
        "    return ffn_weights\n",
        "\n",
        "ffn_weights = extract_ffn_second_dense_weights(gpt2_model)"
      ],
      "metadata": {
        "id": "aAOoSohPen83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3b95a8f-c4f8-4003-c528-bad2a9912957"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tf.Variable 'tfgpt2lm_head_model/transformer/h_._0/mlp/c_proj/weight:0' shape=(3072, 768) dtype=float32, numpy=\n",
            "array([[-0.10640587,  0.15262553,  0.03359779, ...,  0.16453362,\n",
            "        -0.04087063,  0.12173471],\n",
            "       [ 0.03767035, -0.05911045,  0.08402098, ...,  0.09362248,\n",
            "        -0.09900269, -0.0383853 ],\n",
            "       [-0.0751109 ,  0.00819661, -0.09503949, ...,  0.05139565,\n",
            "        -0.00313113, -0.03031024],\n",
            "       ...,\n",
            "       [-0.10314862,  0.0084471 , -0.02996467, ...,  0.0124373 ,\n",
            "         0.00169209, -0.01738732],\n",
            "       [-0.02953189, -0.03279107, -0.00758716, ...,  0.09013154,\n",
            "        -0.04486695,  0.0052709 ],\n",
            "       [-0.02159979,  0.08179252,  0.07957918, ..., -0.05555193,\n",
            "        -0.00797795, -0.08200686]], dtype=float32)>\n",
            "<tf.Variable 'tfgpt2lm_head_model/transformer/h_._1/mlp/c_proj/weight:0' shape=(3072, 768) dtype=float32, numpy=\n",
            "array([[ 0.10997669, -0.02686073, -0.03303344, ..., -0.00979511,\n",
            "        -0.03103735, -0.06873523],\n",
            "       [-0.21017483, -0.1568364 , -0.0895923 , ..., -0.01275893,\n",
            "         0.10914882, -0.1784509 ],\n",
            "       [ 0.07950886, -0.04905485,  0.02184192, ..., -0.03047541,\n",
            "         0.08872238,  0.03437476],\n",
            "       ...,\n",
            "       [-0.00150437, -0.04922873,  0.04324365, ...,  0.01567951,\n",
            "        -0.25235832, -0.00460507],\n",
            "       [ 0.03331204, -0.00525845,  0.0590447 , ...,  0.12849261,\n",
            "         0.10008943,  0.15944314],\n",
            "       [-0.06697052,  0.00283542, -0.15455784, ..., -0.04995948,\n",
            "        -0.03335874,  0.02680166]], dtype=float32)>\n",
            "<tf.Variable 'tfgpt2lm_head_model/transformer/h_._2/mlp/c_proj/weight:0' shape=(3072, 768) dtype=float32, numpy=\n",
            "array([[-0.02447586, -0.20989202, -0.02782869, ...,  0.04591068,\n",
            "        -0.145903  , -0.01741505],\n",
            "       [ 0.12579283,  0.02153219, -0.11815943, ...,  0.06223075,\n",
            "        -0.01928602,  0.03519571],\n",
            "       [-0.06431905, -0.0497903 , -0.09908496, ...,  0.08887358,\n",
            "        -0.01592803,  0.12029443],\n",
            "       ...,\n",
            "       [-0.05237674, -0.17144328, -0.05976378, ..., -0.06219316,\n",
            "        -0.24641657, -0.08427772],\n",
            "       [-0.05112857, -0.00616928, -0.05120523, ..., -0.01853693,\n",
            "         0.04952437,  0.08454016],\n",
            "       [ 0.03735315,  0.08537844,  0.10238375, ..., -0.12298351,\n",
            "        -0.03645371, -0.09400574]], dtype=float32)>\n",
            "<tf.Variable 'tfgpt2lm_head_model/transformer/h_._3/mlp/c_proj/weight:0' shape=(3072, 768) dtype=float32, numpy=\n",
            "array([[ 0.07576783,  0.00628715, -0.08186903, ..., -0.06278385,\n",
            "         0.08428346,  0.16467169],\n",
            "       [ 0.02361375, -0.00348412,  0.04023331, ...,  0.00299722,\n",
            "        -0.07327039,  0.00806136],\n",
            "       [ 0.02199135,  0.08009974,  0.03633054, ...,  0.08763226,\n",
            "        -0.05269175, -0.10238034],\n",
            "       ...,\n",
            "       [ 0.0199105 , -0.08614636, -0.06750677, ..., -0.17352523,\n",
            "        -0.02333833, -0.12188456],\n",
            "       [-0.05596993, -0.05134237, -0.04746917, ...,  0.04191604,\n",
            "         0.17751598, -0.10212966],\n",
            "       [ 0.0006447 ,  0.12763056,  0.02632942, ...,  0.03279496,\n",
            "         0.00868961, -0.1277856 ]], dtype=float32)>\n",
            "<tf.Variable 'tfgpt2lm_head_model/transformer/h_._4/mlp/c_proj/weight:0' shape=(3072, 768) dtype=float32, numpy=\n",
            "array([[-0.01750967,  0.03438131, -0.02620002, ...,  0.09254912,\n",
            "         0.01522355, -0.07222731],\n",
            "       [-0.08077925,  0.01332326,  0.1343643 , ...,  0.1348026 ,\n",
            "        -0.09389073, -0.09779851],\n",
            "       [-0.07680063,  0.00627543,  0.0894789 , ...,  0.07702998,\n",
            "        -0.17175187,  0.10006081],\n",
            "       ...,\n",
            "       [ 0.04927289, -0.00511088,  0.018364  , ..., -0.04178583,\n",
            "         0.08004744, -0.02137691],\n",
            "       [-0.06954475, -0.05334477,  0.02418102, ...,  0.20711513,\n",
            "         0.06285234,  0.15363751],\n",
            "       [-0.01518197, -0.2219018 ,  0.04306404, ..., -0.02172388,\n",
            "        -0.05527365,  0.11349048]], dtype=float32)>\n",
            "<tf.Variable 'tfgpt2lm_head_model/transformer/h_._5/mlp/c_proj/weight:0' shape=(3072, 768) dtype=float32, numpy=\n",
            "array([[ 0.06707083,  0.10658588,  0.10016622, ..., -0.06869408,\n",
            "        -0.07451253,  0.07837158],\n",
            "       [-0.1651444 ,  0.11305191, -0.12592304, ..., -0.1000138 ,\n",
            "         0.08220617,  0.06141069],\n",
            "       [ 0.17730604,  0.0855474 ,  0.04617438, ..., -0.18981159,\n",
            "        -0.07217034,  0.0636403 ],\n",
            "       ...,\n",
            "       [-0.05421976, -0.02185159, -0.01851594, ...,  0.02782854,\n",
            "         0.04012462,  0.10788084],\n",
            "       [-0.07672476, -0.0993299 , -0.12671821, ...,  0.00513432,\n",
            "         0.11383209, -0.04214533],\n",
            "       [-0.1315008 ,  0.0083369 , -0.10962225, ...,  0.09549709,\n",
            "        -0.02982576,  0.12448857]], dtype=float32)>\n",
            "<tf.Variable 'tfgpt2lm_head_model/transformer/h_._6/mlp/c_proj/weight:0' shape=(3072, 768) dtype=float32, numpy=\n",
            "array([[ 0.09656826, -0.09633456,  0.06502675, ..., -0.01387259,\n",
            "         0.06517882, -0.10834515],\n",
            "       [ 0.04606632,  0.12807514, -0.04150132, ...,  0.21167706,\n",
            "         0.33303022,  0.02086084],\n",
            "       [-0.07277907, -0.06277147,  0.0046153 , ..., -0.15757011,\n",
            "         0.02596385,  0.11306138],\n",
            "       ...,\n",
            "       [ 0.10707928,  0.05193875, -0.03568225, ...,  0.1998204 ,\n",
            "         0.18108246, -0.08629364],\n",
            "       [-0.02207571,  0.06200051, -0.10674077, ...,  0.09004647,\n",
            "        -0.2254228 ,  0.06587723],\n",
            "       [ 0.24717812,  0.02542589,  0.03041094, ...,  0.03517671,\n",
            "        -0.07159096, -0.16031   ]], dtype=float32)>\n",
            "<tf.Variable 'tfgpt2lm_head_model/transformer/h_._7/mlp/c_proj/weight:0' shape=(3072, 768) dtype=float32, numpy=\n",
            "array([[ 0.02264274, -0.03599349,  0.10797467, ..., -0.09460898,\n",
            "         0.12323809,  0.01291635],\n",
            "       [-0.02132343, -0.09943391, -0.02568838, ...,  0.21332857,\n",
            "         0.01209327, -0.1258218 ],\n",
            "       [-0.20264234,  0.06211682,  0.31254783, ..., -0.1408497 ,\n",
            "        -0.07243747,  0.05390531],\n",
            "       ...,\n",
            "       [ 0.08921878, -0.14655916, -0.08188278, ...,  0.10222121,\n",
            "         0.03777475,  0.03035112],\n",
            "       [ 0.02178272, -0.13303214, -0.00249092, ...,  0.08116213,\n",
            "        -0.10116148, -0.08025869],\n",
            "       [ 0.00106471,  0.03692944, -0.01530966, ..., -0.06761301,\n",
            "         0.01219872, -0.03881805]], dtype=float32)>\n",
            "<tf.Variable 'tfgpt2lm_head_model/transformer/h_._8/mlp/c_proj/weight:0' shape=(3072, 768) dtype=float32, numpy=\n",
            "array([[-0.0888536 , -0.01912015, -0.05610085, ...,  0.22685342,\n",
            "        -0.07102828,  0.24980469],\n",
            "       [-0.20851193, -0.12896053, -0.07763953, ...,  0.04200519,\n",
            "         0.10062822,  0.0164259 ],\n",
            "       [-0.18099259,  0.15953618,  0.10691606, ..., -0.03598038,\n",
            "         0.2723562 ,  0.09517843],\n",
            "       ...,\n",
            "       [ 0.11789965,  0.11151859, -0.16609772, ..., -0.0924228 ,\n",
            "        -0.08692707,  0.0027187 ],\n",
            "       [ 0.11551704, -0.18893854,  0.13251995, ...,  0.004259  ,\n",
            "        -0.01438979, -0.00447199],\n",
            "       [-0.07456315, -0.2047861 ,  0.05288561, ..., -0.06612562,\n",
            "        -0.1876628 , -0.05006231]], dtype=float32)>\n",
            "<tf.Variable 'tfgpt2lm_head_model/transformer/h_._9/mlp/c_proj/weight:0' shape=(3072, 768) dtype=float32, numpy=\n",
            "array([[ 0.27937692, -0.04261704, -0.17971188, ..., -0.02572536,\n",
            "        -0.01071734,  0.10496436],\n",
            "       [-0.11658345, -0.10087252, -0.18096453, ..., -0.51259094,\n",
            "        -0.01599623, -0.39708775],\n",
            "       [-0.04181359,  0.15629546, -0.1174107 , ..., -0.2811517 ,\n",
            "         0.19627069, -0.16386089],\n",
            "       ...,\n",
            "       [-0.19930845, -0.07058249,  0.02880447, ..., -0.10987945,\n",
            "         0.09794372, -0.18420365],\n",
            "       [-0.04564386,  0.17832345, -0.05255263, ...,  0.13836677,\n",
            "        -0.17079422, -0.00605511],\n",
            "       [ 0.18092138,  0.11931177, -0.16888972, ..., -0.1464643 ,\n",
            "        -0.10348847,  0.01418374]], dtype=float32)>\n",
            "<tf.Variable 'tfgpt2lm_head_model/transformer/h_._10/mlp/c_proj/weight:0' shape=(3072, 768) dtype=float32, numpy=\n",
            "array([[ 0.0544838 ,  0.20381473, -0.22983   , ...,  0.02762095,\n",
            "         0.06231604,  0.08485523],\n",
            "       [ 0.16491841, -0.20410599,  0.01822783, ..., -0.08994278,\n",
            "        -0.0285466 , -0.16263679],\n",
            "       [ 0.1342364 ,  0.11233464, -0.03328435, ...,  0.06572494,\n",
            "         0.06441467, -0.15386058],\n",
            "       ...,\n",
            "       [ 0.40369612,  0.216426  , -0.13686733, ...,  0.17330717,\n",
            "         0.00686518,  0.10386262],\n",
            "       [ 0.05166091,  0.09496337, -0.47874716, ..., -0.0026667 ,\n",
            "        -0.10534672,  0.11970413],\n",
            "       [ 0.18038704, -0.1297618 ,  0.14018275, ...,  0.04444762,\n",
            "         0.24482979, -0.16736671]], dtype=float32)>\n",
            "<tf.Variable 'tfgpt2lm_head_model/transformer/h_._11/mlp/c_proj/weight:0' shape=(3072, 768) dtype=float32, numpy=\n",
            "array([[ 0.06994224,  0.09164011, -0.0322468 , ..., -0.01936196,\n",
            "         0.0526397 ,  0.08270326],\n",
            "       [ 0.17096087,  0.0076067 ,  0.11295437, ..., -0.23873799,\n",
            "         0.01896575, -0.1059324 ],\n",
            "       [-0.02801954,  0.23290561, -0.06370908, ...,  0.03692453,\n",
            "        -0.15519764,  0.03622181],\n",
            "       ...,\n",
            "       [ 0.15329406,  0.12164909,  0.03110475, ..., -0.14413482,\n",
            "        -0.2425633 , -0.13593116],\n",
            "       [ 0.00783435, -0.08154643,  0.04876423, ...,  0.15811516,\n",
            "        -0.05199301,  0.07204337],\n",
            "       [ 0.05541262,  0.10599843, -0.11731609, ..., -0.26282227,\n",
            "         0.12192673,  0.01051193]], dtype=float32)>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# input of EX1. hidden_states  # (layers, examples, seq, embedding dimension)\n",
        "gpt_last_token = hidden_states[-1][0][-1]\n",
        "\n",
        "def compute_cosine_similarities(gpt_last_token, ffn_weights):\n",
        "    # Normalize the CLS token output\n",
        "    cls_norm = np.linalg.norm(gpt_last_token)\n",
        "    cls_token_normalized = gpt_last_token / cls_norm\n",
        "\n",
        "    cosine_similarities = []\n",
        "\n",
        "    for layer_weights in ffn_weights:\n",
        "        # Transpose the weights to align dimensions with CLS token output\n",
        "        # layer_weights shape is (3072, 768), after transpose it will be (768, 3072)\n",
        "        transposed_weights = layer_weights.T\n",
        "\n",
        "        # Normalize the neuron weights\n",
        "        neuron_norms = np.linalg.norm(transposed_weights, axis=0)\n",
        "        normalized_neurons = transposed_weights / neuron_norms\n",
        "\n",
        "        # Compute the dot product\n",
        "        dot_product = np.dot(cls_token_normalized, normalized_neurons)\n",
        "\n",
        "        cosine_similarities.append(dot_product)\n",
        "\n",
        "    return cosine_similarities\n",
        "\n",
        "#'gpt_last_token' is the last token of final transformer layer . shape of (768,)\n",
        "# And 'ffn_weights' is a list of arrays, each of shape (3072, 768)\n",
        "cosine_similarities_neurons = compute_cosine_similarities(gpt_last_token, ffn_weights)\n",
        "\n",
        "print(len(cosine_similarities_neurons))\n",
        "print(cosine_similarities_neurons[0].shape)\n",
        "\n",
        "#12 layers, 3072 neurons\n",
        "#a list of 12 [] and each has 3072 in length"
      ],
      "metadata": {
        "id": "Wno3EbEjen-p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "431affd2-0ae8-4719-c119-7a657708b6db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12\n",
            "(3072,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Zero out weights strategically\n",
        "\n",
        "\"\"\"\n",
        "Based on the decoder-based gpt's nature of information delivery, zeroing-out bar for first 5 layers are higher (according to layer similarity) and gradually getting lower in the following layers.\n",
        "As for the layer 11, since it is the decision-making layer, we can zero out most neurons except preseved ones.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "-6gQan6KfTZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#input 0, layer 1, first layer of transfomrers\n",
        "for layer in range(1, 13):\n",
        "    print(cosine_similarities_layer[5][layer])\n",
        "\n",
        "#try to come up with an equation for layer similarity and a bar for neurons to be removed."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uG5pSA2hine",
        "outputId": "01a4ce78-fe4e-4a80-96e0-8866a3c37d7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.08073993027210236\n",
            "0.08839960396289825\n",
            "0.09865099936723709\n",
            "0.10909189283847809\n",
            "0.10344599932432175\n",
            "0.11567986756563187\n",
            "0.13689970970153809\n",
            "0.1637384593486786\n",
            "0.19258950650691986\n",
            "0.18796499073505402\n",
            "0.19811399281024933\n",
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#input 0, layer 1, first layer of transfomrers\n",
        "#output: masked_neurons_list\n",
        "#input: cosine_similarities_neurons, cosine_similarities_layer, alpha\n",
        "#if the last token of layer N is close to last layer, it means Nth layer get the right information to make the call and neruons in this layer can handle the task properly,\n",
        "#As a result, neurons having low simiarlarity in this layer can be removed. The bar can be slightly higher\n",
        "#cosine similarity can be negative, cos_sin = 0 irrelavent, -1~0, 0~1\n",
        "\n",
        "def pruning_strategy(cosine_similarities_neurons, cosine_similarities_layer, alpha):\n",
        "    masked_neurons_list = [[] for i in range(12)]\n",
        "    for layer in range(5):\n",
        "        for num, simi in enumerate(cosine_similarities_neurons[layer]):\n",
        "            if cosine_similarities_layer[5][layer]*alpha > simi > -cosine_similarities_layer[5][layer]*alpha:\n",
        "                masked_neurons_list[layer].append(num)\n",
        "\n",
        "        print(f\"Threshold of pruning {cosine_similarities_layer[5][layer]*alpha}\")\n",
        "        print(f\"number of neruons being masked in layer {layer}: {len(masked_neurons_list[layer])}\")\n",
        "\n",
        "    #layer 11\n",
        "    for layer in range(5,12):\n",
        "        for num, simi in enumerate(cosine_similarities_neurons[layer]):\n",
        "            if simi < 3*cosine_similarities_layer[5][layer]*alpha:\n",
        "                masked_neurons_list[layer].append(num)\n",
        "        print(f\"Threshold of pruning {cosine_similarities_layer[5][layer]*alpha}\")\n",
        "        print(f\"number of neruons being masked in layer {layer}: {len(masked_neurons_list[layer])}\")\n",
        "\n",
        "    return masked_neurons_list\n",
        "\n",
        "masked_neurons_list = pruning_strategy(cosine_similarities_neurons, cosine_similarities_layer, alpha=0.15)\n",
        "print(masked_neurons_list)"
      ],
      "metadata": {
        "id": "eT1rvNh5ij1t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78db27c8-dc19-4be9-c840-434f44cb9403"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshold of pruning -0.02292773276567459\n",
            "number of neruons being masked in layer 0: 0\n",
            "Threshold of pruning 0.005978638306260109\n",
            "number of neruons being masked in layer 1: 353\n",
            "Threshold of pruning 0.006234094686806202\n",
            "number of neruons being masked in layer 2: 404\n",
            "Threshold of pruning 0.00878318902105093\n",
            "number of neruons being masked in layer 3: 532\n",
            "Threshold of pruning 0.01154554970562458\n",
            "number of neruons being masked in layer 4: 724\n",
            "Threshold of pruning 0.012514302134513855\n",
            "number of neruons being masked in layer 5: 2537\n",
            "Threshold of pruning 0.014355476573109626\n",
            "number of neruons being masked in layer 6: 2614\n",
            "Threshold of pruning 0.018847954273223878\n",
            "number of neruons being masked in layer 7: 2791\n",
            "Threshold of pruning 0.02213795632123947\n",
            "number of neruons being masked in layer 8: 2858\n",
            "Threshold of pruning 0.01988786235451698\n",
            "number of neruons being masked in layer 9: 2723\n",
            "Threshold of pruning 0.0183973990380764\n",
            "number of neruons being masked in layer 10: 2388\n",
            "Threshold of pruning 0.022244922816753387\n",
            "number of neruons being masked in layer 11: 2123\n",
            "[[], [11, 16, 22, 26, 40, 43, 66, 67, 79, 110, 112, 120, 137, 140, 152, 154, 157, 160, 162, 163, 177, 200, 204, 217, 219, 225, 263, 267, 272, 273, 280, 287, 300, 308, 309, 310, 316, 330, 336, 355, 374, 378, 401, 406, 411, 412, 423, 427, 429, 443, 451, 456, 467, 476, 502, 506, 507, 515, 517, 525, 531, 533, 543, 550, 551, 552, 553, 570, 583, 590, 597, 609, 617, 635, 659, 664, 676, 704, 708, 724, 732, 736, 751, 762, 786, 787, 790, 800, 801, 810, 812, 813, 816, 845, 853, 864, 879, 891, 894, 895, 900, 902, 915, 919, 924, 938, 957, 965, 967, 982, 997, 1001, 1025, 1043, 1044, 1046, 1060, 1085, 1089, 1109, 1116, 1119, 1137, 1152, 1158, 1169, 1181, 1184, 1192, 1198, 1204, 1226, 1228, 1236, 1263, 1273, 1280, 1297, 1303, 1312, 1314, 1320, 1324, 1325, 1346, 1349, 1361, 1392, 1409, 1422, 1433, 1447, 1452, 1471, 1472, 1477, 1483, 1531, 1543, 1555, 1558, 1560, 1563, 1565, 1574, 1576, 1577, 1578, 1579, 1582, 1584, 1598, 1612, 1617, 1620, 1622, 1623, 1624, 1625, 1627, 1630, 1636, 1637, 1638, 1640, 1650, 1658, 1666, 1690, 1692, 1698, 1711, 1713, 1727, 1734, 1747, 1750, 1751, 1765, 1766, 1783, 1799, 1800, 1802, 1812, 1816, 1819, 1827, 1836, 1844, 1848, 1855, 1862, 1863, 1869, 1874, 1879, 1886, 1893, 1897, 1908, 1914, 1917, 1942, 1945, 1958, 1959, 1969, 1973, 2007, 2023, 2033, 2035, 2043, 2045, 2063, 2068, 2071, 2084, 2089, 2110, 2111, 2123, 2126, 2138, 2143, 2148, 2156, 2176, 2196, 2223, 2235, 2244, 2245, 2265, 2266, 2269, 2272, 2279, 2289, 2295, 2297, 2304, 2307, 2311, 2313, 2315, 2325, 2339, 2352, 2355, 2364, 2374, 2399, 2408, 2444, 2456, 2460, 2461, 2466, 2477, 2479, 2484, 2486, 2546, 2560, 2569, 2583, 2585, 2596, 2603, 2609, 2616, 2630, 2636, 2639, 2646, 2648, 2651, 2676, 2679, 2687, 2701, 2713, 2716, 2723, 2724, 2738, 2742, 2751, 2765, 2767, 2779, 2797, 2804, 2821, 2822, 2824, 2839, 2841, 2845, 2851, 2853, 2860, 2861, 2863, 2884, 2886, 2894, 2922, 2924, 2931, 2939, 2945, 2956, 2965, 2974, 2988, 2989, 2990, 2996, 2998, 3008, 3015, 3021, 3023, 3031, 3039, 3047, 3051, 3055, 3056, 3061], [2, 11, 23, 26, 27, 33, 48, 50, 57, 64, 68, 79, 82, 98, 105, 109, 123, 126, 128, 132, 135, 144, 160, 164, 169, 178, 182, 185, 192, 205, 210, 212, 221, 223, 235, 240, 250, 252, 259, 270, 273, 275, 279, 283, 286, 290, 292, 299, 300, 305, 314, 317, 330, 355, 356, 357, 360, 361, 364, 365, 367, 387, 388, 401, 418, 424, 439, 440, 446, 459, 463, 466, 470, 475, 483, 492, 496, 502, 503, 505, 516, 530, 536, 544, 552, 565, 568, 569, 570, 575, 577, 585, 600, 607, 608, 636, 643, 651, 657, 660, 672, 675, 690, 692, 708, 712, 713, 721, 725, 740, 755, 756, 763, 773, 774, 780, 799, 801, 830, 840, 849, 856, 858, 868, 881, 899, 900, 905, 923, 935, 947, 948, 949, 952, 960, 975, 988, 995, 1030, 1031, 1032, 1033, 1044, 1054, 1064, 1070, 1076, 1103, 1104, 1109, 1112, 1114, 1126, 1128, 1131, 1135, 1148, 1149, 1181, 1184, 1211, 1215, 1217, 1223, 1226, 1233, 1242, 1248, 1256, 1264, 1272, 1291, 1297, 1301, 1302, 1313, 1316, 1327, 1330, 1336, 1343, 1345, 1348, 1382, 1387, 1391, 1399, 1403, 1404, 1406, 1450, 1470, 1498, 1500, 1502, 1507, 1513, 1520, 1527, 1552, 1554, 1560, 1561, 1568, 1573, 1576, 1586, 1601, 1635, 1637, 1643, 1652, 1656, 1660, 1669, 1676, 1679, 1686, 1687, 1689, 1692, 1700, 1706, 1707, 1714, 1726, 1727, 1729, 1746, 1747, 1748, 1762, 1772, 1782, 1796, 1799, 1813, 1830, 1846, 1850, 1856, 1872, 1873, 1883, 1885, 1886, 1897, 1901, 1905, 1908, 1920, 1928, 1964, 1978, 1988, 1989, 2000, 2008, 2013, 2021, 2023, 2026, 2029, 2044, 2046, 2083, 2084, 2093, 2099, 2103, 2105, 2114, 2121, 2123, 2126, 2133, 2150, 2157, 2171, 2184, 2208, 2225, 2241, 2246, 2254, 2268, 2282, 2283, 2298, 2312, 2323, 2333, 2341, 2342, 2346, 2352, 2353, 2359, 2363, 2369, 2374, 2383, 2388, 2392, 2404, 2415, 2428, 2437, 2440, 2444, 2448, 2457, 2458, 2466, 2479, 2494, 2503, 2509, 2516, 2529, 2535, 2545, 2550, 2553, 2560, 2571, 2578, 2585, 2599, 2608, 2617, 2618, 2624, 2633, 2648, 2651, 2663, 2667, 2670, 2673, 2678, 2681, 2697, 2705, 2709, 2734, 2739, 2743, 2749, 2752, 2753, 2757, 2758, 2761, 2768, 2776, 2779, 2782, 2784, 2786, 2802, 2818, 2820, 2828, 2836, 2837, 2839, 2847, 2848, 2860, 2869, 2875, 2877, 2884, 2890, 2893, 2900, 2913, 2918, 2919, 2920, 2923, 2932, 2936, 2940, 2948, 2950, 2960, 2967, 2976, 2979, 2980, 2986, 2990, 2992, 2999, 3001, 3003, 3007, 3025, 3035, 3038, 3057, 3060], [8, 18, 23, 37, 39, 45, 49, 52, 60, 64, 65, 67, 68, 69, 85, 90, 91, 95, 96, 97, 98, 99, 106, 107, 110, 113, 114, 121, 139, 155, 168, 169, 187, 196, 198, 199, 202, 208, 224, 229, 233, 234, 235, 239, 259, 274, 285, 286, 287, 289, 290, 297, 308, 317, 320, 321, 322, 323, 334, 341, 349, 351, 352, 359, 360, 365, 376, 390, 396, 403, 411, 423, 424, 429, 431, 435, 439, 449, 456, 458, 468, 469, 472, 483, 487, 496, 513, 514, 524, 537, 538, 540, 545, 549, 550, 552, 555, 572, 575, 578, 589, 590, 591, 594, 598, 609, 614, 617, 619, 626, 630, 632, 641, 644, 645, 651, 657, 658, 660, 666, 669, 674, 685, 688, 700, 702, 713, 717, 723, 729, 740, 743, 745, 747, 749, 752, 753, 760, 762, 764, 767, 774, 785, 791, 797, 804, 840, 843, 853, 857, 867, 869, 881, 887, 897, 916, 919, 920, 921, 933, 935, 940, 946, 953, 961, 964, 976, 978, 985, 986, 989, 994, 1012, 1016, 1020, 1033, 1035, 1039, 1049, 1058, 1059, 1061, 1065, 1073, 1075, 1077, 1078, 1081, 1084, 1093, 1094, 1096, 1099, 1109, 1117, 1123, 1128, 1129, 1137, 1146, 1149, 1155, 1157, 1159, 1160, 1167, 1173, 1179, 1182, 1188, 1189, 1190, 1192, 1195, 1197, 1201, 1205, 1206, 1209, 1234, 1237, 1242, 1243, 1245, 1247, 1252, 1255, 1261, 1270, 1273, 1274, 1275, 1278, 1280, 1284, 1299, 1302, 1309, 1315, 1325, 1327, 1329, 1331, 1348, 1349, 1350, 1354, 1360, 1363, 1378, 1385, 1389, 1395, 1398, 1403, 1404, 1407, 1414, 1415, 1420, 1422, 1426, 1434, 1438, 1448, 1449, 1450, 1452, 1459, 1463, 1465, 1481, 1488, 1501, 1508, 1527, 1538, 1560, 1567, 1572, 1574, 1579, 1614, 1615, 1621, 1623, 1625, 1632, 1640, 1644, 1652, 1655, 1666, 1678, 1684, 1686, 1692, 1709, 1710, 1715, 1721, 1722, 1730, 1748, 1750, 1752, 1756, 1759, 1761, 1773, 1778, 1781, 1784, 1788, 1813, 1819, 1836, 1856, 1860, 1861, 1862, 1869, 1876, 1886, 1897, 1902, 1916, 1928, 1963, 1964, 1966, 1968, 1976, 1982, 1985, 1991, 1993, 2002, 2013, 2020, 2023, 2027, 2028, 2031, 2032, 2046, 2049, 2052, 2066, 2076, 2085, 2089, 2092, 2093, 2094, 2097, 2101, 2103, 2105, 2109, 2110, 2123, 2127, 2131, 2139, 2143, 2145, 2159, 2161, 2168, 2173, 2177, 2181, 2186, 2189, 2190, 2191, 2192, 2201, 2205, 2206, 2212, 2213, 2217, 2223, 2240, 2247, 2258, 2262, 2263, 2266, 2268, 2271, 2273, 2279, 2280, 2282, 2286, 2325, 2326, 2333, 2335, 2338, 2339, 2345, 2353, 2354, 2355, 2357, 2360, 2373, 2375, 2377, 2385, 2386, 2389, 2404, 2405, 2406, 2407, 2413, 2418, 2436, 2438, 2440, 2448, 2455, 2464, 2469, 2472, 2485, 2492, 2493, 2494, 2507, 2509, 2512, 2513, 2515, 2526, 2534, 2535, 2548, 2552, 2558, 2563, 2576, 2578, 2586, 2589, 2593, 2594, 2595, 2599, 2602, 2614, 2622, 2628, 2630, 2635, 2645, 2650, 2655, 2656, 2658, 2661, 2662, 2667, 2668, 2670, 2674, 2685, 2686, 2689, 2693, 2711, 2716, 2722, 2725, 2727, 2729, 2731, 2734, 2739, 2741, 2751, 2761, 2766, 2767, 2772, 2776, 2778, 2781, 2788, 2790, 2806, 2807, 2810, 2811, 2819, 2827, 2832, 2845, 2847, 2849, 2862, 2894, 2899, 2908, 2910, 2911, 2913, 2931, 2933, 2943, 2961, 2965, 2984, 2987, 2995, 3000, 3002, 3005, 3022, 3027, 3044, 3046, 3050, 3051, 3061, 3067, 3068], [0, 2, 4, 6, 8, 11, 13, 18, 22, 27, 34, 47, 52, 57, 58, 61, 62, 65, 70, 76, 84, 87, 90, 91, 97, 98, 100, 101, 104, 107, 109, 111, 116, 117, 118, 123, 133, 134, 140, 141, 145, 146, 153, 160, 166, 173, 174, 179, 201, 202, 203, 210, 218, 225, 230, 231, 235, 240, 242, 245, 248, 266, 269, 271, 281, 282, 285, 292, 293, 294, 296, 297, 301, 309, 313, 314, 320, 324, 326, 328, 338, 353, 355, 359, 362, 364, 369, 370, 373, 379, 381, 385, 390, 392, 400, 401, 403, 405, 408, 411, 412, 414, 415, 422, 426, 433, 435, 441, 448, 449, 458, 461, 474, 475, 481, 485, 492, 497, 501, 510, 523, 528, 529, 534, 536, 537, 539, 542, 547, 563, 565, 566, 567, 568, 569, 570, 571, 572, 575, 577, 579, 585, 606, 618, 619, 620, 623, 629, 636, 640, 642, 655, 658, 659, 660, 664, 665, 672, 679, 681, 694, 696, 700, 703, 708, 720, 724, 725, 728, 731, 735, 737, 751, 754, 757, 766, 768, 773, 779, 780, 785, 790, 792, 793, 797, 804, 808, 812, 819, 821, 826, 828, 833, 841, 842, 843, 844, 846, 849, 852, 853, 862, 885, 889, 896, 899, 900, 903, 905, 911, 913, 917, 925, 928, 930, 932, 933, 936, 941, 947, 954, 956, 957, 966, 968, 972, 975, 978, 984, 992, 994, 997, 998, 1001, 1016, 1018, 1020, 1024, 1031, 1032, 1041, 1042, 1044, 1045, 1046, 1049, 1052, 1055, 1058, 1062, 1067, 1078, 1082, 1091, 1093, 1096, 1099, 1108, 1110, 1112, 1114, 1115, 1118, 1119, 1122, 1132, 1138, 1139, 1149, 1162, 1181, 1187, 1192, 1198, 1208, 1209, 1212, 1215, 1223, 1224, 1228, 1251, 1254, 1256, 1260, 1270, 1272, 1278, 1280, 1290, 1291, 1294, 1297, 1300, 1301, 1311, 1312, 1316, 1318, 1334, 1339, 1340, 1341, 1342, 1343, 1347, 1355, 1358, 1363, 1365, 1373, 1374, 1375, 1377, 1380, 1382, 1388, 1389, 1392, 1403, 1409, 1410, 1412, 1417, 1421, 1422, 1424, 1427, 1429, 1436, 1442, 1443, 1444, 1447, 1455, 1457, 1458, 1462, 1465, 1470, 1472, 1474, 1475, 1486, 1489, 1490, 1491, 1492, 1494, 1496, 1500, 1501, 1509, 1511, 1519, 1521, 1524, 1526, 1529, 1539, 1552, 1554, 1556, 1557, 1560, 1576, 1583, 1588, 1589, 1591, 1595, 1602, 1606, 1609, 1617, 1619, 1622, 1623, 1624, 1626, 1627, 1635, 1642, 1644, 1648, 1653, 1664, 1666, 1672, 1674, 1676, 1681, 1683, 1684, 1688, 1691, 1693, 1695, 1697, 1704, 1724, 1727, 1728, 1734, 1735, 1747, 1748, 1754, 1755, 1762, 1765, 1771, 1774, 1775, 1777, 1783, 1785, 1793, 1798, 1800, 1801, 1804, 1820, 1821, 1822, 1827, 1833, 1837, 1845, 1847, 1848, 1850, 1851, 1857, 1858, 1860, 1864, 1867, 1874, 1884, 1885, 1888, 1889, 1891, 1894, 1901, 1903, 1904, 1911, 1917, 1920, 1923, 1927, 1931, 1933, 1935, 1937, 1943, 1945, 1946, 1948, 1953, 1954, 1956, 1957, 1960, 1965, 1975, 1982, 1983, 1984, 1993, 2001, 2002, 2004, 2019, 2023, 2024, 2027, 2037, 2045, 2046, 2048, 2050, 2054, 2060, 2061, 2063, 2064, 2067, 2069, 2071, 2075, 2079, 2085, 2087, 2094, 2096, 2099, 2103, 2104, 2107, 2121, 2129, 2131, 2132, 2143, 2148, 2153, 2157, 2168, 2172, 2173, 2178, 2179, 2182, 2184, 2189, 2193, 2194, 2195, 2205, 2209, 2210, 2213, 2215, 2225, 2229, 2234, 2238, 2243, 2248, 2253, 2260, 2267, 2269, 2271, 2284, 2294, 2296, 2299, 2304, 2306, 2312, 2316, 2317, 2318, 2323, 2328, 2329, 2333, 2334, 2339, 2343, 2347, 2356, 2359, 2367, 2371, 2372, 2380, 2385, 2389, 2393, 2398, 2402, 2405, 2407, 2411, 2413, 2415, 2416, 2417, 2422, 2427, 2429, 2431, 2436, 2447, 2452, 2455, 2456, 2457, 2458, 2459, 2466, 2467, 2472, 2473, 2476, 2483, 2486, 2488, 2494, 2505, 2506, 2508, 2514, 2515, 2516, 2519, 2522, 2524, 2533, 2537, 2540, 2543, 2544, 2551, 2556, 2561, 2563, 2568, 2572, 2592, 2599, 2600, 2615, 2617, 2624, 2627, 2635, 2636, 2638, 2640, 2647, 2649, 2651, 2660, 2668, 2679, 2686, 2690, 2695, 2702, 2706, 2707, 2708, 2711, 2716, 2725, 2726, 2729, 2733, 2736, 2741, 2743, 2745, 2748, 2749, 2754, 2758, 2766, 2767, 2770, 2771, 2772, 2773, 2779, 2782, 2783, 2787, 2791, 2798, 2802, 2813, 2814, 2820, 2832, 2844, 2845, 2847, 2859, 2864, 2868, 2873, 2893, 2896, 2898, 2902, 2903, 2904, 2915, 2919, 2924, 2929, 2930, 2932, 2934, 2942, 2956, 2960, 2965, 2966, 2973, 2974, 2975, 2977, 2979, 2986, 2988, 2989, 2992, 2998, 3001, 3007, 3012, 3014, 3020, 3026, 3027, 3028, 3034, 3043, 3044, 3045, 3046, 3047, 3048, 3051, 3053, 3056, 3063, 3065], [0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 23, 24, 25, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 38, 39, 40, 45, 48, 49, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 67, 68, 71, 72, 73, 74, 75, 76, 77, 79, 81, 82, 83, 84, 85, 86, 87, 89, 90, 92, 93, 94, 97, 98, 100, 101, 102, 103, 104, 105, 107, 108, 110, 111, 113, 114, 115, 116, 117, 118, 119, 120, 124, 125, 126, 127, 128, 129, 130, 131, 133, 136, 137, 138, 139, 142, 143, 144, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 160, 165, 166, 168, 169, 170, 171, 172, 173, 175, 176, 177, 179, 180, 181, 182, 183, 184, 186, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 199, 200, 201, 202, 203, 204, 206, 207, 209, 210, 211, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 226, 227, 228, 229, 230, 232, 233, 234, 235, 237, 239, 240, 242, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 258, 259, 260, 261, 263, 264, 265, 266, 268, 269, 270, 272, 273, 274, 275, 276, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 290, 291, 292, 293, 294, 295, 296, 297, 298, 300, 302, 303, 304, 305, 306, 307, 308, 311, 312, 313, 314, 316, 317, 318, 319, 320, 321, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 345, 347, 348, 349, 350, 352, 353, 354, 355, 356, 357, 358, 359, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 372, 373, 375, 376, 377, 378, 380, 381, 384, 385, 387, 388, 389, 391, 392, 394, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 413, 414, 415, 416, 417, 418, 419, 421, 422, 423, 424, 425, 428, 429, 430, 431, 432, 433, 434, 436, 437, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 452, 453, 454, 456, 458, 459, 461, 462, 463, 465, 466, 467, 468, 470, 471, 472, 473, 474, 475, 476, 477, 478, 480, 481, 482, 483, 484, 485, 486, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 519, 520, 521, 522, 524, 525, 526, 527, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 541, 542, 543, 544, 546, 547, 548, 551, 552, 553, 554, 555, 556, 557, 558, 559, 561, 562, 563, 565, 566, 567, 568, 569, 570, 571, 573, 574, 575, 576, 578, 580, 581, 582, 583, 585, 586, 587, 588, 589, 590, 592, 593, 594, 595, 596, 597, 598, 600, 602, 603, 604, 605, 606, 607, 608, 609, 610, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 652, 653, 654, 655, 657, 658, 659, 660, 661, 662, 663, 664, 667, 668, 669, 670, 671, 672, 674, 676, 677, 679, 682, 683, 685, 686, 687, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 704, 705, 706, 707, 708, 709, 710, 712, 714, 715, 716, 717, 718, 719, 720, 722, 723, 724, 725, 726, 728, 729, 730, 731, 733, 734, 736, 737, 738, 740, 741, 742, 744, 745, 746, 747, 749, 750, 751, 752, 753, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 769, 770, 772, 774, 777, 779, 780, 781, 782, 783, 784, 786, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 803, 804, 806, 807, 808, 809, 811, 813, 814, 817, 818, 819, 820, 821, 823, 825, 826, 827, 828, 829, 830, 831, 832, 834, 835, 836, 838, 839, 840, 841, 842, 843, 844, 846, 847, 848, 849, 850, 852, 854, 855, 856, 857, 858, 859, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 875, 877, 878, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 904, 905, 906, 907, 909, 910, 911, 913, 915, 916, 917, 918, 921, 922, 924, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 941, 942, 944, 945, 946, 948, 949, 950, 951, 952, 953, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 966, 967, 969, 970, 971, 973, 974, 975, 976, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 999, 1000, 1002, 1003, 1004, 1005, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1017, 1018, 1020, 1021, 1022, 1023, 1024, 1025, 1027, 1029, 1030, 1031, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1059, 1060, 1061, 1062, 1063, 1065, 1066, 1067, 1068, 1070, 1071, 1073, 1075, 1076, 1078, 1080, 1081, 1082, 1083, 1085, 1086, 1087, 1089, 1090, 1093, 1094, 1096, 1098, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1109, 1111, 1112, 1113, 1114, 1116, 1118, 1119, 1122, 1123, 1124, 1125, 1126, 1127, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1140, 1141, 1142, 1144, 1145, 1147, 1148, 1149, 1150, 1152, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1173, 1175, 1176, 1177, 1178, 1179, 1180, 1182, 1183, 1184, 1185, 1186, 1188, 1190, 1191, 1193, 1194, 1195, 1196, 1197, 1199, 1200, 1201, 1202, 1203, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1219, 1220, 1221, 1222, 1223, 1226, 1227, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1237, 1238, 1239, 1240, 1242, 1243, 1244, 1245, 1247, 1248, 1251, 1252, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1263, 1264, 1266, 1267, 1268, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1279, 1280, 1281, 1282, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1293, 1294, 1295, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1307, 1309, 1310, 1311, 1312, 1313, 1314, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1340, 1341, 1342, 1344, 1345, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1373, 1374, 1375, 1377, 1378, 1379, 1380, 1381, 1382, 1384, 1385, 1388, 1389, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1401, 1402, 1403, 1404, 1405, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1462, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1507, 1509, 1510, 1512, 1513, 1515, 1517, 1518, 1520, 1521, 1522, 1523, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545, 1546, 1547, 1548, 1549, 1550, 1551, 1552, 1554, 1555, 1557, 1558, 1559, 1562, 1563, 1564, 1565, 1566, 1567, 1568, 1569, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589, 1591, 1593, 1595, 1596, 1597, 1598, 1599, 1600, 1601, 1602, 1603, 1604, 1605, 1606, 1607, 1610, 1611, 1612, 1613, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1623, 1624, 1625, 1626, 1627, 1629, 1630, 1631, 1632, 1633, 1634, 1635, 1636, 1637, 1638, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650, 1651, 1654, 1657, 1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1690, 1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701, 1702, 1703, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1712, 1713, 1714, 1715, 1717, 1718, 1719, 1720, 1721, 1723, 1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1739, 1740, 1741, 1742, 1744, 1746, 1747, 1748, 1749, 1750, 1751, 1753, 1756, 1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1768, 1769, 1770, 1772, 1773, 1774, 1775, 1776, 1777, 1779, 1780, 1782, 1783, 1784, 1785, 1786, 1787, 1789, 1790, 1791, 1792, 1793, 1794, 1797, 1799, 1800, 1801, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819, 1820, 1822, 1823, 1825, 1826, 1827, 1828, 1829, 1830, 1833, 1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841, 1843, 1844, 1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1859, 1860, 1861, 1863, 1864, 1865, 1866, 1868, 1869, 1870, 1871, 1872, 1874, 1876, 1877, 1878, 1879, 1880, 1881, 1882, 1883, 1884, 1885, 1886, 1887, 1888, 1889, 1890, 1892, 1893, 1894, 1895, 1899, 1900, 1901, 1902, 1904, 1907, 1908, 1910, 1911, 1912, 1913, 1914, 1915, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1925, 1926, 1927, 1929, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949, 1951, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1962, 1963, 1964, 1965, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2007, 2008, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2026, 2028, 2029, 2030, 2031, 2032, 2035, 2036, 2038, 2039, 2040, 2042, 2044, 2045, 2046, 2047, 2048, 2049, 2050, 2051, 2052, 2054, 2055, 2056, 2057, 2058, 2059, 2060, 2061, 2063, 2064, 2065, 2066, 2067, 2069, 2070, 2071, 2072, 2073, 2074, 2076, 2077, 2078, 2080, 2081, 2082, 2084, 2085, 2086, 2087, 2089, 2090, 2091, 2092, 2093, 2094, 2095, 2096, 2097, 2098, 2099, 2100, 2101, 2102, 2104, 2105, 2106, 2107, 2108, 2109, 2110, 2111, 2112, 2113, 2114, 2117, 2119, 2120, 2121, 2122, 2123, 2125, 2126, 2127, 2128, 2129, 2130, 2131, 2132, 2133, 2134, 2135, 2136, 2137, 2140, 2141, 2142, 2143, 2144, 2145, 2146, 2147, 2148, 2149, 2150, 2151, 2152, 2153, 2154, 2156, 2157, 2158, 2159, 2160, 2161, 2162, 2163, 2164, 2166, 2167, 2168, 2169, 2171, 2172, 2173, 2175, 2176, 2177, 2179, 2180, 2181, 2182, 2183, 2184, 2185, 2186, 2187, 2188, 2189, 2190, 2192, 2193, 2194, 2195, 2196, 2197, 2198, 2199, 2200, 2201, 2204, 2205, 2206, 2207, 2208, 2209, 2210, 2211, 2212, 2213, 2214, 2215, 2216, 2218, 2219, 2220, 2221, 2222, 2223, 2224, 2225, 2226, 2227, 2228, 2229, 2230, 2231, 2232, 2233, 2234, 2235, 2237, 2238, 2241, 2242, 2243, 2245, 2246, 2248, 2249, 2250, 2251, 2252, 2254, 2255, 2256, 2257, 2258, 2259, 2260, 2261, 2262, 2263, 2265, 2266, 2267, 2268, 2269, 2270, 2272, 2274, 2275, 2276, 2280, 2281, 2282, 2283, 2284, 2285, 2287, 2288, 2289, 2290, 2291, 2292, 2293, 2294, 2295, 2296, 2298, 2300, 2302, 2303, 2304, 2305, 2307, 2308, 2309, 2310, 2311, 2312, 2313, 2314, 2315, 2316, 2317, 2318, 2319, 2320, 2322, 2323, 2324, 2325, 2326, 2327, 2328, 2329, 2330, 2332, 2333, 2335, 2336, 2337, 2339, 2340, 2342, 2343, 2344, 2345, 2347, 2348, 2349, 2350, 2351, 2353, 2354, 2356, 2357, 2358, 2359, 2360, 2361, 2362, 2363, 2364, 2367, 2368, 2369, 2370, 2371, 2372, 2373, 2374, 2375, 2376, 2378, 2381, 2382, 2383, 2384, 2385, 2386, 2387, 2388, 2389, 2391, 2392, 2393, 2394, 2395, 2396, 2397, 2398, 2399, 2400, 2401, 2402, 2405, 2406, 2407, 2408, 2410, 2411, 2412, 2415, 2416, 2417, 2418, 2419, 2420, 2422, 2423, 2427, 2428, 2429, 2430, 2431, 2432, 2433, 2436, 2437, 2438, 2439, 2440, 2441, 2442, 2443, 2444, 2445, 2446, 2447, 2448, 2449, 2450, 2451, 2452, 2453, 2456, 2457, 2458, 2461, 2462, 2464, 2465, 2466, 2467, 2469, 2470, 2471, 2472, 2473, 2475, 2476, 2477, 2478, 2480, 2481, 2482, 2483, 2484, 2486, 2487, 2488, 2489, 2490, 2491, 2492, 2493, 2494, 2495, 2497, 2498, 2501, 2502, 2503, 2504, 2505, 2506, 2508, 2509, 2511, 2512, 2513, 2514, 2515, 2516, 2517, 2518, 2520, 2521, 2522, 2523, 2525, 2526, 2527, 2528, 2529, 2530, 2531, 2533, 2534, 2535, 2536, 2538, 2539, 2540, 2541, 2542, 2543, 2544, 2545, 2546, 2547, 2548, 2549, 2550, 2551, 2552, 2553, 2554, 2555, 2556, 2557, 2558, 2560, 2561, 2562, 2563, 2564, 2565, 2566, 2567, 2569, 2570, 2571, 2573, 2574, 2575, 2576, 2577, 2578, 2579, 2581, 2582, 2583, 2584, 2585, 2586, 2588, 2589, 2590, 2591, 2592, 2593, 2594, 2595, 2596, 2597, 2598, 2599, 2600, 2601, 2602, 2603, 2604, 2605, 2606, 2608, 2609, 2610, 2612, 2613, 2614, 2615, 2616, 2618, 2619, 2620, 2621, 2622, 2623, 2625, 2626, 2627, 2629, 2630, 2631, 2632, 2633, 2634, 2635, 2636, 2637, 2638, 2639, 2642, 2643, 2644, 2645, 2647, 2648, 2649, 2651, 2652, 2654, 2657, 2658, 2659, 2660, 2661, 2662, 2663, 2664, 2665, 2666, 2667, 2669, 2671, 2672, 2673, 2674, 2675, 2676, 2677, 2678, 2681, 2682, 2683, 2684, 2686, 2687, 2688, 2689, 2690, 2691, 2692, 2693, 2694, 2695, 2696, 2697, 2698, 2699, 2701, 2702, 2703, 2704, 2705, 2706, 2708, 2709, 2710, 2711, 2713, 2715, 2716, 2717, 2718, 2719, 2721, 2722, 2723, 2724, 2725, 2726, 2728, 2730, 2731, 2732, 2733, 2734, 2736, 2738, 2740, 2741, 2742, 2744, 2745, 2746, 2748, 2749, 2750, 2751, 2752, 2755, 2756, 2757, 2758, 2759, 2762, 2763, 2764, 2765, 2766, 2767, 2768, 2770, 2771, 2772, 2774, 2775, 2776, 2777, 2778, 2780, 2782, 2783, 2784, 2785, 2786, 2787, 2788, 2789, 2790, 2791, 2792, 2793, 2794, 2795, 2796, 2797, 2798, 2799, 2800, 2801, 2802, 2803, 2805, 2807, 2808, 2809, 2810, 2811, 2812, 2813, 2814, 2815, 2816, 2817, 2820, 2822, 2823, 2824, 2826, 2827, 2828, 2829, 2830, 2831, 2832, 2833, 2834, 2835, 2836, 2838, 2839, 2841, 2842, 2843, 2845, 2846, 2847, 2848, 2849, 2850, 2852, 2856, 2857, 2858, 2859, 2860, 2861, 2862, 2863, 2864, 2866, 2867, 2868, 2869, 2870, 2871, 2872, 2873, 2875, 2876, 2877, 2878, 2879, 2880, 2881, 2882, 2883, 2885, 2886, 2887, 2888, 2889, 2890, 2891, 2892, 2893, 2894, 2896, 2897, 2898, 2899, 2900, 2901, 2902, 2903, 2905, 2906, 2907, 2908, 2909, 2911, 2913, 2914, 2915, 2916, 2917, 2918, 2919, 2920, 2921, 2922, 2923, 2924, 2925, 2926, 2928, 2929, 2931, 2932, 2933, 2935, 2936, 2937, 2938, 2939, 2940, 2941, 2942, 2943, 2944, 2945, 2946, 2947, 2948, 2950, 2951, 2952, 2953, 2954, 2956, 2957, 2958, 2959, 2960, 2962, 2963, 2964, 2965, 2967, 2968, 2969, 2971, 2972, 2973, 2974, 2975, 2976, 2977, 2980, 2981, 2984, 2985, 2986, 2988, 2989, 2990, 2991, 2992, 2993, 2994, 2995, 2996, 2997, 2998, 2999, 3000, 3001, 3003, 3004, 3005, 3008, 3009, 3010, 3012, 3013, 3014, 3015, 3016, 3017, 3018, 3019, 3020, 3021, 3022, 3023, 3024, 3025, 3027, 3029, 3030, 3031, 3032, 3033, 3034, 3035, 3036, 3037, 3038, 3039, 3040, 3041, 3043, 3044, 3045, 3046, 3047, 3049, 3050, 3052, 3053, 3054, 3055, 3057, 3058, 3059, 3060, 3062, 3063, 3064, 3065, 3066, 3067, 3068, 3069], [0, 1, 4, 5, 6, 7, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 31, 32, 33, 36, 37, 38, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 73, 74, 75, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86, 88, 89, 90, 92, 93, 94, 95, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 115, 116, 117, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 132, 133, 134, 135, 137, 139, 140, 141, 144, 145, 146, 147, 148, 149, 150, 152, 154, 155, 156, 158, 159, 160, 161, 162, 163, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 219, 220, 221, 223, 225, 226, 227, 228, 229, 230, 231, 232, 233, 235, 236, 237, 238, 239, 240, 241, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 256, 257, 258, 259, 261, 262, 263, 264, 265, 266, 267, 268, 270, 271, 272, 273, 274, 275, 276, 277, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 292, 293, 294, 295, 296, 297, 299, 300, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 324, 325, 326, 327, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 360, 361, 363, 366, 367, 368, 369, 370, 372, 373, 375, 376, 377, 378, 379, 380, 381, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 404, 405, 406, 407, 409, 410, 411, 412, 413, 415, 418, 419, 420, 421, 422, 423, 424, 425, 427, 428, 429, 430, 431, 432, 433, 434, 435, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 450, 452, 453, 454, 455, 456, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 470, 471, 473, 474, 475, 476, 477, 478, 480, 481, 482, 483, 484, 485, 487, 488, 489, 490, 492, 493, 494, 495, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 521, 522, 523, 524, 526, 527, 528, 529, 530, 531, 533, 534, 535, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 557, 558, 559, 560, 561, 562, 563, 564, 566, 567, 568, 569, 570, 571, 572, 573, 575, 577, 578, 579, 580, 581, 583, 584, 585, 586, 588, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 611, 612, 613, 614, 615, 617, 618, 619, 620, 621, 622, 624, 625, 626, 627, 628, 631, 632, 633, 634, 635, 636, 637, 639, 640, 641, 643, 644, 645, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 661, 662, 664, 666, 667, 669, 670, 671, 672, 673, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 689, 690, 691, 693, 694, 695, 696, 697, 698, 699, 700, 702, 703, 704, 705, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 742, 744, 745, 747, 748, 749, 752, 753, 754, 756, 757, 759, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 784, 785, 786, 787, 788, 789, 791, 792, 794, 795, 797, 798, 799, 800, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 815, 816, 817, 818, 819, 820, 821, 822, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 841, 842, 843, 844, 845, 848, 849, 851, 852, 854, 855, 857, 858, 859, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 876, 877, 878, 879, 880, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 922, 923, 924, 925, 926, 927, 928, 929, 931, 932, 933, 934, 935, 936, 937, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 979, 981, 982, 983, 984, 985, 986, 987, 988, 989, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1002, 1004, 1006, 1007, 1008, 1011, 1012, 1014, 1015, 1016, 1017, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1038, 1040, 1041, 1042, 1043, 1044, 1045, 1048, 1050, 1051, 1052, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1062, 1064, 1065, 1067, 1068, 1069, 1071, 1073, 1074, 1075, 1076, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1086, 1087, 1088, 1089, 1090, 1091, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1120, 1121, 1122, 1123, 1124, 1125, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1175, 1176, 1177, 1178, 1179, 1180, 1182, 1183, 1184, 1185, 1186, 1187, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1197, 1198, 1200, 1201, 1202, 1203, 1204, 1205, 1207, 1208, 1210, 1211, 1212, 1213, 1214, 1216, 1217, 1218, 1219, 1220, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1246, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1277, 1278, 1279, 1280, 1281, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1355, 1356, 1357, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1373, 1374, 1375, 1376, 1377, 1378, 1380, 1382, 1383, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1467, 1468, 1469, 1470, 1472, 1473, 1474, 1476, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1523, 1525, 1526, 1527, 1528, 1530, 1531, 1532, 1533, 1534, 1535, 1537, 1538, 1540, 1541, 1542, 1543, 1544, 1545, 1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1557, 1558, 1559, 1561, 1562, 1563, 1564, 1566, 1567, 1568, 1569, 1570, 1571, 1573, 1575, 1576, 1577, 1578, 1579, 1582, 1583, 1586, 1589, 1591, 1592, 1593, 1595, 1597, 1598, 1599, 1600, 1601, 1602, 1603, 1604, 1605, 1608, 1609, 1611, 1613, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1623, 1624, 1625, 1626, 1627, 1628, 1629, 1632, 1633, 1634, 1635, 1636, 1638, 1639, 1641, 1642, 1645, 1647, 1649, 1651, 1652, 1655, 1656, 1657, 1659, 1660, 1662, 1663, 1664, 1665, 1666, 1667, 1669, 1670, 1672, 1674, 1675, 1676, 1677, 1678, 1679, 1681, 1683, 1685, 1686, 1687, 1689, 1690, 1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701, 1702, 1703, 1704, 1705, 1707, 1709, 1711, 1713, 1714, 1715, 1716, 1720, 1721, 1722, 1723, 1724, 1727, 1728, 1730, 1731, 1732, 1733, 1735, 1736, 1738, 1739, 1741, 1742, 1743, 1744, 1745, 1746, 1747, 1748, 1749, 1750, 1751, 1752, 1753, 1754, 1755, 1756, 1757, 1761, 1762, 1763, 1764, 1765, 1766, 1767, 1768, 1769, 1770, 1771, 1772, 1773, 1774, 1775, 1777, 1778, 1779, 1780, 1781, 1782, 1784, 1786, 1787, 1788, 1790, 1791, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1811, 1812, 1813, 1815, 1816, 1817, 1818, 1819, 1821, 1822, 1824, 1825, 1826, 1828, 1832, 1833, 1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1844, 1846, 1847, 1848, 1849, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1861, 1863, 1864, 1865, 1866, 1867, 1868, 1869, 1870, 1871, 1872, 1873, 1875, 1877, 1878, 1879, 1880, 1881, 1882, 1883, 1884, 1885, 1886, 1887, 1889, 1891, 1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899, 1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910, 1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1932, 1933, 1935, 1936, 1937, 1938, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949, 1950, 1951, 1954, 1956, 1957, 1958, 1959, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1985, 1986, 1987, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1999, 2000, 2001, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2016, 2017, 2018, 2020, 2021, 2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033, 2034, 2036, 2039, 2040, 2041, 2042, 2043, 2044, 2045, 2046, 2047, 2048, 2049, 2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057, 2058, 2059, 2060, 2061, 2062, 2063, 2064, 2065, 2066, 2067, 2068, 2069, 2070, 2071, 2072, 2073, 2074, 2075, 2077, 2079, 2080, 2081, 2082, 2083, 2084, 2085, 2086, 2087, 2088, 2089, 2090, 2091, 2092, 2093, 2094, 2095, 2096, 2098, 2099, 2100, 2101, 2102, 2103, 2104, 2105, 2106, 2107, 2109, 2110, 2111, 2112, 2114, 2115, 2116, 2117, 2118, 2119, 2121, 2122, 2124, 2125, 2126, 2127, 2128, 2129, 2130, 2131, 2132, 2133, 2134, 2135, 2137, 2138, 2139, 2140, 2141, 2142, 2143, 2144, 2145, 2146, 2147, 2148, 2149, 2150, 2151, 2152, 2153, 2154, 2155, 2156, 2157, 2158, 2159, 2160, 2163, 2164, 2165, 2166, 2168, 2169, 2170, 2171, 2172, 2173, 2174, 2175, 2176, 2178, 2180, 2181, 2182, 2183, 2184, 2185, 2186, 2187, 2188, 2189, 2190, 2191, 2192, 2193, 2196, 2198, 2200, 2201, 2202, 2204, 2205, 2207, 2208, 2209, 2210, 2211, 2212, 2213, 2215, 2218, 2220, 2221, 2222, 2223, 2224, 2225, 2226, 2227, 2230, 2233, 2234, 2235, 2236, 2237, 2238, 2239, 2240, 2241, 2242, 2243, 2244, 2246, 2247, 2248, 2249, 2250, 2251, 2252, 2253, 2254, 2255, 2256, 2257, 2258, 2259, 2260, 2261, 2262, 2263, 2264, 2266, 2267, 2268, 2269, 2270, 2271, 2272, 2273, 2274, 2275, 2276, 2277, 2278, 2279, 2280, 2281, 2282, 2283, 2284, 2285, 2286, 2287, 2288, 2289, 2291, 2292, 2293, 2294, 2295, 2297, 2298, 2299, 2300, 2301, 2302, 2303, 2304, 2305, 2306, 2307, 2308, 2309, 2310, 2311, 2312, 2313, 2314, 2315, 2317, 2318, 2319, 2320, 2322, 2324, 2325, 2326, 2327, 2328, 2329, 2330, 2331, 2332, 2333, 2334, 2335, 2336, 2338, 2340, 2341, 2342, 2343, 2344, 2345, 2346, 2347, 2348, 2349, 2350, 2351, 2352, 2353, 2355, 2358, 2359, 2361, 2362, 2363, 2364, 2365, 2366, 2367, 2368, 2369, 2370, 2371, 2372, 2373, 2374, 2375, 2377, 2378, 2380, 2381, 2382, 2383, 2384, 2385, 2386, 2388, 2391, 2392, 2393, 2394, 2396, 2397, 2398, 2399, 2400, 2401, 2402, 2403, 2404, 2405, 2407, 2408, 2409, 2410, 2411, 2412, 2415, 2416, 2417, 2418, 2419, 2422, 2423, 2424, 2425, 2426, 2427, 2428, 2429, 2430, 2431, 2432, 2433, 2434, 2435, 2436, 2438, 2439, 2440, 2441, 2442, 2443, 2444, 2445, 2446, 2447, 2448, 2449, 2450, 2451, 2452, 2454, 2455, 2456, 2457, 2458, 2459, 2460, 2461, 2462, 2463, 2464, 2465, 2466, 2467, 2468, 2470, 2471, 2472, 2473, 2474, 2475, 2476, 2477, 2478, 2480, 2481, 2482, 2483, 2484, 2485, 2486, 2487, 2488, 2489, 2490, 2491, 2492, 2494, 2495, 2496, 2497, 2498, 2499, 2500, 2501, 2502, 2504, 2505, 2506, 2508, 2509, 2510, 2511, 2512, 2513, 2515, 2516, 2517, 2518, 2519, 2520, 2521, 2522, 2523, 2524, 2525, 2526, 2527, 2528, 2531, 2532, 2533, 2534, 2536, 2537, 2538, 2539, 2540, 2541, 2542, 2543, 2545, 2546, 2547, 2549, 2551, 2553, 2554, 2555, 2556, 2557, 2558, 2559, 2561, 2562, 2565, 2566, 2567, 2568, 2569, 2570, 2571, 2572, 2573, 2574, 2575, 2576, 2577, 2579, 2580, 2581, 2582, 2583, 2584, 2585, 2586, 2587, 2588, 2589, 2590, 2591, 2593, 2594, 2595, 2596, 2597, 2598, 2599, 2600, 2601, 2602, 2604, 2605, 2606, 2607, 2608, 2609, 2610, 2611, 2612, 2613, 2614, 2615, 2618, 2619, 2620, 2621, 2622, 2624, 2625, 2626, 2627, 2628, 2629, 2631, 2632, 2633, 2634, 2635, 2636, 2638, 2639, 2640, 2644, 2645, 2646, 2647, 2648, 2650, 2651, 2654, 2656, 2657, 2658, 2659, 2660, 2661, 2662, 2663, 2665, 2667, 2668, 2669, 2671, 2672, 2673, 2674, 2675, 2676, 2678, 2679, 2680, 2681, 2682, 2684, 2685, 2686, 2687, 2688, 2689, 2690, 2691, 2692, 2693, 2697, 2698, 2699, 2700, 2702, 2703, 2704, 2705, 2706, 2707, 2708, 2710, 2712, 2713, 2714, 2715, 2716, 2717, 2718, 2719, 2720, 2721, 2722, 2723, 2724, 2725, 2726, 2727, 2728, 2729, 2730, 2731, 2732, 2733, 2734, 2736, 2738, 2739, 2741, 2743, 2744, 2745, 2746, 2747, 2748, 2750, 2751, 2752, 2753, 2754, 2755, 2759, 2760, 2762, 2763, 2764, 2765, 2766, 2767, 2768, 2770, 2772, 2773, 2775, 2776, 2777, 2778, 2780, 2781, 2782, 2783, 2784, 2786, 2787, 2788, 2789, 2790, 2791, 2792, 2793, 2794, 2795, 2796, 2797, 2798, 2799, 2800, 2803, 2804, 2806, 2807, 2808, 2810, 2812, 2813, 2814, 2816, 2817, 2818, 2819, 2820, 2821, 2822, 2823, 2824, 2825, 2827, 2828, 2830, 2831, 2832, 2833, 2835, 2836, 2837, 2838, 2840, 2842, 2843, 2844, 2845, 2846, 2847, 2848, 2849, 2850, 2851, 2853, 2854, 2856, 2857, 2859, 2860, 2862, 2863, 2864, 2865, 2868, 2869, 2870, 2871, 2872, 2873, 2874, 2875, 2876, 2877, 2878, 2879, 2880, 2881, 2883, 2884, 2885, 2886, 2887, 2888, 2889, 2893, 2894, 2895, 2897, 2898, 2899, 2900, 2901, 2903, 2904, 2905, 2906, 2907, 2908, 2909, 2910, 2912, 2913, 2914, 2917, 2918, 2919, 2920, 2921, 2922, 2923, 2924, 2925, 2926, 2927, 2928, 2929, 2931, 2932, 2933, 2935, 2937, 2938, 2939, 2940, 2941, 2942, 2943, 2944, 2945, 2946, 2947, 2948, 2949, 2950, 2951, 2952, 2953, 2954, 2955, 2956, 2957, 2958, 2959, 2960, 2961, 2962, 2963, 2964, 2965, 2966, 2968, 2971, 2972, 2975, 2976, 2977, 2978, 2979, 2980, 2981, 2982, 2983, 2984, 2985, 2986, 2987, 2988, 2989, 2990, 2991, 2992, 2993, 2994, 2995, 2996, 2997, 2999, 3000, 3002, 3003, 3004, 3005, 3007, 3008, 3009, 3010, 3011, 3012, 3013, 3014, 3015, 3016, 3017, 3018, 3019, 3020, 3021, 3022, 3023, 3024, 3025, 3026, 3029, 3030, 3031, 3032, 3034, 3035, 3037, 3038, 3039, 3040, 3041, 3042, 3043, 3044, 3047, 3048, 3049, 3050, 3051, 3052, 3054, 3055, 3056, 3058, 3060, 3061, 3062, 3063, 3064, 3065, 3067, 3068, 3070, 3071], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 50, 51, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 96, 97, 98, 99, 100, 101, 102, 103, 104, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 130, 131, 132, 133, 134, 135, 136, 138, 139, 140, 141, 142, 143, 144, 145, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 237, 238, 239, 241, 243, 244, 245, 246, 247, 248, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 301, 302, 303, 304, 306, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 328, 329, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 436, 437, 438, 439, 440, 441, 442, 443, 444, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 462, 463, 464, 465, 466, 467, 468, 469, 470, 472, 473, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 492, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 657, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 676, 677, 679, 680, 681, 682, 684, 685, 686, 687, 689, 690, 691, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 742, 743, 744, 745, 746, 747, 748, 750, 751, 752, 753, 754, 755, 756, 757, 759, 760, 761, 762, 763, 764, 765, 766, 768, 769, 770, 771, 772, 773, 774, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 805, 806, 807, 808, 809, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 824, 825, 826, 827, 828, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 842, 843, 844, 845, 846, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 861, 862, 863, 864, 865, 866, 867, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 896, 897, 898, 899, 900, 901, 902, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 937, 938, 940, 942, 943, 944, 945, 946, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 959, 960, 961, 962, 963, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1030, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1058, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1070, 1071, 1073, 1074, 1075, 1076, 1077, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1100, 1101, 1102, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1253, 1255, 1257, 1258, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1346, 1347, 1349, 1350, 1351, 1352, 1353, 1354, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1391, 1392, 1393, 1394, 1395, 1396, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1414, 1415, 1416, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1432, 1433, 1434, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1470, 1471, 1472, 1473, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523, 1524, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545, 1546, 1547, 1548, 1549, 1551, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567, 1568, 1569, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1578, 1579, 1580, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589, 1590, 1592, 1593, 1595, 1596, 1597, 1598, 1599, 1600, 1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611, 1612, 1613, 1614, 1615, 1616, 1617, 1618, 1620, 1621, 1622, 1623, 1624, 1625, 1626, 1628, 1629, 1630, 1631, 1632, 1633, 1634, 1635, 1636, 1637, 1638, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650, 1651, 1652, 1653, 1656, 1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668, 1669, 1671, 1672, 1673, 1674, 1675, 1677, 1678, 1679, 1680, 1681, 1682, 1683, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1693, 1694, 1697, 1699, 1700, 1701, 1702, 1703, 1704, 1706, 1707, 1709, 1710, 1711, 1713, 1714, 1715, 1716, 1718, 1719, 1720, 1721, 1722, 1723, 1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1737, 1738, 1740, 1741, 1742, 1743, 1744, 1745, 1746, 1747, 1748, 1749, 1750, 1751, 1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1767, 1768, 1769, 1770, 1772, 1773, 1774, 1775, 1777, 1778, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1791, 1792, 1793, 1794, 1797, 1798, 1799, 1800, 1801, 1803, 1804, 1805, 1806, 1807, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1818, 1819, 1820, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833, 1834, 1835, 1836, 1837, 1838, 1839, 1841, 1842, 1843, 1844, 1845, 1846, 1848, 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1866, 1867, 1868, 1869, 1871, 1872, 1873, 1874, 1875, 1876, 1878, 1879, 1880, 1882, 1883, 1884, 1885, 1886, 1887, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899, 1900, 1902, 1903, 1905, 1906, 1907, 1908, 1909, 1910, 1912, 1913, 1914, 1916, 1917, 1918, 1919, 1920, 1922, 1925, 1926, 1927, 1928, 1929, 1930, 1932, 1933, 1934, 1935, 1936, 1937, 1940, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949, 1951, 1953, 1954, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1966, 1967, 1968, 1969, 1970, 1972, 1973, 1974, 1975, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1999, 2000, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2026, 2028, 2029, 2030, 2031, 2032, 2033, 2034, 2035, 2036, 2037, 2038, 2039, 2041, 2042, 2043, 2044, 2045, 2047, 2048, 2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057, 2058, 2059, 2060, 2061, 2062, 2063, 2064, 2065, 2067, 2068, 2069, 2072, 2073, 2074, 2075, 2076, 2077, 2078, 2079, 2080, 2081, 2082, 2083, 2084, 2085, 2087, 2088, 2089, 2090, 2091, 2092, 2093, 2094, 2095, 2096, 2097, 2098, 2099, 2100, 2101, 2102, 2103, 2104, 2105, 2106, 2107, 2108, 2109, 2110, 2111, 2112, 2113, 2114, 2115, 2116, 2117, 2118, 2119, 2120, 2121, 2122, 2124, 2125, 2126, 2127, 2129, 2130, 2132, 2133, 2134, 2135, 2136, 2137, 2138, 2139, 2140, 2141, 2143, 2144, 2145, 2146, 2147, 2148, 2149, 2150, 2151, 2152, 2153, 2154, 2155, 2156, 2157, 2158, 2159, 2160, 2161, 2162, 2163, 2164, 2165, 2166, 2167, 2168, 2169, 2170, 2171, 2172, 2173, 2174, 2175, 2176, 2177, 2178, 2180, 2181, 2182, 2183, 2184, 2185, 2186, 2187, 2188, 2189, 2190, 2191, 2192, 2193, 2194, 2195, 2196, 2197, 2198, 2199, 2200, 2201, 2202, 2203, 2204, 2207, 2208, 2209, 2210, 2211, 2212, 2213, 2214, 2215, 2216, 2217, 2218, 2219, 2220, 2222, 2223, 2224, 2225, 2227, 2228, 2229, 2230, 2231, 2234, 2235, 2236, 2237, 2238, 2239, 2240, 2241, 2242, 2243, 2244, 2246, 2247, 2248, 2249, 2250, 2251, 2253, 2254, 2255, 2256, 2257, 2258, 2259, 2260, 2261, 2262, 2263, 2264, 2265, 2266, 2267, 2268, 2269, 2270, 2271, 2272, 2273, 2274, 2275, 2276, 2277, 2278, 2279, 2280, 2281, 2282, 2284, 2285, 2287, 2289, 2290, 2292, 2294, 2295, 2296, 2297, 2298, 2299, 2300, 2301, 2302, 2303, 2304, 2305, 2306, 2307, 2308, 2310, 2313, 2314, 2315, 2316, 2317, 2318, 2319, 2320, 2321, 2322, 2323, 2324, 2326, 2327, 2328, 2329, 2330, 2331, 2332, 2333, 2334, 2335, 2336, 2337, 2338, 2339, 2340, 2341, 2342, 2343, 2344, 2345, 2346, 2347, 2348, 2350, 2351, 2352, 2353, 2354, 2355, 2357, 2358, 2359, 2360, 2361, 2362, 2363, 2365, 2367, 2368, 2369, 2370, 2371, 2372, 2373, 2374, 2375, 2376, 2377, 2378, 2379, 2380, 2381, 2383, 2384, 2385, 2386, 2387, 2388, 2389, 2390, 2391, 2392, 2393, 2394, 2395, 2396, 2397, 2399, 2401, 2402, 2403, 2405, 2406, 2407, 2408, 2409, 2410, 2411, 2413, 2414, 2415, 2416, 2417, 2418, 2419, 2420, 2423, 2424, 2425, 2426, 2427, 2428, 2429, 2430, 2432, 2433, 2434, 2435, 2436, 2437, 2438, 2439, 2440, 2442, 2443, 2444, 2445, 2446, 2447, 2448, 2449, 2450, 2452, 2453, 2454, 2455, 2456, 2457, 2458, 2460, 2461, 2462, 2463, 2464, 2465, 2466, 2467, 2469, 2470, 2471, 2472, 2473, 2474, 2475, 2476, 2477, 2478, 2479, 2480, 2481, 2482, 2483, 2484, 2485, 2486, 2487, 2488, 2490, 2491, 2492, 2493, 2494, 2495, 2496, 2497, 2498, 2499, 2500, 2501, 2502, 2503, 2504, 2505, 2506, 2507, 2508, 2509, 2510, 2511, 2512, 2513, 2516, 2517, 2518, 2519, 2520, 2521, 2522, 2523, 2524, 2526, 2527, 2528, 2529, 2530, 2531, 2532, 2533, 2534, 2535, 2536, 2537, 2539, 2540, 2541, 2542, 2543, 2544, 2545, 2546, 2547, 2548, 2549, 2550, 2551, 2552, 2553, 2554, 2555, 2556, 2557, 2558, 2559, 2560, 2561, 2562, 2563, 2564, 2566, 2567, 2568, 2569, 2570, 2571, 2572, 2573, 2574, 2575, 2576, 2577, 2578, 2579, 2580, 2581, 2582, 2583, 2584, 2585, 2586, 2587, 2588, 2589, 2590, 2591, 2592, 2593, 2594, 2595, 2596, 2597, 2598, 2599, 2600, 2601, 2603, 2605, 2606, 2609, 2610, 2611, 2613, 2614, 2615, 2616, 2617, 2619, 2620, 2621, 2622, 2624, 2625, 2626, 2627, 2628, 2629, 2630, 2631, 2632, 2633, 2634, 2635, 2637, 2638, 2639, 2640, 2641, 2642, 2643, 2644, 2645, 2646, 2647, 2649, 2650, 2651, 2653, 2654, 2655, 2656, 2657, 2660, 2661, 2662, 2663, 2664, 2665, 2666, 2667, 2668, 2669, 2670, 2671, 2672, 2673, 2674, 2675, 2676, 2677, 2678, 2679, 2680, 2681, 2682, 2683, 2685, 2686, 2687, 2688, 2689, 2690, 2691, 2692, 2693, 2694, 2695, 2696, 2697, 2698, 2699, 2700, 2702, 2703, 2704, 2705, 2706, 2709, 2710, 2711, 2712, 2713, 2714, 2715, 2717, 2718, 2719, 2720, 2721, 2722, 2724, 2725, 2726, 2727, 2728, 2729, 2730, 2731, 2732, 2734, 2735, 2736, 2737, 2738, 2739, 2740, 2742, 2743, 2744, 2745, 2746, 2747, 2748, 2749, 2750, 2751, 2752, 2753, 2754, 2755, 2756, 2757, 2758, 2759, 2760, 2761, 2762, 2763, 2764, 2765, 2766, 2767, 2768, 2769, 2770, 2772, 2773, 2774, 2775, 2776, 2777, 2778, 2780, 2781, 2783, 2784, 2785, 2786, 2787, 2788, 2789, 2791, 2792, 2793, 2794, 2795, 2796, 2797, 2798, 2799, 2800, 2801, 2802, 2803, 2804, 2805, 2806, 2807, 2808, 2809, 2810, 2811, 2812, 2813, 2814, 2815, 2816, 2817, 2818, 2819, 2821, 2822, 2823, 2824, 2825, 2826, 2827, 2828, 2829, 2830, 2831, 2832, 2833, 2834, 2835, 2836, 2837, 2838, 2840, 2841, 2842, 2843, 2844, 2845, 2846, 2847, 2848, 2849, 2850, 2851, 2852, 2853, 2854, 2855, 2856, 2857, 2858, 2859, 2860, 2861, 2862, 2863, 2864, 2865, 2866, 2867, 2868, 2869, 2870, 2871, 2872, 2873, 2874, 2875, 2877, 2878, 2879, 2881, 2882, 2883, 2884, 2885, 2886, 2887, 2889, 2890, 2892, 2893, 2894, 2895, 2896, 2897, 2898, 2899, 2900, 2901, 2902, 2903, 2904, 2905, 2906, 2907, 2908, 2909, 2910, 2911, 2912, 2913, 2914, 2915, 2916, 2917, 2918, 2919, 2920, 2921, 2922, 2924, 2925, 2928, 2929, 2933, 2934, 2935, 2936, 2937, 2939, 2940, 2941, 2942, 2943, 2944, 2945, 2946, 2947, 2948, 2949, 2950, 2951, 2953, 2954, 2955, 2956, 2957, 2958, 2959, 2960, 2962, 2963, 2964, 2965, 2966, 2967, 2968, 2969, 2970, 2971, 2972, 2973, 2974, 2975, 2976, 2977, 2978, 2979, 2980, 2982, 2983, 2984, 2985, 2986, 2987, 2988, 2989, 2991, 2992, 2993, 2994, 2995, 2996, 2997, 2998, 2999, 3000, 3002, 3003, 3004, 3005, 3007, 3009, 3010, 3011, 3012, 3013, 3014, 3015, 3016, 3017, 3018, 3019, 3020, 3021, 3022, 3023, 3024, 3025, 3026, 3027, 3029, 3030, 3031, 3032, 3033, 3034, 3035, 3036, 3037, 3038, 3039, 3040, 3041, 3042, 3043, 3044, 3045, 3047, 3048, 3049, 3050, 3051, 3052, 3054, 3055, 3056, 3057, 3058, 3059, 3060, 3061, 3062, 3063, 3064, 3065, 3066, 3067, 3068, 3069, 3070, 3071], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 108, 109, 110, 111, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 161, 162, 163, 164, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 252, 253, 254, 255, 256, 257, 258, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 272, 273, 274, 275, 276, 278, 279, 280, 281, 282, 283, 284, 285, 286, 288, 289, 290, 291, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 381, 382, 384, 385, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 422, 424, 425, 426, 427, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 608, 609, 610, 611, 612, 614, 615, 616, 617, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 642, 643, 644, 645, 647, 648, 650, 651, 652, 653, 654, 655, 658, 659, 660, 661, 662, 663, 665, 666, 667, 668, 669, 670, 671, 673, 674, 675, 676, 677, 678, 679, 680, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 709, 710, 711, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 884, 885, 886, 887, 888, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 972, 973, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1091, 1092, 1093, 1094, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1105, 1106, 1107, 1108, 1109, 1110, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1219, 1221, 1222, 1223, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1277, 1278, 1279, 1280, 1281, 1283, 1284, 1285, 1287, 1288, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1308, 1309, 1310, 1312, 1314, 1316, 1317, 1318, 1319, 1321, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1337, 1339, 1340, 1341, 1342, 1343, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1430, 1431, 1433, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1539, 1541, 1542, 1543, 1544, 1545, 1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567, 1568, 1569, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589, 1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600, 1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611, 1612, 1613, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1624, 1625, 1626, 1627, 1628, 1629, 1630, 1631, 1632, 1633, 1634, 1635, 1636, 1637, 1638, 1639, 1640, 1641, 1643, 1645, 1646, 1647, 1649, 1650, 1651, 1652, 1653, 1655, 1656, 1657, 1658, 1659, 1660, 1662, 1663, 1664, 1665, 1666, 1667, 1668, 1669, 1670, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679, 1680, 1681, 1682, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1693, 1695, 1696, 1697, 1698, 1699, 1700, 1701, 1702, 1703, 1704, 1706, 1707, 1708, 1709, 1710, 1711, 1712, 1713, 1714, 1715, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723, 1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739, 1741, 1742, 1743, 1744, 1745, 1746, 1747, 1748, 1749, 1750, 1751, 1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759, 1760, 1761, 1763, 1764, 1765, 1766, 1767, 1768, 1769, 1770, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1829, 1831, 1832, 1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1843, 1844, 1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1862, 1864, 1865, 1866, 1867, 1868, 1869, 1870, 1871, 1872, 1873, 1874, 1875, 1876, 1877, 1878, 1879, 1880, 1881, 1882, 1883, 1884, 1885, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1895, 1897, 1898, 1899, 1900, 1902, 1903, 1905, 1906, 1907, 1908, 1909, 1910, 1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1939, 1941, 1942, 1943, 1945, 1946, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1990, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2010, 2011, 2012, 2013, 2014, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2027, 2028, 2029, 2031, 2032, 2033, 2034, 2035, 2036, 2037, 2039, 2040, 2041, 2043, 2044, 2045, 2046, 2047, 2048, 2049, 2050, 2052, 2053, 2054, 2055, 2056, 2057, 2058, 2059, 2060, 2061, 2062, 2063, 2064, 2065, 2066, 2067, 2068, 2069, 2070, 2071, 2072, 2073, 2074, 2075, 2076, 2077, 2078, 2079, 2080, 2081, 2082, 2083, 2084, 2085, 2086, 2087, 2088, 2089, 2090, 2091, 2092, 2093, 2094, 2095, 2096, 2097, 2098, 2099, 2100, 2101, 2102, 2103, 2104, 2105, 2106, 2107, 2108, 2109, 2110, 2112, 2113, 2115, 2116, 2117, 2118, 2119, 2120, 2121, 2122, 2124, 2125, 2126, 2128, 2129, 2130, 2131, 2132, 2133, 2134, 2135, 2136, 2137, 2138, 2139, 2140, 2141, 2142, 2143, 2144, 2145, 2146, 2147, 2148, 2149, 2150, 2151, 2152, 2153, 2154, 2155, 2156, 2157, 2158, 2159, 2160, 2161, 2162, 2164, 2165, 2166, 2167, 2169, 2172, 2173, 2174, 2175, 2176, 2177, 2178, 2179, 2180, 2181, 2182, 2183, 2184, 2185, 2186, 2187, 2188, 2189, 2190, 2191, 2192, 2193, 2194, 2195, 2197, 2198, 2199, 2200, 2201, 2202, 2203, 2204, 2205, 2206, 2207, 2208, 2210, 2211, 2212, 2213, 2214, 2215, 2216, 2217, 2218, 2219, 2220, 2221, 2222, 2223, 2224, 2225, 2226, 2227, 2228, 2229, 2230, 2231, 2232, 2233, 2234, 2235, 2236, 2237, 2238, 2239, 2240, 2241, 2242, 2243, 2245, 2247, 2248, 2249, 2250, 2251, 2252, 2253, 2255, 2256, 2257, 2258, 2259, 2260, 2261, 2262, 2263, 2264, 2265, 2266, 2267, 2268, 2269, 2270, 2271, 2272, 2273, 2274, 2275, 2276, 2277, 2278, 2279, 2280, 2281, 2283, 2284, 2285, 2288, 2289, 2290, 2291, 2293, 2295, 2296, 2297, 2298, 2300, 2301, 2302, 2303, 2304, 2305, 2306, 2307, 2308, 2309, 2310, 2311, 2312, 2313, 2314, 2315, 2316, 2317, 2318, 2319, 2320, 2321, 2322, 2323, 2324, 2325, 2326, 2327, 2328, 2329, 2330, 2331, 2332, 2333, 2334, 2335, 2336, 2337, 2338, 2339, 2340, 2342, 2343, 2344, 2346, 2347, 2348, 2349, 2350, 2351, 2353, 2354, 2355, 2356, 2358, 2359, 2360, 2362, 2363, 2364, 2368, 2369, 2370, 2371, 2372, 2373, 2374, 2376, 2377, 2378, 2379, 2380, 2381, 2383, 2384, 2385, 2386, 2387, 2388, 2389, 2390, 2391, 2393, 2394, 2395, 2396, 2397, 2398, 2399, 2400, 2401, 2402, 2403, 2404, 2405, 2406, 2407, 2408, 2409, 2410, 2411, 2412, 2413, 2414, 2415, 2416, 2417, 2418, 2419, 2420, 2421, 2422, 2423, 2424, 2426, 2427, 2428, 2429, 2430, 2431, 2432, 2433, 2434, 2435, 2436, 2437, 2438, 2439, 2440, 2441, 2442, 2443, 2445, 2446, 2447, 2448, 2449, 2450, 2451, 2452, 2453, 2454, 2455, 2456, 2458, 2459, 2460, 2461, 2462, 2464, 2465, 2466, 2467, 2468, 2469, 2470, 2471, 2472, 2473, 2474, 2475, 2477, 2478, 2479, 2481, 2482, 2483, 2484, 2485, 2486, 2487, 2489, 2490, 2491, 2492, 2493, 2494, 2495, 2496, 2497, 2498, 2499, 2500, 2501, 2502, 2503, 2504, 2505, 2506, 2507, 2508, 2510, 2511, 2512, 2513, 2514, 2515, 2516, 2518, 2520, 2521, 2522, 2523, 2524, 2525, 2526, 2527, 2528, 2529, 2530, 2531, 2532, 2533, 2534, 2535, 2536, 2537, 2538, 2539, 2540, 2541, 2542, 2543, 2544, 2545, 2546, 2547, 2548, 2549, 2550, 2551, 2552, 2553, 2554, 2555, 2556, 2557, 2559, 2560, 2561, 2562, 2563, 2564, 2565, 2566, 2567, 2568, 2569, 2570, 2571, 2572, 2573, 2574, 2575, 2576, 2577, 2578, 2579, 2580, 2581, 2582, 2583, 2584, 2585, 2586, 2587, 2588, 2589, 2590, 2591, 2593, 2594, 2595, 2596, 2597, 2598, 2599, 2600, 2601, 2602, 2603, 2604, 2605, 2606, 2608, 2609, 2610, 2611, 2612, 2613, 2614, 2615, 2616, 2617, 2618, 2619, 2620, 2621, 2622, 2623, 2624, 2625, 2626, 2627, 2628, 2629, 2630, 2631, 2632, 2633, 2635, 2636, 2637, 2638, 2639, 2640, 2642, 2643, 2644, 2645, 2646, 2647, 2648, 2649, 2650, 2651, 2652, 2654, 2655, 2656, 2657, 2658, 2659, 2660, 2661, 2662, 2663, 2664, 2666, 2669, 2670, 2671, 2672, 2673, 2674, 2675, 2676, 2677, 2679, 2680, 2681, 2682, 2683, 2684, 2685, 2686, 2687, 2688, 2689, 2690, 2691, 2692, 2693, 2694, 2696, 2697, 2698, 2699, 2700, 2701, 2702, 2703, 2704, 2705, 2706, 2707, 2708, 2710, 2711, 2712, 2713, 2714, 2715, 2716, 2717, 2718, 2719, 2720, 2721, 2722, 2723, 2724, 2725, 2726, 2728, 2729, 2730, 2731, 2732, 2733, 2734, 2735, 2736, 2737, 2738, 2739, 2740, 2741, 2742, 2744, 2745, 2746, 2747, 2748, 2749, 2750, 2751, 2752, 2753, 2754, 2755, 2756, 2757, 2758, 2759, 2760, 2761, 2762, 2763, 2764, 2765, 2766, 2767, 2768, 2769, 2770, 2771, 2772, 2773, 2774, 2775, 2776, 2777, 2778, 2779, 2780, 2781, 2782, 2783, 2784, 2785, 2786, 2787, 2788, 2789, 2790, 2791, 2792, 2793, 2794, 2795, 2796, 2797, 2799, 2800, 2801, 2803, 2804, 2805, 2806, 2807, 2808, 2809, 2810, 2811, 2812, 2813, 2814, 2815, 2816, 2818, 2819, 2820, 2821, 2822, 2823, 2824, 2825, 2826, 2827, 2828, 2829, 2830, 2832, 2833, 2835, 2836, 2837, 2838, 2839, 2840, 2841, 2842, 2843, 2844, 2845, 2846, 2847, 2848, 2849, 2850, 2851, 2852, 2855, 2856, 2857, 2858, 2859, 2860, 2862, 2863, 2864, 2865, 2866, 2867, 2868, 2869, 2871, 2872, 2873, 2874, 2875, 2876, 2877, 2878, 2879, 2880, 2881, 2882, 2883, 2884, 2886, 2887, 2888, 2889, 2890, 2892, 2893, 2894, 2895, 2896, 2897, 2898, 2899, 2900, 2901, 2902, 2903, 2904, 2906, 2907, 2908, 2909, 2910, 2911, 2912, 2913, 2914, 2915, 2916, 2918, 2919, 2920, 2921, 2922, 2923, 2924, 2925, 2926, 2927, 2928, 2929, 2930, 2931, 2932, 2933, 2934, 2935, 2936, 2937, 2938, 2939, 2940, 2941, 2942, 2943, 2944, 2946, 2947, 2948, 2949, 2950, 2951, 2952, 2953, 2956, 2957, 2958, 2959, 2960, 2961, 2962, 2963, 2964, 2965, 2966, 2967, 2969, 2970, 2971, 2972, 2973, 2974, 2975, 2976, 2977, 2978, 2979, 2980, 2981, 2982, 2983, 2984, 2985, 2986, 2987, 2988, 2989, 2990, 2991, 2992, 2993, 2994, 2995, 2996, 2997, 2998, 2999, 3000, 3001, 3002, 3003, 3004, 3005, 3006, 3007, 3008, 3009, 3010, 3011, 3012, 3013, 3014, 3015, 3016, 3017, 3018, 3019, 3020, 3021, 3022, 3023, 3024, 3025, 3026, 3027, 3028, 3029, 3030, 3031, 3032, 3033, 3034, 3035, 3036, 3037, 3038, 3040, 3041, 3042, 3043, 3044, 3045, 3046, 3047, 3048, 3049, 3050, 3051, 3052, 3053, 3054, 3055, 3056, 3057, 3058, 3059, 3060, 3063, 3064, 3066, 3067, 3068, 3069, 3070, 3071], [0, 2, 3, 4, 6, 7, 8, 9, 10, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 45, 46, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 131, 132, 133, 136, 139, 140, 141, 142, 143, 144, 145, 147, 148, 149, 150, 151, 152, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 169, 170, 171, 173, 174, 175, 176, 178, 179, 180, 182, 184, 185, 186, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 205, 206, 207, 208, 209, 211, 212, 213, 214, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 229, 230, 231, 233, 234, 235, 236, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 259, 260, 261, 262, 263, 264, 265, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 287, 288, 289, 290, 291, 292, 294, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 311, 312, 313, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 358, 359, 360, 361, 362, 363, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 378, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 397, 398, 399, 400, 401, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 463, 464, 465, 466, 467, 468, 469, 470, 471, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 496, 497, 498, 500, 501, 502, 504, 505, 506, 507, 508, 509, 510, 512, 513, 514, 515, 516, 517, 520, 521, 522, 523, 524, 525, 527, 528, 531, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 564, 565, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 579, 580, 581, 582, 583, 584, 586, 587, 588, 589, 592, 593, 594, 595, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 610, 611, 612, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 634, 635, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 652, 653, 654, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 678, 679, 680, 681, 682, 683, 685, 687, 688, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 705, 706, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 725, 726, 728, 730, 731, 732, 733, 734, 735, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 756, 757, 758, 759, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 804, 805, 806, 807, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 823, 824, 825, 826, 827, 828, 830, 831, 832, 833, 834, 835, 836, 839, 841, 842, 844, 846, 847, 848, 849, 850, 851, 853, 854, 855, 856, 857, 860, 861, 862, 863, 864, 865, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 911, 912, 913, 914, 915, 916, 917, 919, 920, 921, 922, 923, 924, 925, 926, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 977, 979, 980, 981, 982, 983, 984, 985, 986, 987, 989, 990, 992, 993, 994, 995, 996, 997, 998, 1000, 1001, 1003, 1004, 1005, 1006, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1028, 1029, 1030, 1031, 1032, 1034, 1036, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1046, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1107, 1109, 1110, 1111, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1123, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1178, 1179, 1180, 1181, 1182, 1184, 1186, 1187, 1188, 1189, 1190, 1192, 1195, 1196, 1197, 1198, 1199, 1200, 1202, 1204, 1205, 1208, 1209, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1221, 1222, 1225, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1280, 1281, 1282, 1283, 1284, 1285, 1287, 1288, 1290, 1291, 1292, 1294, 1295, 1296, 1297, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1307, 1310, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1391, 1393, 1397, 1398, 1399, 1401, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1415, 1417, 1418, 1419, 1421, 1422, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1439, 1440, 1441, 1442, 1444, 1445, 1446, 1447, 1450, 1451, 1452, 1453, 1454, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1468, 1469, 1470, 1471, 1472, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1492, 1493, 1494, 1495, 1497, 1498, 1499, 1501, 1502, 1503, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1516, 1518, 1519, 1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545, 1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567, 1568, 1569, 1570, 1571, 1572, 1573, 1574, 1575, 1577, 1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589, 1590, 1592, 1594, 1597, 1598, 1599, 1600, 1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1611, 1612, 1613, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1624, 1625, 1626, 1627, 1628, 1629, 1631, 1632, 1633, 1634, 1635, 1636, 1638, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650, 1652, 1654, 1656, 1657, 1658, 1659, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1673, 1674, 1675, 1677, 1678, 1679, 1680, 1681, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1693, 1694, 1697, 1698, 1699, 1700, 1701, 1702, 1704, 1706, 1707, 1708, 1709, 1710, 1711, 1712, 1713, 1714, 1715, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723, 1724, 1725, 1726, 1727, 1728, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739, 1740, 1741, 1742, 1743, 1744, 1745, 1746, 1747, 1748, 1749, 1750, 1751, 1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 1767, 1768, 1770, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 1800, 1801, 1802, 1803, 1805, 1806, 1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833, 1834, 1835, 1836, 1837, 1838, 1840, 1841, 1842, 1844, 1845, 1846, 1847, 1848, 1849, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1866, 1867, 1868, 1869, 1871, 1872, 1873, 1874, 1875, 1876, 1877, 1878, 1879, 1880, 1881, 1882, 1883, 1884, 1885, 1886, 1887, 1888, 1889, 1890, 1891, 1893, 1894, 1895, 1896, 1897, 1898, 1899, 1900, 1902, 1903, 1904, 1907, 1908, 1909, 1910, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1925, 1927, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 1943, 1944, 1945, 1947, 1949, 1950, 1951, 1952, 1953, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1974, 1975, 1976, 1978, 1979, 1980, 1981, 1983, 1984, 1985, 1986, 1988, 1989, 1990, 1991, 1992, 1994, 1995, 1996, 1998, 1999, 2000, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2033, 2034, 2035, 2036, 2037, 2038, 2039, 2040, 2041, 2042, 2044, 2045, 2046, 2047, 2048, 2049, 2050, 2053, 2054, 2055, 2056, 2057, 2058, 2059, 2060, 2061, 2062, 2063, 2064, 2066, 2067, 2068, 2070, 2071, 2072, 2073, 2074, 2075, 2076, 2077, 2078, 2079, 2080, 2081, 2082, 2083, 2084, 2085, 2086, 2087, 2090, 2091, 2092, 2093, 2094, 2096, 2097, 2098, 2100, 2101, 2102, 2103, 2104, 2105, 2106, 2107, 2108, 2109, 2110, 2111, 2112, 2113, 2114, 2115, 2116, 2117, 2118, 2119, 2120, 2121, 2122, 2123, 2124, 2125, 2126, 2127, 2128, 2129, 2130, 2131, 2132, 2133, 2134, 2135, 2136, 2138, 2139, 2140, 2141, 2142, 2143, 2144, 2145, 2146, 2147, 2148, 2150, 2151, 2152, 2154, 2155, 2156, 2157, 2158, 2159, 2160, 2161, 2162, 2163, 2164, 2165, 2166, 2167, 2168, 2169, 2170, 2171, 2173, 2174, 2175, 2176, 2177, 2180, 2181, 2182, 2183, 2184, 2185, 2186, 2187, 2188, 2189, 2190, 2192, 2193, 2194, 2195, 2196, 2197, 2198, 2199, 2200, 2201, 2202, 2204, 2205, 2206, 2207, 2208, 2209, 2210, 2211, 2212, 2214, 2215, 2216, 2217, 2218, 2219, 2220, 2221, 2222, 2223, 2224, 2225, 2227, 2228, 2229, 2230, 2231, 2232, 2233, 2234, 2235, 2236, 2237, 2238, 2239, 2240, 2241, 2243, 2244, 2245, 2246, 2247, 2248, 2249, 2250, 2251, 2252, 2253, 2254, 2255, 2256, 2257, 2258, 2259, 2261, 2262, 2263, 2264, 2265, 2266, 2267, 2268, 2269, 2270, 2271, 2272, 2273, 2274, 2275, 2277, 2278, 2279, 2280, 2282, 2283, 2284, 2286, 2287, 2288, 2289, 2290, 2291, 2292, 2293, 2296, 2297, 2298, 2299, 2300, 2301, 2302, 2303, 2304, 2305, 2306, 2307, 2308, 2310, 2311, 2312, 2313, 2314, 2315, 2317, 2318, 2319, 2320, 2321, 2322, 2323, 2324, 2325, 2326, 2328, 2329, 2330, 2331, 2332, 2333, 2335, 2336, 2337, 2338, 2339, 2340, 2341, 2342, 2343, 2344, 2345, 2346, 2347, 2348, 2349, 2350, 2351, 2352, 2353, 2354, 2355, 2356, 2357, 2358, 2360, 2362, 2363, 2364, 2365, 2366, 2367, 2368, 2369, 2371, 2374, 2376, 2378, 2379, 2380, 2382, 2383, 2384, 2385, 2386, 2387, 2388, 2389, 2390, 2392, 2393, 2394, 2395, 2396, 2397, 2398, 2399, 2400, 2401, 2402, 2403, 2404, 2405, 2406, 2407, 2408, 2409, 2410, 2411, 2412, 2413, 2414, 2415, 2416, 2417, 2418, 2419, 2420, 2421, 2422, 2423, 2424, 2425, 2426, 2427, 2428, 2429, 2430, 2431, 2432, 2433, 2435, 2437, 2439, 2440, 2441, 2442, 2443, 2444, 2446, 2447, 2448, 2449, 2450, 2451, 2452, 2453, 2454, 2456, 2457, 2458, 2460, 2461, 2462, 2464, 2465, 2466, 2467, 2468, 2469, 2470, 2471, 2472, 2473, 2474, 2475, 2476, 2477, 2478, 2479, 2480, 2481, 2483, 2484, 2485, 2486, 2487, 2488, 2489, 2490, 2491, 2493, 2494, 2495, 2496, 2497, 2498, 2499, 2500, 2501, 2502, 2504, 2506, 2507, 2508, 2509, 2510, 2511, 2513, 2514, 2515, 2516, 2517, 2518, 2519, 2520, 2521, 2522, 2523, 2524, 2525, 2526, 2527, 2528, 2529, 2531, 2532, 2533, 2534, 2535, 2536, 2537, 2539, 2540, 2542, 2543, 2544, 2545, 2546, 2547, 2549, 2551, 2552, 2554, 2556, 2557, 2558, 2559, 2560, 2562, 2563, 2564, 2567, 2568, 2569, 2570, 2571, 2572, 2573, 2574, 2575, 2576, 2577, 2578, 2580, 2581, 2582, 2583, 2584, 2585, 2586, 2587, 2588, 2589, 2590, 2591, 2592, 2593, 2594, 2595, 2596, 2597, 2598, 2599, 2600, 2601, 2602, 2603, 2604, 2605, 2606, 2607, 2608, 2609, 2610, 2611, 2612, 2613, 2614, 2615, 2616, 2617, 2618, 2619, 2620, 2621, 2622, 2623, 2624, 2626, 2627, 2628, 2629, 2630, 2631, 2632, 2633, 2634, 2635, 2638, 2639, 2640, 2642, 2643, 2644, 2645, 2646, 2647, 2651, 2653, 2654, 2655, 2656, 2657, 2658, 2659, 2660, 2661, 2662, 2663, 2664, 2665, 2666, 2667, 2668, 2669, 2670, 2671, 2672, 2674, 2676, 2677, 2678, 2679, 2680, 2681, 2682, 2683, 2684, 2685, 2687, 2688, 2689, 2690, 2691, 2692, 2693, 2694, 2695, 2696, 2697, 2698, 2699, 2700, 2701, 2702, 2703, 2704, 2705, 2706, 2707, 2708, 2709, 2710, 2711, 2712, 2713, 2714, 2715, 2716, 2718, 2719, 2720, 2721, 2722, 2723, 2724, 2725, 2726, 2727, 2728, 2729, 2730, 2731, 2732, 2733, 2734, 2735, 2736, 2737, 2738, 2740, 2741, 2742, 2743, 2744, 2745, 2746, 2747, 2748, 2749, 2750, 2751, 2752, 2753, 2755, 2756, 2757, 2760, 2761, 2762, 2763, 2764, 2765, 2766, 2767, 2768, 2769, 2770, 2771, 2772, 2773, 2774, 2775, 2776, 2777, 2778, 2779, 2780, 2782, 2783, 2784, 2787, 2788, 2789, 2790, 2791, 2792, 2794, 2795, 2796, 2797, 2798, 2799, 2800, 2801, 2802, 2804, 2805, 2806, 2807, 2808, 2809, 2810, 2811, 2812, 2813, 2814, 2815, 2816, 2817, 2819, 2820, 2821, 2823, 2824, 2825, 2826, 2827, 2828, 2829, 2830, 2831, 2832, 2834, 2835, 2839, 2840, 2841, 2842, 2843, 2844, 2845, 2846, 2847, 2848, 2849, 2850, 2851, 2852, 2854, 2856, 2857, 2858, 2860, 2861, 2862, 2863, 2865, 2866, 2868, 2869, 2870, 2871, 2872, 2873, 2874, 2875, 2876, 2877, 2878, 2879, 2880, 2883, 2884, 2886, 2887, 2888, 2889, 2891, 2892, 2893, 2894, 2895, 2896, 2897, 2898, 2900, 2901, 2903, 2904, 2905, 2906, 2907, 2908, 2909, 2912, 2914, 2915, 2916, 2917, 2919, 2920, 2921, 2922, 2923, 2924, 2925, 2926, 2927, 2928, 2929, 2930, 2931, 2932, 2933, 2934, 2935, 2936, 2937, 2938, 2939, 2940, 2941, 2942, 2943, 2944, 2946, 2947, 2948, 2949, 2950, 2951, 2953, 2954, 2955, 2956, 2957, 2958, 2959, 2960, 2961, 2962, 2963, 2964, 2965, 2966, 2967, 2968, 2970, 2971, 2972, 2974, 2975, 2976, 2977, 2978, 2979, 2980, 2981, 2982, 2983, 2984, 2985, 2986, 2987, 2988, 2989, 2990, 2991, 2992, 2993, 2994, 2996, 2997, 2998, 2999, 3000, 3001, 3002, 3003, 3004, 3005, 3006, 3007, 3008, 3009, 3010, 3011, 3013, 3014, 3015, 3016, 3017, 3018, 3020, 3021, 3022, 3024, 3026, 3027, 3028, 3029, 3030, 3031, 3032, 3033, 3035, 3036, 3037, 3038, 3039, 3040, 3041, 3042, 3043, 3044, 3045, 3046, 3047, 3048, 3049, 3050, 3051, 3052, 3053, 3054, 3055, 3056, 3058, 3059, 3060, 3061, 3062, 3063, 3064, 3065, 3066, 3067, 3068, 3069, 3070, 3071], [1, 2, 3, 4, 6, 8, 9, 10, 11, 12, 13, 14, 15, 18, 19, 20, 23, 24, 25, 26, 28, 29, 33, 34, 35, 38, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 52, 53, 54, 55, 56, 58, 59, 60, 61, 63, 68, 69, 70, 71, 72, 73, 74, 75, 76, 78, 79, 81, 82, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 95, 96, 99, 101, 102, 103, 105, 107, 108, 109, 110, 111, 112, 114, 116, 118, 119, 120, 122, 123, 124, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 144, 145, 147, 148, 150, 151, 152, 153, 154, 155, 157, 158, 160, 161, 162, 163, 164, 165, 166, 168, 169, 170, 171, 172, 173, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 186, 187, 188, 189, 190, 192, 194, 195, 196, 197, 199, 200, 202, 203, 204, 205, 207, 208, 209, 210, 211, 213, 214, 215, 216, 217, 218, 219, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 233, 236, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 251, 252, 253, 255, 256, 257, 258, 259, 260, 262, 263, 264, 265, 267, 268, 269, 270, 273, 274, 275, 276, 277, 278, 279, 280, 282, 283, 284, 286, 288, 289, 290, 291, 292, 293, 295, 297, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 314, 315, 317, 318, 319, 321, 322, 323, 324, 325, 328, 329, 330, 331, 332, 333, 335, 336, 337, 338, 339, 341, 342, 343, 344, 345, 346, 347, 348, 349, 351, 352, 353, 354, 355, 356, 357, 358, 360, 361, 362, 363, 364, 365, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 381, 384, 385, 387, 389, 390, 391, 393, 394, 395, 396, 397, 398, 399, 400, 402, 403, 404, 405, 406, 407, 409, 410, 411, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 429, 431, 432, 433, 434, 435, 436, 437, 441, 442, 443, 445, 446, 447, 448, 451, 452, 453, 454, 455, 457, 458, 459, 460, 461, 464, 466, 468, 469, 470, 471, 472, 473, 474, 475, 477, 480, 481, 482, 484, 485, 486, 487, 488, 489, 490, 491, 493, 494, 495, 497, 498, 499, 500, 501, 502, 503, 505, 507, 508, 510, 511, 512, 513, 515, 517, 518, 519, 521, 523, 526, 528, 531, 532, 533, 535, 536, 537, 539, 541, 543, 544, 545, 546, 547, 548, 549, 551, 553, 554, 556, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 570, 572, 573, 574, 579, 581, 583, 585, 586, 587, 588, 589, 590, 591, 592, 594, 595, 596, 597, 598, 599, 600, 601, 602, 604, 605, 606, 607, 608, 609, 610, 612, 613, 614, 615, 620, 621, 623, 624, 625, 626, 627, 628, 629, 631, 632, 633, 634, 635, 636, 637, 639, 641, 642, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 658, 659, 660, 661, 663, 665, 666, 668, 669, 670, 671, 672, 673, 675, 678, 681, 682, 683, 684, 685, 688, 690, 691, 692, 693, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 710, 711, 714, 715, 716, 717, 718, 721, 722, 723, 724, 725, 726, 728, 729, 730, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 749, 751, 752, 753, 755, 757, 759, 761, 762, 763, 765, 766, 767, 768, 769, 770, 773, 775, 776, 777, 778, 779, 780, 783, 785, 786, 787, 789, 790, 791, 792, 793, 794, 796, 800, 801, 802, 805, 806, 807, 808, 809, 810, 811, 812, 813, 815, 816, 817, 818, 819, 821, 822, 823, 824, 825, 826, 827, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 869, 870, 872, 873, 875, 876, 877, 878, 879, 882, 883, 884, 885, 886, 887, 888, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 901, 902, 903, 904, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 926, 928, 929, 930, 931, 932, 933, 934, 936, 937, 938, 939, 940, 941, 942, 944, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 967, 970, 971, 972, 973, 975, 978, 979, 981, 982, 983, 984, 985, 987, 988, 990, 992, 993, 994, 995, 996, 997, 998, 999, 1002, 1003, 1004, 1006, 1007, 1008, 1010, 1011, 1012, 1013, 1014, 1016, 1018, 1019, 1020, 1021, 1022, 1023, 1026, 1027, 1029, 1031, 1032, 1033, 1035, 1036, 1037, 1038, 1040, 1041, 1042, 1043, 1044, 1045, 1047, 1048, 1049, 1050, 1051, 1052, 1054, 1055, 1056, 1057, 1059, 1060, 1061, 1062, 1064, 1065, 1066, 1068, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1087, 1088, 1089, 1090, 1091, 1092, 1094, 1095, 1096, 1097, 1098, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1124, 1125, 1126, 1129, 1131, 1132, 1133, 1134, 1137, 1138, 1139, 1141, 1142, 1143, 1145, 1146, 1148, 1149, 1150, 1151, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1164, 1165, 1166, 1167, 1169, 1170, 1171, 1172, 1174, 1175, 1177, 1178, 1180, 1181, 1182, 1184, 1185, 1187, 1188, 1189, 1190, 1191, 1193, 1194, 1195, 1196, 1197, 1198, 1200, 1201, 1202, 1203, 1204, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1215, 1216, 1217, 1218, 1219, 1220, 1223, 1224, 1225, 1226, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1259, 1260, 1262, 1263, 1264, 1265, 1267, 1268, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1283, 1284, 1286, 1287, 1289, 1291, 1293, 1294, 1296, 1297, 1298, 1299, 1300, 1302, 1303, 1304, 1305, 1306, 1307, 1309, 1310, 1311, 1314, 1315, 1316, 1317, 1320, 1322, 1324, 1325, 1326, 1327, 1328, 1329, 1331, 1332, 1333, 1335, 1336, 1337, 1338, 1339, 1340, 1342, 1343, 1344, 1345, 1346, 1348, 1349, 1350, 1352, 1353, 1354, 1355, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1368, 1370, 1371, 1372, 1373, 1376, 1377, 1378, 1379, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1403, 1404, 1406, 1407, 1409, 1411, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1452, 1453, 1454, 1456, 1457, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1469, 1471, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1516, 1518, 1519, 1522, 1523, 1524, 1525, 1526, 1527, 1529, 1530, 1531, 1533, 1535, 1536, 1539, 1540, 1541, 1542, 1543, 1544, 1546, 1547, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1567, 1568, 1569, 1570, 1571, 1572, 1573, 1574, 1576, 1577, 1578, 1579, 1580, 1582, 1583, 1584, 1585, 1587, 1588, 1589, 1590, 1593, 1594, 1595, 1596, 1598, 1599, 1601, 1602, 1603, 1604, 1606, 1607, 1608, 1609, 1610, 1611, 1612, 1613, 1615, 1616, 1617, 1618, 1619, 1622, 1623, 1626, 1627, 1628, 1629, 1630, 1631, 1632, 1633, 1636, 1638, 1643, 1644, 1646, 1647, 1648, 1651, 1652, 1653, 1654, 1656, 1657, 1658, 1659, 1660, 1661, 1664, 1665, 1666, 1667, 1668, 1669, 1670, 1671, 1673, 1674, 1676, 1677, 1678, 1679, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1692, 1693, 1694, 1696, 1697, 1698, 1699, 1700, 1701, 1702, 1704, 1705, 1706, 1707, 1708, 1709, 1710, 1715, 1716, 1718, 1720, 1721, 1722, 1723, 1725, 1726, 1728, 1729, 1730, 1732, 1733, 1735, 1736, 1737, 1739, 1741, 1742, 1744, 1746, 1747, 1748, 1749, 1750, 1751, 1752, 1753, 1754, 1757, 1758, 1760, 1761, 1762, 1767, 1768, 1769, 1770, 1773, 1774, 1775, 1777, 1779, 1780, 1781, 1782, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1794, 1795, 1796, 1797, 1798, 1799, 1800, 1801, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1812, 1813, 1814, 1816, 1818, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1833, 1835, 1838, 1839, 1840, 1841, 1842, 1843, 1844, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1856, 1859, 1863, 1864, 1867, 1868, 1869, 1870, 1874, 1875, 1878, 1880, 1881, 1882, 1883, 1884, 1885, 1888, 1889, 1891, 1892, 1893, 1895, 1896, 1897, 1898, 1899, 1901, 1902, 1903, 1904, 1905, 1907, 1908, 1909, 1910, 1911, 1912, 1913, 1914, 1915, 1916, 1918, 1919, 1920, 1921, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949, 1950, 1951, 1953, 1954, 1955, 1956, 1957, 1959, 1960, 1961, 1962, 1964, 1965, 1967, 1968, 1969, 1971, 1974, 1975, 1976, 1977, 1979, 1980, 1981, 1982, 1984, 1985, 1986, 1988, 1989, 1990, 1991, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2001, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2015, 2017, 2018, 2019, 2020, 2022, 2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2033, 2034, 2035, 2037, 2038, 2039, 2040, 2041, 2043, 2044, 2045, 2046, 2047, 2049, 2050, 2052, 2054, 2055, 2059, 2060, 2061, 2062, 2063, 2065, 2067, 2068, 2069, 2070, 2071, 2072, 2073, 2077, 2079, 2080, 2081, 2082, 2084, 2085, 2086, 2088, 2089, 2091, 2092, 2093, 2094, 2095, 2096, 2097, 2098, 2099, 2100, 2101, 2102, 2103, 2105, 2106, 2107, 2108, 2109, 2110, 2111, 2112, 2113, 2115, 2116, 2117, 2119, 2120, 2121, 2122, 2123, 2124, 2125, 2126, 2128, 2129, 2130, 2132, 2133, 2135, 2136, 2137, 2138, 2139, 2140, 2141, 2142, 2143, 2144, 2145, 2146, 2148, 2149, 2150, 2151, 2153, 2154, 2156, 2157, 2158, 2159, 2160, 2162, 2163, 2164, 2165, 2166, 2167, 2168, 2169, 2171, 2172, 2173, 2174, 2175, 2176, 2177, 2178, 2179, 2180, 2181, 2182, 2184, 2187, 2188, 2190, 2192, 2193, 2194, 2195, 2196, 2197, 2198, 2199, 2200, 2201, 2202, 2203, 2205, 2206, 2207, 2208, 2209, 2210, 2211, 2213, 2214, 2215, 2217, 2218, 2219, 2220, 2224, 2225, 2226, 2229, 2230, 2234, 2235, 2236, 2238, 2239, 2240, 2242, 2243, 2244, 2245, 2246, 2248, 2249, 2251, 2252, 2253, 2255, 2256, 2257, 2258, 2259, 2261, 2262, 2264, 2266, 2267, 2269, 2271, 2272, 2273, 2274, 2275, 2276, 2277, 2278, 2279, 2280, 2281, 2282, 2283, 2285, 2286, 2287, 2288, 2290, 2291, 2292, 2293, 2295, 2296, 2297, 2298, 2299, 2300, 2301, 2303, 2304, 2305, 2306, 2307, 2308, 2311, 2312, 2313, 2314, 2315, 2316, 2319, 2320, 2321, 2323, 2324, 2325, 2326, 2327, 2328, 2329, 2330, 2331, 2332, 2333, 2334, 2335, 2336, 2337, 2339, 2340, 2341, 2343, 2344, 2347, 2348, 2349, 2352, 2355, 2356, 2357, 2358, 2360, 2362, 2363, 2366, 2369, 2370, 2371, 2373, 2375, 2376, 2377, 2379, 2380, 2381, 2382, 2383, 2385, 2386, 2388, 2390, 2391, 2392, 2393, 2394, 2395, 2396, 2397, 2400, 2402, 2403, 2404, 2406, 2407, 2408, 2410, 2413, 2414, 2415, 2416, 2417, 2418, 2419, 2420, 2421, 2423, 2424, 2426, 2427, 2428, 2429, 2430, 2431, 2432, 2433, 2435, 2436, 2438, 2439, 2440, 2441, 2443, 2445, 2446, 2447, 2448, 2449, 2451, 2452, 2453, 2454, 2456, 2457, 2458, 2459, 2460, 2462, 2464, 2465, 2467, 2469, 2470, 2471, 2472, 2473, 2474, 2476, 2477, 2478, 2479, 2482, 2483, 2485, 2486, 2487, 2488, 2489, 2490, 2491, 2492, 2493, 2494, 2498, 2499, 2500, 2501, 2502, 2503, 2505, 2506, 2507, 2508, 2510, 2511, 2512, 2513, 2514, 2515, 2516, 2517, 2518, 2519, 2520, 2521, 2522, 2523, 2524, 2525, 2526, 2528, 2531, 2533, 2534, 2535, 2536, 2537, 2538, 2539, 2540, 2541, 2542, 2543, 2544, 2545, 2546, 2547, 2548, 2550, 2552, 2553, 2554, 2555, 2556, 2557, 2558, 2560, 2561, 2562, 2563, 2565, 2566, 2567, 2569, 2570, 2572, 2574, 2575, 2577, 2582, 2584, 2585, 2586, 2587, 2588, 2589, 2590, 2591, 2592, 2593, 2594, 2596, 2597, 2599, 2601, 2603, 2604, 2605, 2606, 2607, 2608, 2609, 2610, 2611, 2613, 2614, 2615, 2616, 2617, 2618, 2619, 2620, 2622, 2623, 2627, 2628, 2629, 2630, 2631, 2634, 2635, 2636, 2637, 2638, 2639, 2641, 2643, 2644, 2646, 2649, 2651, 2652, 2653, 2655, 2657, 2658, 2660, 2661, 2662, 2663, 2664, 2665, 2667, 2668, 2669, 2670, 2671, 2672, 2674, 2675, 2676, 2677, 2678, 2679, 2680, 2681, 2683, 2684, 2685, 2686, 2687, 2688, 2689, 2690, 2691, 2693, 2694, 2695, 2696, 2697, 2698, 2699, 2700, 2701, 2702, 2703, 2706, 2707, 2708, 2710, 2711, 2712, 2713, 2714, 2715, 2716, 2717, 2719, 2720, 2721, 2722, 2723, 2726, 2727, 2728, 2729, 2730, 2731, 2733, 2735, 2738, 2739, 2740, 2743, 2744, 2745, 2746, 2747, 2748, 2749, 2750, 2751, 2752, 2754, 2755, 2756, 2758, 2759, 2760, 2761, 2762, 2763, 2764, 2766, 2767, 2768, 2769, 2770, 2772, 2773, 2776, 2780, 2781, 2782, 2783, 2784, 2785, 2786, 2787, 2788, 2790, 2791, 2792, 2793, 2794, 2796, 2797, 2798, 2799, 2800, 2801, 2803, 2806, 2807, 2809, 2810, 2811, 2812, 2814, 2815, 2816, 2817, 2818, 2819, 2821, 2822, 2823, 2824, 2825, 2826, 2827, 2829, 2830, 2831, 2832, 2834, 2835, 2836, 2839, 2840, 2841, 2842, 2843, 2844, 2845, 2846, 2847, 2848, 2849, 2850, 2851, 2852, 2853, 2854, 2855, 2856, 2858, 2860, 2862, 2864, 2865, 2866, 2867, 2868, 2869, 2870, 2871, 2872, 2873, 2874, 2875, 2876, 2877, 2878, 2880, 2881, 2882, 2883, 2884, 2886, 2887, 2888, 2889, 2891, 2892, 2893, 2894, 2895, 2897, 2898, 2899, 2901, 2902, 2903, 2905, 2906, 2907, 2908, 2909, 2910, 2911, 2912, 2916, 2918, 2919, 2921, 2922, 2925, 2926, 2927, 2928, 2929, 2930, 2932, 2934, 2935, 2936, 2937, 2938, 2939, 2940, 2942, 2944, 2945, 2948, 2949, 2950, 2951, 2952, 2953, 2954, 2955, 2956, 2957, 2959, 2960, 2961, 2962, 2963, 2964, 2965, 2966, 2967, 2970, 2971, 2972, 2974, 2976, 2977, 2978, 2981, 2982, 2983, 2984, 2985, 2987, 2988, 2990, 2991, 2992, 2993, 2994, 2995, 2997, 2998, 3001, 3002, 3003, 3004, 3005, 3007, 3008, 3009, 3010, 3014, 3015, 3016, 3017, 3018, 3019, 3024, 3026, 3028, 3029, 3030, 3032, 3033, 3034, 3035, 3036, 3037, 3038, 3041, 3042, 3043, 3044, 3046, 3047, 3048, 3049, 3050, 3051, 3053, 3055, 3056, 3059, 3060, 3061, 3062, 3064, 3065, 3066, 3067, 3068, 3070, 3071], [4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 23, 25, 27, 29, 30, 32, 35, 36, 37, 42, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 56, 57, 60, 61, 62, 63, 64, 66, 67, 69, 70, 71, 73, 74, 75, 76, 79, 81, 82, 83, 84, 85, 86, 87, 88, 90, 91, 93, 94, 95, 98, 100, 101, 102, 103, 107, 108, 109, 110, 111, 112, 114, 115, 116, 117, 120, 121, 122, 123, 124, 125, 126, 128, 131, 132, 134, 135, 137, 138, 139, 141, 142, 144, 145, 146, 147, 149, 150, 151, 153, 154, 155, 156, 157, 159, 160, 161, 162, 164, 165, 166, 167, 168, 171, 174, 176, 178, 179, 180, 181, 182, 183, 184, 187, 188, 189, 190, 193, 194, 197, 199, 200, 202, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 215, 216, 218, 219, 220, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 235, 238, 239, 241, 242, 243, 244, 245, 246, 247, 248, 249, 251, 252, 253, 254, 255, 256, 260, 261, 263, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 281, 282, 283, 284, 285, 288, 290, 292, 294, 296, 297, 298, 302, 303, 304, 305, 306, 307, 308, 310, 311, 313, 314, 315, 316, 319, 323, 324, 326, 327, 328, 329, 330, 334, 336, 340, 341, 345, 346, 347, 349, 350, 351, 353, 355, 356, 358, 360, 361, 362, 364, 365, 366, 368, 369, 370, 371, 372, 375, 376, 378, 379, 380, 381, 384, 386, 387, 389, 390, 391, 393, 395, 397, 398, 399, 400, 402, 403, 404, 406, 407, 408, 410, 411, 412, 413, 415, 416, 417, 418, 419, 421, 425, 427, 428, 430, 431, 432, 433, 435, 436, 437, 439, 441, 443, 444, 446, 447, 448, 450, 453, 454, 456, 457, 459, 462, 463, 464, 466, 467, 469, 470, 474, 475, 476, 477, 479, 481, 482, 483, 485, 486, 487, 489, 490, 491, 492, 493, 494, 495, 496, 498, 499, 500, 501, 503, 504, 505, 506, 507, 508, 514, 515, 516, 518, 520, 522, 525, 526, 527, 529, 530, 531, 532, 533, 534, 535, 538, 541, 542, 544, 545, 547, 548, 549, 551, 553, 555, 556, 557, 558, 559, 560, 561, 563, 564, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 583, 584, 587, 589, 590, 591, 594, 596, 597, 599, 601, 602, 603, 606, 607, 608, 609, 614, 615, 616, 618, 619, 620, 621, 622, 628, 629, 630, 631, 632, 633, 634, 636, 637, 638, 642, 646, 647, 648, 649, 650, 651, 653, 654, 656, 657, 658, 660, 661, 662, 663, 664, 665, 666, 667, 671, 673, 674, 675, 676, 677, 678, 680, 681, 682, 684, 686, 687, 690, 694, 695, 697, 698, 700, 701, 702, 703, 705, 708, 709, 710, 711, 712, 715, 716, 717, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 731, 735, 736, 737, 741, 742, 743, 744, 746, 747, 748, 749, 750, 751, 753, 754, 755, 756, 757, 758, 760, 761, 763, 764, 765, 766, 767, 768, 770, 771, 772, 773, 774, 777, 778, 780, 781, 783, 787, 788, 789, 790, 792, 793, 797, 798, 800, 801, 804, 807, 809, 810, 812, 814, 815, 816, 817, 818, 820, 821, 822, 824, 825, 827, 828, 829, 831, 832, 833, 835, 836, 837, 838, 840, 841, 842, 843, 844, 845, 849, 850, 851, 852, 853, 855, 856, 857, 858, 859, 860, 864, 865, 870, 871, 872, 873, 874, 875, 876, 878, 879, 882, 883, 884, 885, 887, 888, 891, 892, 893, 895, 896, 897, 898, 899, 900, 901, 902, 903, 905, 906, 907, 908, 909, 910, 912, 913, 915, 916, 917, 919, 920, 921, 922, 923, 924, 925, 927, 928, 930, 931, 932, 933, 934, 939, 940, 941, 942, 943, 944, 946, 947, 948, 951, 954, 955, 956, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 969, 971, 972, 973, 976, 977, 979, 980, 981, 983, 984, 988, 989, 991, 992, 993, 994, 999, 1002, 1004, 1005, 1006, 1008, 1009, 1011, 1012, 1013, 1014, 1015, 1016, 1018, 1019, 1021, 1023, 1024, 1025, 1026, 1028, 1029, 1030, 1033, 1035, 1036, 1037, 1038, 1039, 1043, 1044, 1045, 1047, 1048, 1049, 1050, 1051, 1052, 1054, 1055, 1057, 1059, 1062, 1063, 1065, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1075, 1076, 1078, 1079, 1080, 1081, 1082, 1083, 1085, 1086, 1088, 1089, 1090, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1102, 1103, 1104, 1105, 1108, 1110, 1111, 1112, 1113, 1114, 1115, 1117, 1118, 1121, 1122, 1124, 1128, 1129, 1130, 1131, 1132, 1133, 1136, 1138, 1139, 1140, 1143, 1144, 1145, 1146, 1148, 1149, 1150, 1153, 1154, 1155, 1156, 1157, 1158, 1160, 1161, 1163, 1164, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1174, 1176, 1178, 1180, 1181, 1182, 1183, 1184, 1186, 1188, 1190, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1201, 1202, 1203, 1204, 1208, 1209, 1210, 1211, 1212, 1214, 1215, 1216, 1219, 1220, 1222, 1223, 1224, 1226, 1229, 1233, 1234, 1235, 1236, 1237, 1239, 1241, 1243, 1245, 1246, 1247, 1248, 1249, 1250, 1252, 1253, 1254, 1255, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1269, 1270, 1273, 1274, 1275, 1278, 1279, 1280, 1281, 1282, 1283, 1285, 1286, 1287, 1290, 1291, 1292, 1295, 1296, 1297, 1298, 1299, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1314, 1315, 1318, 1319, 1320, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1333, 1335, 1336, 1338, 1339, 1340, 1341, 1342, 1344, 1346, 1347, 1348, 1351, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1367, 1369, 1370, 1373, 1375, 1376, 1380, 1381, 1384, 1385, 1386, 1387, 1388, 1390, 1393, 1394, 1395, 1396, 1397, 1399, 1401, 1403, 1404, 1405, 1406, 1408, 1409, 1410, 1411, 1414, 1415, 1416, 1417, 1419, 1420, 1421, 1423, 1425, 1426, 1427, 1429, 1430, 1432, 1436, 1438, 1439, 1440, 1443, 1444, 1445, 1446, 1448, 1449, 1450, 1451, 1452, 1453, 1456, 1457, 1460, 1461, 1462, 1463, 1464, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1480, 1481, 1482, 1483, 1484, 1485, 1487, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1497, 1501, 1502, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1514, 1516, 1517, 1518, 1521, 1522, 1523, 1524, 1526, 1527, 1529, 1535, 1536, 1538, 1540, 1541, 1542, 1544, 1545, 1546, 1549, 1551, 1553, 1555, 1557, 1558, 1559, 1564, 1565, 1566, 1569, 1571, 1573, 1574, 1577, 1578, 1579, 1581, 1582, 1583, 1588, 1589, 1590, 1592, 1593, 1597, 1598, 1599, 1601, 1602, 1603, 1604, 1607, 1610, 1612, 1614, 1616, 1618, 1619, 1620, 1621, 1622, 1623, 1625, 1626, 1627, 1629, 1630, 1631, 1632, 1633, 1634, 1638, 1639, 1641, 1647, 1648, 1649, 1651, 1653, 1654, 1655, 1656, 1657, 1658, 1659, 1661, 1662, 1663, 1664, 1667, 1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1679, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1693, 1694, 1697, 1699, 1702, 1703, 1704, 1706, 1708, 1711, 1712, 1713, 1715, 1718, 1720, 1721, 1722, 1723, 1724, 1725, 1727, 1728, 1729, 1730, 1731, 1734, 1735, 1736, 1738, 1740, 1741, 1743, 1746, 1747, 1749, 1750, 1751, 1752, 1756, 1757, 1758, 1762, 1763, 1764, 1765, 1767, 1768, 1769, 1770, 1772, 1773, 1775, 1777, 1778, 1779, 1780, 1782, 1784, 1785, 1786, 1787, 1790, 1791, 1792, 1796, 1797, 1798, 1799, 1800, 1801, 1803, 1804, 1807, 1808, 1809, 1810, 1812, 1814, 1815, 1817, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1830, 1831, 1832, 1835, 1836, 1837, 1838, 1841, 1843, 1844, 1846, 1850, 1852, 1854, 1857, 1861, 1862, 1864, 1865, 1866, 1868, 1869, 1870, 1871, 1874, 1877, 1879, 1881, 1882, 1884, 1888, 1889, 1890, 1892, 1893, 1894, 1898, 1900, 1902, 1903, 1904, 1905, 1906, 1907, 1910, 1911, 1912, 1913, 1915, 1917, 1918, 1919, 1920, 1921, 1922, 1924, 1926, 1928, 1929, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1938, 1939, 1940, 1941, 1942, 1943, 1944, 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1956, 1957, 1959, 1961, 1962, 1964, 1966, 1968, 1970, 1971, 1972, 1973, 1975, 1976, 1977, 1978, 1981, 1982, 1983, 1985, 1986, 1987, 1988, 1990, 1991, 1992, 1994, 1995, 1996, 1997, 1999, 2001, 2002, 2003, 2004, 2005, 2006, 2009, 2011, 2012, 2013, 2014, 2015, 2017, 2018, 2019, 2020, 2021, 2022, 2024, 2026, 2027, 2028, 2029, 2030, 2031, 2035, 2036, 2037, 2038, 2039, 2040, 2041, 2042, 2043, 2045, 2046, 2048, 2049, 2050, 2051, 2052, 2054, 2055, 2056, 2057, 2058, 2059, 2060, 2062, 2063, 2064, 2066, 2067, 2068, 2070, 2072, 2073, 2074, 2075, 2076, 2077, 2078, 2079, 2082, 2083, 2085, 2086, 2087, 2088, 2090, 2091, 2094, 2096, 2098, 2099, 2101, 2102, 2106, 2107, 2109, 2110, 2111, 2112, 2113, 2114, 2115, 2116, 2117, 2119, 2120, 2121, 2122, 2123, 2126, 2127, 2129, 2130, 2131, 2132, 2133, 2134, 2136, 2137, 2138, 2140, 2141, 2142, 2144, 2145, 2147, 2148, 2149, 2150, 2152, 2153, 2156, 2159, 2160, 2162, 2164, 2165, 2166, 2168, 2169, 2170, 2172, 2174, 2175, 2178, 2180, 2181, 2182, 2183, 2184, 2185, 2186, 2188, 2192, 2196, 2197, 2198, 2199, 2201, 2202, 2203, 2204, 2208, 2211, 2212, 2213, 2215, 2217, 2218, 2220, 2222, 2223, 2224, 2226, 2227, 2228, 2229, 2230, 2231, 2232, 2233, 2234, 2236, 2238, 2239, 2242, 2243, 2244, 2245, 2247, 2248, 2250, 2251, 2252, 2253, 2254, 2256, 2259, 2260, 2261, 2262, 2263, 2265, 2268, 2269, 2270, 2272, 2273, 2274, 2275, 2278, 2279, 2280, 2281, 2282, 2283, 2284, 2287, 2288, 2289, 2290, 2291, 2292, 2293, 2294, 2295, 2297, 2298, 2299, 2301, 2302, 2303, 2305, 2306, 2307, 2308, 2309, 2311, 2312, 2314, 2315, 2317, 2318, 2319, 2320, 2321, 2322, 2323, 2325, 2327, 2330, 2332, 2333, 2334, 2337, 2340, 2341, 2342, 2344, 2346, 2347, 2349, 2351, 2352, 2353, 2354, 2355, 2359, 2360, 2361, 2362, 2364, 2366, 2367, 2368, 2369, 2370, 2371, 2372, 2373, 2374, 2377, 2379, 2382, 2386, 2388, 2389, 2391, 2392, 2393, 2394, 2398, 2400, 2401, 2402, 2405, 2407, 2408, 2410, 2411, 2412, 2413, 2414, 2415, 2418, 2419, 2420, 2421, 2422, 2423, 2425, 2428, 2430, 2431, 2432, 2433, 2435, 2436, 2439, 2440, 2441, 2442, 2443, 2444, 2446, 2447, 2448, 2449, 2450, 2451, 2452, 2454, 2455, 2456, 2457, 2458, 2460, 2461, 2463, 2465, 2466, 2467, 2468, 2469, 2471, 2472, 2474, 2476, 2477, 2478, 2479, 2483, 2484, 2485, 2486, 2489, 2492, 2493, 2494, 2495, 2496, 2497, 2498, 2499, 2500, 2501, 2502, 2503, 2504, 2506, 2507, 2508, 2509, 2510, 2511, 2513, 2514, 2515, 2516, 2518, 2519, 2520, 2522, 2524, 2525, 2527, 2528, 2531, 2532, 2535, 2538, 2541, 2542, 2543, 2545, 2546, 2547, 2549, 2550, 2552, 2553, 2554, 2556, 2557, 2558, 2559, 2560, 2562, 2563, 2566, 2567, 2571, 2572, 2573, 2574, 2575, 2577, 2578, 2579, 2580, 2581, 2582, 2583, 2584, 2585, 2586, 2588, 2590, 2592, 2593, 2594, 2595, 2596, 2597, 2598, 2601, 2603, 2604, 2608, 2609, 2610, 2612, 2613, 2615, 2616, 2618, 2619, 2620, 2621, 2623, 2624, 2626, 2627, 2628, 2629, 2630, 2631, 2632, 2633, 2635, 2636, 2637, 2638, 2639, 2640, 2641, 2642, 2643, 2644, 2646, 2647, 2648, 2650, 2652, 2653, 2654, 2655, 2656, 2657, 2658, 2660, 2663, 2664, 2665, 2666, 2667, 2669, 2670, 2671, 2672, 2674, 2675, 2676, 2678, 2681, 2682, 2683, 2686, 2688, 2689, 2690, 2691, 2693, 2694, 2695, 2696, 2697, 2698, 2699, 2700, 2701, 2702, 2703, 2704, 2705, 2706, 2710, 2712, 2713, 2715, 2716, 2717, 2718, 2719, 2721, 2722, 2723, 2725, 2726, 2727, 2728, 2730, 2731, 2732, 2735, 2738, 2739, 2741, 2742, 2743, 2745, 2746, 2747, 2748, 2749, 2750, 2751, 2752, 2753, 2754, 2755, 2757, 2758, 2760, 2761, 2762, 2763, 2764, 2765, 2767, 2769, 2770, 2771, 2773, 2774, 2775, 2776, 2777, 2779, 2781, 2783, 2785, 2786, 2787, 2788, 2789, 2791, 2792, 2795, 2798, 2799, 2800, 2801, 2804, 2805, 2807, 2809, 2810, 2811, 2812, 2813, 2814, 2815, 2816, 2817, 2818, 2819, 2820, 2821, 2823, 2825, 2826, 2827, 2828, 2829, 2830, 2831, 2832, 2833, 2836, 2837, 2838, 2840, 2841, 2842, 2844, 2846, 2853, 2854, 2855, 2857, 2858, 2859, 2860, 2861, 2862, 2863, 2864, 2865, 2867, 2870, 2872, 2874, 2876, 2877, 2878, 2879, 2881, 2882, 2883, 2884, 2885, 2887, 2888, 2889, 2890, 2891, 2893, 2894, 2895, 2896, 2897, 2898, 2899, 2901, 2902, 2905, 2906, 2907, 2908, 2909, 2911, 2912, 2914, 2915, 2916, 2917, 2918, 2919, 2920, 2923, 2925, 2926, 2929, 2930, 2934, 2936, 2937, 2938, 2939, 2940, 2942, 2943, 2944, 2945, 2947, 2948, 2949, 2950, 2951, 2953, 2954, 2955, 2956, 2957, 2958, 2959, 2960, 2963, 2964, 2965, 2966, 2967, 2968, 2969, 2970, 2971, 2972, 2973, 2974, 2975, 2976, 2977, 2979, 2980, 2982, 2989, 2990, 2991, 2992, 2993, 2994, 2996, 2997, 2998, 2999, 3000, 3001, 3002, 3003, 3005, 3006, 3008, 3009, 3010, 3011, 3012, 3013, 3014, 3015, 3016, 3017, 3021, 3023, 3024, 3025, 3026, 3027, 3030, 3031, 3032, 3033, 3034, 3035, 3036, 3037, 3038, 3039, 3040, 3043, 3044, 3045, 3046, 3047, 3049, 3050, 3051, 3052, 3053, 3054, 3057, 3058, 3059, 3060, 3062, 3064, 3065, 3066, 3068, 3069, 3070, 3071]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#zero out the weights except presrved neurons.\n",
        "preserved_neuron_list = [[],\n",
        "                       [],\n",
        "                       [],\n",
        "                       [],\n",
        "                       [],\n",
        "                       [],\n",
        "                       [],\n",
        "                       [],\n",
        "                       [],\n",
        "                       [],\n",
        "                       [804, 1410, 1521, 1783, 2931],\n",
        "                       [309, 458, 480, 598, 796, 847, 869, 975, 1120, 1126, 1231, 1289, 1322, 1428, 1575, 1637, 1860, 1998, 2375, 2378, 2600, 2822]]\n",
        "\n",
        "num_neurons = 3072\n",
        "masks = []\n",
        "\n",
        "\n",
        "for i, masked_neurons in enumerate(masked_neurons_list):\n",
        "    mask = np.ones(num_neurons)\n",
        "    if masked_neurons not in preserved_neuron_list[i]:\n",
        "      mask[masked_neurons] = 0\n",
        "      masks.append(mask)"
      ],
      "metadata": {
        "id": "fGSECIWzfOT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#zero out -> quantization + sparse matrix pruning\n",
        "# Assuming bert_model is your pre-trained BERT model\n",
        "#tf_bert_model/bert/encoder/layer_._0/output/dense/kernel\n",
        "#  f\"tfgpt2lm_head_model/transformer/h_._{layer_num}/mlp/c_proj/weight:0\"\n",
        "\n",
        "\n",
        "for var in gpt2_model.variables:\n",
        "    if 'mlp/c_proj/weight' in var.name:\n",
        "        # Extract layer number from variable name\n",
        "        layer_num = int(var.name.split('/')[2].split('_')[2])\n",
        "\n",
        "        # Get the current weights\n",
        "        weights = var.numpy()\n",
        "\n",
        "        # Apply the mask #(3072,)\n",
        "        mask = masks[layer_num]\n",
        "        weights *= mask.reshape(-1, 1)  # Reshape mask and apply to weights\n",
        "\n",
        "        # Assign the modified weights back to the variable\n",
        "        var.assign(weights)\n"
      ],
      "metadata": {
        "id": "E7cQvfiJfOaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#to check in if the weights are correctly zero out\n",
        "for var in gpt2_model.variables:\n",
        "    if 'mlp/c_proj/weight' in var.name:\n",
        "        print(var.name, var.numpy()[0:3, 0:3])  # Print a small section of the weights"
      ],
      "metadata": {
        "id": "eaufJLBLfZKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#bert_model has been zeroed out.\n",
        "gpt_logit_model_classification.evaluate(gpt_test_inputs, gpt_test_labels)\n",
        "\n",
        "\n",
        "#            accuracy process time (1000 inputs)\n",
        "# 0%        --  0.825 23.6\n",
        "#alpha 0.01 --  0.48 10.28 (Remove all negative as well) (0.825 23.6)\n",
        "\n",
        "# 0%        --  0.867, 23.1\n",
        "#alpha 0.05  -- 0.863 20.54 (0.867, 23.1) (3times only in layer 11 )    5640\n",
        "#alpha 0.075 --  0.836 20.55 (0.867, 23.1) (3times only in layer 11 )   5965\n",
        "#alpha 0.1 --   0.808 20.65 (0.867, 23.1) (3times only in layer 11 )    7554\n",
        "#alpha 0.125 -- 0.763 20.6  (0.867, 23.1) (3times only in layer 11 )    9907\n",
        "#alpha 0.15 --  0.667 20.53 (0.867, 23.1) (3times only in layer 11 )    11668\n",
        "#alpha 0.15 -- 0.783  20.53 (0.867, 23.1)(with layer 10+11)             13074\n",
        "#alpha 0.15 -- 0.837  12.98 (0.867, 23.1)(with layer 9+10+11)           15224\n",
        "#alpha 0.15 -- 0.829  20.54 (0.867, 23.1)(with layer 8+9+10+11)         17607\n",
        "#alpha 0.15 -- 0.820  20.53 (0.867, 23.1)(with layer 7+8+9+10+11)       23715\n",
        "#alpha 0.15 -- 0.684  12.54 (0.867, 23.1)(with layer 6+7+8+9+10+11)     24388\n",
        "\n",
        "\n",
        " #some neruons in the layer10 are stop model from getting correct answer...\n",
        "\n",
        "\n",
        "#alpha 0.05  -- 0.833, 20.69  (0.849,23.38) (layer 11)                 3905\n",
        "#alpha 0.075 -- 0.773, 20.52 (0.816,24.08) (layer 11)                  6175\n",
        "#alpha 0.1 --   0.805, 20.53 (0.816,24.08) (layer 11)                  6841\n",
        "#alpha 0.125 -- 0.787, 20.53 (0.816,24.08) (layer 11)                  7980\n",
        "#alpha 0.15 --  0.767, 10.93/10.62/20.55 (0.816,24.08) (layer 11)      9588\n",
        "#alpha 0.15 --  0.813, 10.68/10.76/20.52 (0.816,24.08) (layer 10,11)   11596\n",
        "#alpha 0.15 --  0.787, 10.69/20.54 (0.816,24.08) (layer 9,10,11)       12384\n",
        "#alpha 0.15 --  0.778, 20.66 (0.816,24.08) (layer 8,9,10,11)           15641\n",
        "#alpha 0.15 --  0.735, 20.52 (0.816,24.08) (layer 7,8,9,10,11)         18469\n",
        "#alpha 0.15 --  0.725, 10.71/20.55 (0.816,24.08) (layer 6,7,8,9,10,11) 23045\n",
        "#alpha 0.15 --  0.749, 20.52 (0.816,24.08) (layer 5,6,7,8,9,10,11)     23625\n",
        "\n",
        "#alpha 0.1  -- 0.584  12.47 (0.792, 23.04) (layer 9.10,11)\n",
        "#alpha 0.125  -- 0.554  20.52 (0.792, 23.04) (layer 9,10,11)\n",
        "#alpha 0.15  -- 0.547 20.52 (0.792, 23.04) (layer 9,10,11)\n",
        "\n",
        "#alpha 0.05  -- 0.817, 20.53 (0.816,24.08) (layer 9,10,11)\n",
        "#alpha 0.075 -- 0.822, 20.52 (0.816,24.08) (layer 9,10,11)\n",
        "#alpha 0.1 --   0.817, 20.52 (0.816,24.08) (layer 9,10,11)\n",
        "#alpha 0.125 -- 0.818, 20.54  (0.816,24.08) (layer 9,10,11)\n",
        "#alpha 0.15 --  0.817 20.52 (0.816,24.08) (layer 9,10,11)\n",
        "#alpha 0.15 --  0.819 20.52 (0.816,24.08) (layer 8, 9,10,11)\n",
        "#alpha 0.15 --  0.828  20.53 (0.816,24.08) (layer 7, 8,9,10,11)\n",
        "#alpha 0.15 --  0.822  10.45/10.6 (0.816,24.08) (layer 6,7,8,9,10,11)\n",
        "#alpha 0.15 --  0.768  20.53 (0.816,24.08) (layer 5,6,7,8,9,10,11)\n",
        "#alpha 0.15 --  0.768 20.52 (0.816,24.08) (layer 4,5,6,7,8,9,10,11)\n",
        "#alpha 0.15 --  0.72 20.63 (0.816,24.08) (layer 3,4,5,6,7,8,9,10,11)\n",
        "\n",
        "#There are some neruons in layer 9,10 are hindering model from getting correct answer\n",
        "#lots neuron in layer 7,8 can also be pruned off since they are literally useless\n",
        "#but in layer6, if you prune off too much neuron, then it may affect the ability of capturing patterns.\n",
        "#it is shown in the layer similarity in the plot of input5, input 5 can reveal the mechanism of information delivery inside the model (input5 = Negative plotting)\n",
        "# But the accuracy is by testing data.\n",
        "# Re test it again using best hyperparameter layer 9~11 and alpha =0.1, 0.125, 0.15\n",
        "# scan top down and bottom up.\n",
        "# the inference time fluate a bit, don't know why"
      ],
      "metadata": {
        "id": "-TC4HBg7facE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "358db888-8d98-4252-8801-f43c0b957d9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 11s 330ms/step - loss: 1.8163 - accuracy: 0.7490\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.8162636756896973, 0.7490000128746033]"
            ]
          },
          "metadata": {},
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "prediction = gpt_logit_model_classification.predict(gpt_test_inputs)\n",
        "end_time = time.time()\n",
        "\n",
        "elapsed_time = end_time - start_time\n",
        "print(\"Elapsed time: {:.2f} seconds\".format(elapsed_time))"
      ],
      "metadata": {
        "id": "wbT5W9Udfaeh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e212f6c-9a91-44cf-b45e-ce3a18e62b12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 11s 340ms/step\n",
            "Elapsed time: 20.52 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "alpha 0.05  -- 0.817, 20.53 (0.849,23.38) (layer 11)\n",
        "Threshold of pruning -0.007642577588558197\n",
        "number of neruons being masked in layer 0: 0\n",
        "Threshold of pruning 0.0019928794354200363\n",
        "number of neruons being masked in layer 1: 130\n",
        "Threshold of pruning 0.002078031562268734\n",
        "number of neruons being masked in layer 2: 133\n",
        "Threshold of pruning 0.0029277296736836436\n",
        "number of neruons being masked in layer 3: 183\n",
        "Threshold of pruning 0.003848516568541527\n",
        "number of neruons being masked in layer 4: 236\n",
        "Threshold of pruning 0.004171434044837952\n",
        "number of neruons being masked in layer 5: 241\n",
        "Threshold of pruning 0.004785158857703209\n",
        "number of neruons being masked in layer 6: 284\n",
        "Threshold of pruning 0.0062826514244079595\n",
        "number of neruons being masked in layer 7: 375\n",
        "Threshold of pruning 0.007379318773746491\n",
        "number of neruons being masked in layer 8: 374\n",
        "Threshold of pruning 0.006629287451505661\n",
        "number of neruons being masked in layer 9: 332\n",
        "Threshold of pruning 0.0061324663460254674\n",
        "number of neruons being masked in layer 10: 187\n",
        "Threshold of pruning 0.007414974272251129\n",
        "number of neruons being masked in layer 11: 1430\n",
        "\n",
        "\n",
        "3,905\n",
        "\n",
        "alpha 0.075 -- 0.822, 20.52 (0.816,24.08) (layer 11)\n",
        "Threshold of pruning -0.011463866382837294\n",
        "number of neruons being masked in layer 0: 0\n",
        "Threshold of pruning 0.0029893191531300545\n",
        "number of neruons being masked in layer 1: 178\n",
        "Threshold of pruning 0.003117047343403101\n",
        "number of neruons being masked in layer 2: 198\n",
        "Threshold of pruning 0.004391594510525465\n",
        "number of neruons being masked in layer 3: 269\n",
        "Threshold of pruning 0.00577277485281229\n",
        "number of neruons being masked in layer 4: 367\n",
        "Threshold of pruning 0.006257151067256927\n",
        "number of neruons being masked in layer 5: 358\n",
        "Threshold of pruning 0.007177738286554813\n",
        "number of neruons being masked in layer 6: 446\n",
        "Threshold of pruning 0.009423977136611939\n",
        "number of neruons being masked in layer 7: 547\n",
        "Threshold of pruning 0.011068978160619735\n",
        "number of neruons being masked in layer 8: 579\n",
        "Threshold of pruning 0.00994393117725849\n",
        "number of neruons being masked in layer 9: 496\n",
        "Threshold of pruning 0.0091986995190382\n",
        "number of neruons being masked in layer 10: 295\n",
        "Threshold of pruning 0.011122461408376694\n",
        "number of neruons being masked in layer 11: 1632\n",
        "\n",
        "6175\n",
        "\n",
        "alpha 0.1 --   0.817, 20.52 (0.816,24.08) (layer 11)\n",
        "Threshold of pruning -0.015285155177116394\n",
        "number of neruons being masked in layer 0: 0\n",
        "Threshold of pruning 0.003985758870840073\n",
        "number of neruons being masked in layer 1: 241\n",
        "Threshold of pruning 0.004156063124537468\n",
        "number of neruons being masked in layer 2: 277\n",
        "Threshold of pruning 0.005855459347367287\n",
        "number of neruons being masked in layer 3: 363\n",
        "Threshold of pruning 0.007697033137083054\n",
        "number of neruons being masked in layer 4: 499\n",
        "Threshold of pruning 0.008342868089675904\n",
        "number of neruons being masked in layer 5: 493\n",
        "Threshold of pruning 0.009570317715406418\n",
        "number of neruons being masked in layer 6: 586\n",
        "Threshold of pruning 0.012565302848815919\n",
        "number of neruons being masked in layer 7: 710\n",
        "Threshold of pruning 0.014758637547492981\n",
        "number of neruons being masked in layer 8: 770\n",
        "Threshold of pruning 0.013258574903011322\n",
        "number of neruons being masked in layer 9: 666\n",
        "Threshold of pruning 0.012264932692050935\n",
        "number of neruons being masked in layer 10: 370\n",
        "Threshold of pruning 0.014829948544502258\n",
        "number of neruons being masked in layer 11: 1816\n",
        "\n",
        "6841\n",
        "\n",
        "alpha 0.125 -- 0.818, 20.54  (0.816,24.08) (layer 11)\n",
        "Threshold of pruning -0.019106443971395493\n",
        "number of neruons being masked in layer 0: 0\n",
        "Threshold of pruning 0.004982198588550091\n",
        "number of neruons being masked in layer 1: 296\n",
        "Threshold of pruning 0.005195078905671835\n",
        "number of neruons being masked in layer 2: 339\n",
        "Threshold of pruning 0.007319324184209108\n",
        "number of neruons being masked in layer 3: 442\n",
        "Threshold of pruning 0.009621291421353817\n",
        "number of neruons being masked in layer 4: 616\n",
        "Threshold of pruning 0.01042858511209488\n",
        "number of neruons being masked in layer 5: 617\n",
        "Threshold of pruning 0.011962897144258022\n",
        "number of neruons being masked in layer 6: 727\n",
        "Threshold of pruning 0.015706628561019897\n",
        "number of neruons being masked in layer 7: 891\n",
        "Threshold of pruning 0.018448296934366226\n",
        "number of neruons being masked in layer 8: 986\n",
        "Threshold of pruning 0.016573218628764153\n",
        "number of neruons being masked in layer 9: 822\n",
        "Threshold of pruning 0.015331165865063667\n",
        "number of neruons being masked in layer 10: 457\n",
        "Threshold of pruning 0.018537435680627823\n",
        "number of neruons being masked in layer 11: 1987\n",
        "\n",
        "7980\n",
        "\n",
        "alpha 0.15 --  0.817 20.52 (0.816,24.08) (layer 11)\n",
        "Threshold of pruning -0.02292773276567459\n",
        "number of neruons being masked in layer 0: 0\n",
        "Threshold of pruning 0.005978638306260109\n",
        "number of neruons being masked in layer 1: 353\n",
        "Threshold of pruning 0.006234094686806202\n",
        "number of neruons being masked in layer 2: 404\n",
        "Threshold of pruning 0.00878318902105093\n",
        "number of neruons being masked in layer 3: 532\n",
        "Threshold of pruning 0.01154554970562458\n",
        "number of neruons being masked in layer 4: 724\n",
        "Threshold of pruning 0.012514302134513855\n",
        "number of neruons being masked in layer 5: 725\n",
        "Threshold of pruning 0.014355476573109626\n",
        "number of neruons being masked in layer 6: 868\n",
        "Threshold of pruning 0.018847954273223878\n",
        "number of neruons being masked in layer 7: 1063\n",
        "Threshold of pruning 0.02213795632123947\n",
        "number of neruons being masked in layer 8: 1176\n",
        "Threshold of pruning 0.01988786235451698\n",
        "number of neruons being masked in layer 9: 970\n",
        "Threshold of pruning 0.0183973990380764\n",
        "number of neruons being masked in layer 10: 550\n",
        "Threshold of pruning 0.022244922816753387\n",
        "number of neruons being masked in layer 11: 2123\n",
        "\n",
        "9588\n",
        "\n",
        "alpha 0.15 --  0.819 20.52 (0.816,24.08) (layer 10,11)\n",
        "Threshold of pruning -0.02292773276567459\n",
        "number of neruons being masked in layer 0: 0\n",
        "Threshold of pruning 0.005978638306260109\n",
        "number of neruons being masked in layer 1: 353\n",
        "Threshold of pruning 0.006234094686806202\n",
        "number of neruons being masked in layer 2: 404\n",
        "Threshold of pruning 0.00878318902105093\n",
        "number of neruons being masked in layer 3: 532\n",
        "Threshold of pruning 0.01154554970562458\n",
        "number of neruons being masked in layer 4: 724\n",
        "Threshold of pruning 0.012514302134513855\n",
        "number of neruons being masked in layer 5: 725\n",
        "Threshold of pruning 0.014355476573109626\n",
        "number of neruons being masked in layer 6: 868\n",
        "Threshold of pruning 0.018847954273223878\n",
        "number of neruons being masked in layer 7: 1063\n",
        "Threshold of pruning 0.02213795632123947\n",
        "number of neruons being masked in layer 8: 1176\n",
        "Threshold of pruning 0.01988786235451698\n",
        "number of neruons being masked in layer 9: 970\n",
        "Threshold of pruning 0.0183973990380764\n",
        "number of neruons being masked in layer 10: 2388\n",
        "Threshold of pruning 0.022244922816753387\n",
        "number of neruons being masked in layer 11: 2123\n",
        "\n",
        "11596\n",
        "\n",
        "\n",
        "alpha 0.15 --  0.828  20.53 (0.816,24.08) (layer 9,10,11)\n",
        "Threshold of pruning -0.02292773276567459\n",
        "number of neruons being masked in layer 0: 0\n",
        "Threshold of pruning 0.005978638306260109\n",
        "number of neruons being masked in layer 1: 353\n",
        "Threshold of pruning 0.006234094686806202\n",
        "number of neruons being masked in layer 2: 404\n",
        "Threshold of pruning 0.00878318902105093\n",
        "number of neruons being masked in layer 3: 532\n",
        "Threshold of pruning 0.01154554970562458\n",
        "number of neruons being masked in layer 4: 724\n",
        "Threshold of pruning 0.012514302134513855\n",
        "number of neruons being masked in layer 5: 725\n",
        "Threshold of pruning 0.014355476573109626\n",
        "number of neruons being masked in layer 6: 868\n",
        "Threshold of pruning 0.018847954273223878\n",
        "number of neruons being masked in layer 7: 1063\n",
        "Threshold of pruning 0.02213795632123947\n",
        "number of neruons being masked in layer 8: 1176\n",
        "Threshold of pruning 0.01988786235451698\n",
        "number of neruons being masked in layer 9: 2723\n",
        "Threshold of pruning 0.0183973990380764\n",
        "number of neruons being masked in layer 10: 2388\n",
        "Threshold of pruning 0.022244922816753387\n",
        "number of neruons being masked in layer 11: 2123\n",
        "\n",
        "12384\n",
        "\n",
        "alpha 0.15 --  0.822  10.45/10.6 (0.816,24.08) (layer 8,9,10,11)\n",
        "Threshold of pruning -0.02292773276567459\n",
        "number of neruons being masked in layer 0: 0\n",
        "Threshold of pruning 0.005978638306260109\n",
        "number of neruons being masked in layer 1: 353\n",
        "Threshold of pruning 0.006234094686806202\n",
        "number of neruons being masked in layer 2: 404\n",
        "Threshold of pruning 0.00878318902105093\n",
        "number of neruons being masked in layer 3: 532\n",
        "Threshold of pruning 0.01154554970562458\n",
        "number of neruons being masked in layer 4: 724\n",
        "Threshold of pruning 0.012514302134513855\n",
        "number of neruons being masked in layer 5: 725\n",
        "Threshold of pruning 0.014355476573109626\n",
        "number of neruons being masked in layer 6: 868\n",
        "Threshold of pruning 0.018847954273223878\n",
        "number of neruons being masked in layer 7: 1063\n",
        "Threshold of pruning 0.02213795632123947\n",
        "number of neruons being masked in layer 8: 2858\n",
        "Threshold of pruning 0.01988786235451698\n",
        "number of neruons being masked in layer 9: 2723\n",
        "Threshold of pruning 0.0183973990380764\n",
        "number of neruons being masked in layer 10: 2388\n",
        "Threshold of pruning 0.022244922816753387\n",
        "number of neruons being masked in layer 11: 2123\n",
        "\n",
        "15641\n",
        "\n",
        "alpha 0.15 --  0.768  20.53 (0.816,24.08) (layer 7,8,9,10,11)\n",
        "Threshold of pruning -0.02292773276567459\n",
        "number of neruons being masked in layer 0: 0\n",
        "Threshold of pruning 0.005978638306260109\n",
        "number of neruons being masked in layer 1: 353\n",
        "Threshold of pruning 0.006234094686806202\n",
        "number of neruons being masked in layer 2: 404\n",
        "Threshold of pruning 0.00878318902105093\n",
        "number of neruons being masked in layer 3: 532\n",
        "Threshold of pruning 0.01154554970562458\n",
        "number of neruons being masked in layer 4: 724\n",
        "Threshold of pruning 0.012514302134513855\n",
        "number of neruons being masked in layer 5: 725\n",
        "Threshold of pruning 0.014355476573109626\n",
        "number of neruons being masked in layer 6: 868\n",
        "Threshold of pruning 0.018847954273223878\n",
        "number of neruons being masked in layer 7: 2791\n",
        "Threshold of pruning 0.02213795632123947\n",
        "number of neruons being masked in layer 8: 2858\n",
        "Threshold of pruning 0.01988786235451698\n",
        "number of neruons being masked in layer 9: 2723\n",
        "Threshold of pruning 0.0183973990380764\n",
        "number of neruons being masked in layer 10: 2388\n",
        "Threshold of pruning 0.022244922816753387\n",
        "number of neruons being masked in layer 11: 2123\n",
        "\n",
        "18469\n",
        "\n",
        "alpha 0.15 --  0.768 20.52 (0.816,24.08) (layer 6,7,8,9,10,11)\n",
        "Threshold of pruning -0.02292773276567459\n",
        "number of neruons being masked in layer 0: 0\n",
        "Threshold of pruning 0.005978638306260109\n",
        "number of neruons being masked in layer 1: 353\n",
        "Threshold of pruning 0.006234094686806202\n",
        "number of neruons being masked in layer 2: 404\n",
        "Threshold of pruning 0.00878318902105093\n",
        "number of neruons being masked in layer 3: 532\n",
        "Threshold of pruning 0.01154554970562458\n",
        "number of neruons being masked in layer 4: 724\n",
        "Threshold of pruning 0.012514302134513855\n",
        "number of neruons being masked in layer 5: 725\n",
        "Threshold of pruning 0.014355476573109626\n",
        "number of neruons being masked in layer 6: 2614\n",
        "Threshold of pruning 0.018847954273223878\n",
        "number of neruons being masked in layer 7: 2791\n",
        "Threshold of pruning 0.02213795632123947\n",
        "number of neruons being masked in layer 8: 2858\n",
        "Threshold of pruning 0.01988786235451698\n",
        "number of neruons being masked in layer 9: 2723\n",
        "Threshold of pruning 0.0183973990380764\n",
        "number of neruons being masked in layer 10: 2388\n",
        "Threshold of pruning 0.022244922816753387\n",
        "number of neruons being masked in layer 11: 2123\n",
        "\n",
        "23045\n",
        "\n",
        "alpha 0.15 --  0.72 20.63 (0.816,24.08) (layer 5,6,7,8,9,10,11)Threshold of pruning -0.02292773276567459\n",
        "number of neruons being masked in layer 0: 0\n",
        "Threshold of pruning 0.005978638306260109\n",
        "number of neruons being masked in layer 1: 353\n",
        "Threshold of pruning 0.006234094686806202\n",
        "number of neruons being masked in layer 2: 404\n",
        "Threshold of pruning 0.00878318902105093\n",
        "number of neruons being masked in layer 3: 532\n",
        "Threshold of pruning 0.01154554970562458\n",
        "number of neruons being masked in layer 4: 724\n",
        "Threshold of pruning 0.012514302134513855\n",
        "number of neruons being masked in layer 5: 2537\n",
        "Threshold of pruning 0.014355476573109626\n",
        "number of neruons being masked in layer 6: 2614\n",
        "Threshold of pruning 0.018847954273223878\n",
        "number of neruons being masked in layer 7: 2791\n",
        "Threshold of pruning 0.02213795632123947\n",
        "number of neruons being masked in layer 8: 2858\n",
        "Threshold of pruning 0.01988786235451698\n",
        "number of neruons being masked in layer 9: 2723\n",
        "Threshold of pruning 0.0183973990380764\n",
        "number of neruons being masked in layer 10: 2388\n",
        "Threshold of pruning 0.022244922816753387\n",
        "number of neruons being masked in layer 11: 2123\n",
        "\n",
        "23625\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "8I9_fP1A6uDq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5000ffc-9adf-4c3e-c331-ed5ad799a049"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "alpha 0.05 -- Total Masked Neurons: 0, Neurons per Layer: []\n",
            "alpha 0.075 -- Total Masked Neurons: 0, Neurons per Layer: []\n",
            "alpha 0.1 -- Total Masked Neurons: 0, Neurons per Layer: []\n",
            "alpha 0.125 -- Total Masked Neurons: 0, Neurons per Layer: []\n",
            "alpha 0.15 -- Total Masked Neurons: 0, Neurons per Layer: []\n",
            "alpha 0.15 -- Total Masked Neurons: 0, Neurons per Layer: []\n",
            "alpha 0.15 -- Total Masked Neurons: 0, Neurons per Layer: []\n",
            "alpha 0.15 -- Total Masked Neurons: 0, Neurons per Layer: []\n",
            "alpha 0.15 -- Total Masked Neurons: 0, Neurons per Layer: []\n",
            "alpha 0.15 -- Total Masked Neurons: 0, Neurons per Layer: []\n",
            "alpha 0.15 -- Total Masked Neurons: 0, Neurons per Layer: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#alpha 0.05  -- 0.817, 20.53 (0.816,24.08) (layer 9,10,11)\n",
        "# Threshold of pruning -0.008282925188541413\n",
        "# number of neruons being masked in layer 0: 0\n",
        "# Threshold of pruning 0.0014311897568404675\n",
        "# number of neruons being masked in layer 1: 98\n",
        "# Threshold of pruning 0.0013770012184977532\n",
        "# number of neruons being masked in layer 2: 95\n",
        "# Threshold of pruning 0.0018456179648637773\n",
        "# number of neruons being masked in layer 3: 98\n",
        "# Threshold of pruning 0.002240518108010292\n",
        "# number of neruons being masked in layer 4: 148\n",
        "# Threshold of pruning 0.0018868006765842438\n",
        "# number of neruons being masked in layer 5: 129\n",
        "# Threshold of pruning 0.0020474625751376154\n",
        "# number of neruons being masked in layer 6: 125\n",
        "# Threshold of pruning 0.002420696243643761\n",
        "# number of neruons being masked in layer 7: 126\n",
        "# Threshold of pruning 0.003420105576515198\n",
        "# number of neruons being masked in layer 8: 185\n",
        "# Threshold of pruning 0.004050713777542114\n",
        "# number of neruons being masked in layer 9: 1860\n",
        "# Threshold of pruning 0.005295529216527939\n",
        "# number of neruons being masked in layer 10: 1988\n",
        "# Threshold of pruning 0.008389303833246231\n",
        "# number of neruons being masked in layer 11: 1484\n",
        "\n",
        "# 6186\n",
        "\n",
        "#alpha 0.075 (0.816,24.08) (layer 9,10,11)\n",
        "# Threshold of pruning -0.016565850377082827\n",
        "# number of neruons being masked in layer 0: 0\n",
        "# Threshold of pruning 0.002862379513680935\n",
        "# number of neruons being masked in layer 1: 166\n",
        "# Threshold of pruning 0.0027540024369955064\n",
        "# number of neruons being masked in layer 2: 171\n",
        "# Threshold of pruning 0.0036912359297275547\n",
        "# number of neruons being masked in layer 3: 221\n",
        "# Threshold of pruning 0.004481036216020584\n",
        "# number of neruons being masked in layer 4: 303\n",
        "# Threshold of pruning 0.0037736013531684876\n",
        "# number of neruons being masked in layer 5: 262\n",
        "# Threshold of pruning 0.004094925150275231\n",
        "# number of neruons being masked in layer 6: 254\n",
        "# Threshold of pruning 0.004841392487287522\n",
        "# number of neruons being masked in layer 7: 279\n",
        "# Threshold of pruning 0.006840211153030396\n",
        "# number of neruons being masked in layer 8: 385\n",
        "# Threshold of pruning 0.008101427555084228\n",
        "# number of neruons being masked in layer 9: 2130\n",
        "# Threshold of pruning 0.010591058433055878\n",
        "# number of neruons being masked in layer 10: 2164\n",
        "# Threshold of pruning 0.016778607666492463\n",
        "# number of neruons being masked in layer 11: 1913\n",
        "\n",
        "#alpha 0.1 (0.816,24.08) (layer 9,10,11)\n",
        "# Threshold of pruning -0.012424387782812117\n",
        "# number of neruons being masked in layer 0: 0\n",
        "# Threshold of pruning 0.002146784635260701\n",
        "# number of neruons being masked in layer 1: 131\n",
        "# Threshold of pruning 0.0020655018277466295\n",
        "# number of neruons being masked in layer 2: 137\n",
        "# Threshold of pruning 0.0027684269472956656\n",
        "# number of neruons being masked in layer 3: 162\n",
        "# Threshold of pruning 0.003360777162015438\n",
        "# number of neruons being masked in layer 4: 230\n",
        "# Threshold of pruning 0.0028302010148763654\n",
        "# number of neruons being masked in layer 5: 186\n",
        "# Threshold of pruning 0.0030711938627064226\n",
        "# number of neruons being masked in layer 6: 192\n",
        "# Threshold of pruning 0.0036310443654656408\n",
        "# number of neruons being masked in layer 7: 195\n",
        "# Threshold of pruning 0.005130158364772796\n",
        "# number of neruons being masked in layer 8: 270\n",
        "# Threshold of pruning 0.0060760706663131716\n",
        "# number of neruons being masked in layer 9: 2007\n",
        "# Threshold of pruning 0.007943293824791907\n",
        "# number of neruons being masked in layer 10: 2069\n",
        "# Threshold of pruning 0.012583955749869346\n",
        "# number of neruons being masked in layer 11: 1724\n",
        "\n",
        "6748\n",
        "\n",
        "#alpha 0.125 (0.816,24.08) (layer 9,10,11)\n",
        "# Threshold of pruning -0.02070731297135353\n",
        "# number of neruons being masked in layer 0: 0\n",
        "# Threshold of pruning 0.0035779743921011686\n",
        "# number of neruons being masked in layer 1: 206\n",
        "# Threshold of pruning 0.003442503046244383\n",
        "# number of neruons being masked in layer 2: 219\n",
        "# Threshold of pruning 0.004614044912159443\n",
        "# number of neruons being masked in layer 3: 284\n",
        "# Threshold of pruning 0.00560129527002573\n",
        "# number of neruons being masked in layer 4: 367\n",
        "# Threshold of pruning 0.0047170016914606094\n",
        "# number of neruons being masked in layer 5: 326\n",
        "# Threshold of pruning 0.005118656437844038\n",
        "# number of neruons being masked in layer 6: 309\n",
        "# Threshold of pruning 0.006051740609109402\n",
        "# number of neruons being masked in layer 7: 349\n",
        "# Threshold of pruning 0.008550263941287994\n",
        "# number of neruons being masked in layer 8: 470\n",
        "# Threshold of pruning 0.010126784443855286\n",
        "# number of neruons being masked in layer 9: 2250\n",
        "# Threshold of pruning 0.013238823041319847\n",
        "# number of neruons being masked in layer 10: 2260\n",
        "\n",
        "6843\n",
        "\n",
        "#alpha 0.15 (0.816,24.08) (layer 9,10,11)\n",
        "# Threshold of pruning -0.024848775565624235\n",
        "# number of neruons being masked in layer 0: 0\n",
        "# Threshold of pruning 0.004293569270521402\n",
        "# number of neruons being masked in layer 1: 250\n",
        "# Threshold of pruning 0.004131003655493259\n",
        "# number of neruons being masked in layer 2: 264\n",
        "# Threshold of pruning 0.005536853894591331\n",
        "# number of neruons being masked in layer 3: 338\n",
        "# Threshold of pruning 0.006721554324030876\n",
        "# number of neruons being masked in layer 4: 429\n",
        "# Threshold of pruning 0.005660402029752731\n",
        "# number of neruons being masked in layer 5: 387\n",
        "# Threshold of pruning 0.006142387725412845\n",
        "# number of neruons being masked in layer 6: 371\n",
        "# Threshold of pruning 0.0072620887309312815\n",
        "# number of neruons being masked in layer 7: 409\n",
        "# Threshold of pruning 0.010260316729545593\n",
        "# number of neruons being masked in layer 8: 581\n",
        "# Threshold of pruning 0.012152141332626343\n",
        "# number of neruons being masked in layer 9: 2379\n",
        "# Threshold of pruning 0.015886587649583814\n",
        "# number of neruons being masked in layer 10: 2330\n",
        "# Threshold of pruning 0.025167911499738693\n",
        "# number of neruons being masked in layer 11: 2255\n",
        "\n",
        "7030\n",
        "\n",
        "#alpha 0.15 --   (0.816,24.08) (layer 8, 9,10,11)\n",
        "# Threshold of pruning -0.024848775565624235\n",
        "# number of neruons being masked in layer 0: 0\n",
        "# Threshold of pruning 0.004293569270521402\n",
        "# number of neruons being masked in layer 1: 250\n",
        "# Threshold of pruning 0.004131003655493259\n",
        "# number of neruons being masked in layer 2: 264\n",
        "# Threshold of pruning 0.005536853894591331\n",
        "# number of neruons being masked in layer 3: 338\n",
        "# Threshold of pruning 0.006721554324030876\n",
        "# number of neruons being masked in layer 4: 429\n",
        "# Threshold of pruning 0.005660402029752731\n",
        "# number of neruons being masked in layer 5: 387\n",
        "# Threshold of pruning 0.006142387725412845\n",
        "# number of neruons being masked in layer 6: 371\n",
        "# Threshold of pruning 0.0072620887309312815\n",
        "# number of neruons being masked in layer 7: 409\n",
        "# Threshold of pruning 0.010260316729545593\n",
        "# number of neruons being masked in layer 8: 2334\n",
        "# Threshold of pruning 0.012152141332626343\n",
        "# number of neruons being masked in layer 9: 2379\n",
        "# Threshold of pruning 0.015886587649583814\n",
        "# number of neruons being masked in layer 10: 2330\n",
        "# Threshold of pruning 0.025167911499738693\n",
        "# number of neruons being masked in layer 11: 2255\n",
        "\n",
        "11126\n",
        "\n",
        "\n",
        "#alpha 0.15 --   (0.816,24.08) (layer 7,8,9,10,11)\n",
        "# Threshold of pruning -0.024848775565624235\n",
        "# number of neruons being masked in layer 0: 0\n",
        "# Threshold of pruning 0.004293569270521402\n",
        "# number of neruons being masked in layer 1: 250\n",
        "# Threshold of pruning 0.004131003655493259\n",
        "# number of neruons being masked in layer 2: 264\n",
        "# Threshold of pruning 0.005536853894591331\n",
        "# number of neruons being masked in layer 3: 338\n",
        "# Threshold of pruning 0.006721554324030876\n",
        "# number of neruons being masked in layer 4: 429\n",
        "# Threshold of pruning 0.005660402029752731\n",
        "# number of neruons being masked in layer 5: 387\n",
        "# Threshold of pruning 0.006142387725412845\n",
        "# number of neruons being masked in layer 6: 371\n",
        "# Threshold of pruning 0.0072620887309312815\n",
        "# number of neruons being masked in layer 7: 2133\n",
        "# Threshold of pruning 0.010260316729545593\n",
        "# number of neruons being masked in layer 8: 2334\n",
        "# Threshold of pruning 0.012152141332626343\n",
        "# number of neruons being masked in layer 9: 2379\n",
        "# Threshold of pruning 0.015886587649583814\n",
        "# number of neruons being masked in layer 10: 2330\n",
        "# Threshold of pruning 0.025167911499738693\n",
        "# number of neruons being masked in layer 11: 2255\n",
        "\n",
        "14570\n",
        "\n",
        "#alpha 0.15 --   (0.816,24.08)(layer 6,7,8,9,10,11)\n",
        "# Threshold of pruning -0.024848775565624235\n",
        "# number of neruons being masked in layer 0: 0\n",
        "# Threshold of pruning 0.004293569270521402\n",
        "# number of neruons being masked in layer 1: 250\n",
        "# Threshold of pruning 0.004131003655493259\n",
        "# number of neruons being masked in layer 2: 264\n",
        "# Threshold of pruning 0.005536853894591331\n",
        "# number of neruons being masked in layer 3: 338\n",
        "# Threshold of pruning 0.006721554324030876\n",
        "# number of neruons being masked in layer 4: 429\n",
        "# Threshold of pruning 0.005660402029752731\n",
        "# number of neruons being masked in layer 5: 387\n",
        "# Threshold of pruning 0.006142387725412845\n",
        "# number of neruons being masked in layer 6: 2022\n",
        "# Threshold of pruning 0.0072620887309312815\n",
        "# number of neruons being masked in layer 7: 2133\n",
        "# Threshold of pruning 0.010260316729545593\n",
        "# number of neruons being masked in layer 8: 2334\n",
        "# Threshold of pruning 0.012152141332626343\n",
        "# number of neruons being masked in layer 9: 2379\n",
        "# Threshold of pruning 0.015886587649583814\n",
        "# number of neruons being masked in layer 10: 2330\n",
        "# Threshold of pruning 0.025167911499738693\n",
        "# number of neruons being masked in layer 11: 2255\n",
        "\n",
        "15061\n",
        "\n",
        "#layer5~11\n",
        "# Threshold of pruning -0.024848775565624235\n",
        "# number of neruons being masked in layer 0: 0\n",
        "# Threshold of pruning 0.004293569270521402\n",
        "# number of neruons being masked in layer 1: 250\n",
        "# Threshold of pruning 0.004131003655493259\n",
        "# number of neruons being masked in layer 2: 264\n",
        "# Threshold of pruning 0.005536853894591331\n",
        "# number of neruons being masked in layer 3: 338\n",
        "# Threshold of pruning 0.006721554324030876\n",
        "# number of neruons being masked in layer 4: 429\n",
        "# Threshold of pruning 0.005660402029752731\n",
        "# number of neruons being masked in layer 5: 2031\n",
        "# Threshold of pruning 0.006142387725412845\n",
        "# number of neruons being masked in layer 6: 2022\n",
        "# Threshold of pruning 0.0072620887309312815\n",
        "# number of neruons being masked in layer 7: 2133\n",
        "# Threshold of pruning 0.010260316729545593\n",
        "# number of neruons being masked in layer 8: 2334\n",
        "# Threshold of pruning 0.012152141332626343\n",
        "# number of neruons being masked in layer 9: 2379\n",
        "# Threshold of pruning 0.015886587649583814\n",
        "# number of neruons being masked in layer 10: 2330\n",
        "# Threshold of pruning 0.025167911499738693\n",
        "# number of neruons being masked in layer 11: 2255\n",
        "\n",
        "20475\n",
        "\n",
        "#layer4~11\n",
        "# Threshold of pruning -0.024848775565624235\n",
        "# number of neruons being masked in layer 0: 0\n",
        "# Threshold of pruning 0.004293569270521402\n",
        "# number of neruons being masked in layer 1: 250\n",
        "# Threshold of pruning 0.004131003655493259\n",
        "# number of neruons being masked in layer 2: 264\n",
        "# Threshold of pruning 0.005536853894591331\n",
        "# number of neruons being masked in layer 3: 338\n",
        "# Threshold of pruning 0.006721554324030876\n",
        "# number of neruons being masked in layer 4: 2154\n",
        "# Threshold of pruning 0.005660402029752731\n",
        "# number of neruons being masked in layer 5: 2031\n",
        "# Threshold of pruning 0.006142387725412845\n",
        "# number of neruons being masked in layer 6: 2022\n",
        "# Threshold of pruning 0.0072620887309312815\n",
        "# number of neruons being masked in layer 7: 2133\n",
        "# Threshold of pruning 0.010260316729545593\n",
        "# number of neruons being masked in layer 8: 2334\n",
        "# Threshold of pruning 0.012152141332626343\n",
        "# number of neruons being masked in layer 9: 2379\n",
        "# Threshold of pruning 0.015886587649583814\n",
        "# number of neruons being masked in layer 10: 2330\n",
        "# Threshold of pruning 0.025167911499738693\n",
        "# number of neruons being masked in layer 11: 2255\n",
        "\n",
        "20470\n",
        "\n",
        "# layer4~11\n",
        "# Threshold of pruning -0.024848775565624235\n",
        "# number of neruons being masked in layer 0: 0\n",
        "# Threshold of pruning 0.004293569270521402\n",
        "# number of neruons being masked in layer 1: 250\n",
        "# Threshold of pruning 0.004131003655493259\n",
        "# number of neruons being masked in layer 2: 264\n",
        "# Threshold of pruning 0.005536853894591331\n",
        "# number of neruons being masked in layer 3: 338\n",
        "# Threshold of pruning 0.006721554324030876\n",
        "# number of neruons being masked in layer 4: 2154\n",
        "# Threshold of pruning 0.005660402029752731\n",
        "# number of neruons being masked in layer 5: 2031\n",
        "# Threshold of pruning 0.006142387725412845\n",
        "# number of neruons being masked in layer 6: 2022\n",
        "# Threshold of pruning 0.0072620887309312815\n",
        "# number of neruons being masked in layer 7: 2133\n",
        "# Threshold of pruning 0.010260316729545593\n",
        "# number of neruons being masked in layer 8: 2334\n",
        "# Threshold of pruning 0.012152141332626343\n",
        "# number of neruons being masked in layer 9: 2379\n",
        "# Threshold of pruning 0.015886587649583814\n",
        "# number of neruons being masked in layer 10: 2330\n",
        "# Threshold of pruning 0.025167911499738693\n",
        "# number of neruons being masked in layer 11: 2255\n",
        "\n",
        "20470\n",
        "\n",
        "#layer 3~11\n",
        "# Threshold of pruning -0.024848775565624235\n",
        "# number of neruons being masked in layer 0: 0\n",
        "# Threshold of pruning 0.004293569270521402\n",
        "# number of neruons being masked in layer 1: 250\n",
        "# Threshold of pruning 0.004131003655493259\n",
        "# number of neruons being masked in layer 2: 264\n",
        "# Threshold of pruning 0.005536853894591331\n",
        "# number of neruons being masked in layer 3: 2017\n",
        "# Threshold of pruning 0.006721554324030876\n",
        "# number of neruons being masked in layer 4: 2154\n",
        "# Threshold of pruning 0.005660402029752731\n",
        "# number of neruons being masked in layer 5: 2031\n",
        "# Threshold of pruning 0.006142387725412845\n",
        "# number of neruons being masked in layer 6: 2022\n",
        "# Threshold of pruning 0.0072620887309312815\n",
        "# number of neruons being masked in layer 7: 2133\n",
        "# Threshold of pruning 0.010260316729545593\n",
        "# number of neruons being masked in layer 8: 2334\n",
        "# Threshold of pruning 0.012152141332626343\n",
        "# number of neruons being masked in layer 9: 2379\n",
        "# Threshold of pruning 0.015886587649583814\n",
        "# number of neruons being masked in layer 10: 2330\n",
        "# Threshold of pruning 0.025167911499738693\n",
        "# number of neruons being masked in layer 11: 2255\n",
        "\n",
        "23569"
      ],
      "metadata": {
        "id": "UFahrD_Pfuly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# alpha = 0.01\n",
        "# if simi < cosine_similarities_layer[0][layer]*alpha:\n",
        "# Threshold of pruning -0.0010250626504421235\n",
        "# number of neruons being masked in layer 0: 1494\n",
        "# Threshold of pruning 0.0007526144385337829\n",
        "# number of neruons being masked in layer 1: 1489\n",
        "# Threshold of pruning 0.000886850655078888\n",
        "# number of neruons being masked in layer 2: 1497\n",
        "# Threshold of pruning 0.0010219812393188477\n",
        "# number of neruons being masked in layer 3: 1522\n",
        "# Threshold of pruning 0.0009761258214712143\n",
        "# number of neruons being masked in layer 4: 1544\n",
        "# Threshold of pruning 0.0010622094571590423\n",
        "# number of neruons being masked in layer 5: 1572\n",
        "# Threshold of pruning 0.0011643679440021515\n",
        "# number of neruons being masked in layer 6: 1543\n",
        "# Threshold of pruning 0.0013090051710605622\n",
        "# number of neruons being masked in layer 7: 1548\n",
        "# Threshold of pruning 0.0013178519904613495\n",
        "# number of neruons being masked in layer 8: 1585\n",
        "# Threshold of pruning 0.0015621431171894075\n",
        "# number of neruons being masked in layer 9: 1577\n",
        "# Threshold of pruning 0.0015955017507076263\n",
        "# number of neruons being masked in layer 10: 1724\n",
        "# Threshold of pruning 0.0015955017507076263\n",
        "# number of neruons being masked in layer 11: 1118\n",
        "\n",
        "\n",
        "From here!\n",
        "\n",
        "#if cosine_similarities_layer[5][layer]*alpha > simi > -cosine_similarities_layer[5][layer]*alpha:\n",
        "#if simi < 3*cosine_similarities_layer[5][layer]*alpha:\n",
        "#alpha =0.05\n",
        "# Threshold of pruning -0.003942283987998963\n",
        "# number of neruons being masked in layer 0: 0\n",
        "# Threshold of pruning 0.004036996513605118\n",
        "# number of neruons being masked in layer 1: 226\n",
        "# Threshold of pruning 0.004419980198144913\n",
        "# number of neruons being masked in layer 2: 269\n",
        "# Threshold of pruning 0.004932549968361855\n",
        "# number of neruons being masked in layer 3: 299\n",
        "# Threshold of pruning 0.005454594641923905\n",
        "# number of neruons being masked in layer 4: 316\n",
        "# Threshold of pruning 0.005172299966216088\n",
        "# number of neruons being masked in layer 5: 308\n",
        "# Threshold of pruning 0.005783993378281594\n",
        "# number of neruons being masked in layer 6: 339\n",
        "# Threshold of pruning 0.0068449854850769045\n",
        "# number of neruons being masked in layer 7: 387\n",
        "# Threshold of pruning 0.008186922967433929\n",
        "# number of neruons being masked in layer 8: 436\n",
        "# Threshold of pruning 0.009629475325345993\n",
        "# number of neruons being masked in layer 9: 553\n",
        "# Threshold of pruning 0.009398249536752702\n",
        "# number of neruons being masked in layer 10: 306\n",
        "# Threshold of pruning 0.009398249536752702\n",
        "# number of neruons being masked in layer 11: 1601\n",
        "\n",
        "5640\n",
        "\n",
        "#alpha = 0.07\n",
        "# Threshold of pruning -0.005519197583198548\n",
        "# number of neruons being masked in layer 0: 0\n",
        "# Threshold of pruning 0.005651795119047166\n",
        "# number of neruons being masked in layer 1: 307\n",
        "# Threshold of pruning 0.006187972277402878\n",
        "# number of neruons being masked in layer 2: 391\n",
        "# Threshold of pruning 0.006905569955706597\n",
        "# number of neruons being masked in layer 3: 419\n",
        "# Threshold of pruning 0.007636432498693467\n",
        "# number of neruons being masked in layer 4: 455\n",
        "# Threshold of pruning 0.007241219952702523\n",
        "# number of neruons being masked in layer 5: 445\n",
        "# Threshold of pruning 0.008097590729594232\n",
        "# number of neruons being masked in layer 6: 464\n",
        "# Threshold of pruning 0.009582979679107666\n",
        "# number of neruons being masked in layer 7: 537\n",
        "# Threshold of pruning 0.011461692154407503\n",
        "# number of neruons being masked in layer 8: 651\n",
        "# Threshold of pruning 0.01348126545548439\n",
        "# number of neruons being masked in layer 9: 763\n",
        "# Threshold of pruning 0.013157549351453782\n",
        "# number of neruons being masked in layer 10: 433\n",
        "# Threshold of pruning 0.013157549351453782\n",
        "# number of neruons being masked in layer 11: 1815\n",
        "\n",
        "5965\n",
        "\n",
        "#alpha = 0.1\n",
        "# Threshold of pruning -0.007884567975997925\n",
        "# number of neruons being masked in layer 0: 0\n",
        "# Threshold of pruning 0.008073993027210236\n",
        "# number of neruons being masked in layer 1: 437\n",
        "# Threshold of pruning 0.008839960396289826\n",
        "# number of neruons being masked in layer 2: 532\n",
        "# Threshold of pruning 0.00986509993672371\n",
        "# number of neruons being masked in layer 3: 602\n",
        "# Threshold of pruning 0.01090918928384781\n",
        "# number of neruons being masked in layer 4: 655\n",
        "# Threshold of pruning 0.010344599932432176\n",
        "# number of neruons being masked in layer 5: 610\n",
        "# Threshold of pruning 0.011567986756563188\n",
        "# number of neruons being masked in layer 6: 659\n",
        "# Threshold of pruning 0.013689970970153809\n",
        "# number of neruons being masked in layer 7: 768\n",
        "# Threshold of pruning 0.016373845934867858\n",
        "# number of neruons being masked in layer 8: 908\n",
        "# Threshold of pruning 0.019258950650691987\n",
        "# number of neruons being masked in layer 9: 1049\n",
        "# Threshold of pruning 0.018796499073505404\n",
        "# number of neruons being masked in layer 10: 614\n",
        "# Threshold of pruning 0.018796499073505404\n",
        "# number of neruons being masked in layer 11: 2108\n",
        "\n",
        "7554\n",
        "\n",
        "#0.125\n",
        "# Threshold of pruning -0.009855709969997406\n",
        "# number of neruons being masked in layer 0: 0\n",
        "# Threshold of pruning 0.010092491284012794\n",
        "# number of neruons being masked in layer 1: 563\n",
        "# Threshold of pruning 0.011049950495362282\n",
        "# number of neruons being masked in layer 2: 679\n",
        "# Threshold of pruning 0.012331374920904636\n",
        "# number of neruons being masked in layer 3: 719\n",
        "# Threshold of pruning 0.013636486604809761\n",
        "# number of neruons being masked in layer 4: 821\n",
        "# Threshold of pruning 0.012930749915540218\n",
        "# number of neruons being masked in layer 5: 761\n",
        "# Threshold of pruning 0.014459983445703983\n",
        "# number of neruons being masked in layer 6: 844\n",
        "# Threshold of pruning 0.01711246371269226\n",
        "# number of neruons being masked in layer 7: 968\n",
        "# Threshold of pruning 0.020467307418584824\n",
        "# number of neruons being masked in layer 8: 1105\n",
        "# Threshold of pruning 0.024073688313364983\n",
        "# number of neruons being masked in layer 9: 1259\n",
        "# Threshold of pruning 0.023495623841881752\n",
        "# number of neruons being masked in layer 10: 755\n",
        "# Threshold of pruning 0.023495623841881752\n",
        "# number of neruons being masked in layer 11: 2303\n",
        "\n",
        "9907\n",
        "\n",
        "#0.15\n",
        "# Threshold of pruning -0.011826851963996887\n",
        "# number of neruons being masked in layer 0: 0\n",
        "# Threshold of pruning 0.012110989540815353\n",
        "# number of neruons being masked in layer 1: 680\n",
        "# Threshold of pruning 0.013259940594434737\n",
        "# number of neruons being masked in layer 2: 806\n",
        "# Threshold of pruning 0.014797649905085563\n",
        "# number of neruons being masked in layer 3: 835\n",
        "# Threshold of pruning 0.016363783925771713\n",
        "# number of neruons being masked in layer 4: 1002\n",
        "# Threshold of pruning 0.01551689989864826\n",
        "# number of neruons being masked in layer 5: 905\n",
        "# Threshold of pruning 0.01735198013484478\n",
        "# number of neruons being masked in layer 6: 976\n",
        "# Threshold of pruning 0.02053495645523071\n",
        "# number of neruons being masked in layer 7: 1152\n",
        "# Threshold of pruning 0.02456076890230179\n",
        "# number of neruons being masked in layer 8: 1307\n",
        "# Threshold of pruning 0.02888842597603798\n",
        "# number of neruons being masked in layer 9: 1439\n",
        "# Threshold of pruning 0.0281947486102581\n",
        "# number of neruons being masked in layer 10: 908\n",
        "# Threshold of pruning 0.0281947486102581\n",
        "# number of neruons being masked in layer 11: 2448\n",
        "\n",
        "11668\n",
        "\n",
        "#0.15 but with layer 10, 11\n",
        "# Threshold of pruning -0.011826851963996887\n",
        "# number of neruons being masked in layer 0: 0\n",
        "# Threshold of pruning 0.012110989540815353\n",
        "# number of neruons being masked in layer 1: 680\n",
        "# Threshold of pruning 0.013259940594434737\n",
        "# number of neruons being masked in layer 2: 806\n",
        "# Threshold of pruning 0.014797649905085563\n",
        "# number of neruons being masked in layer 3: 835\n",
        "# Threshold of pruning 0.016363783925771713\n",
        "# number of neruons being masked in layer 4: 1002\n",
        "# Threshold of pruning 0.01551689989864826\n",
        "# number of neruons being masked in layer 5: 905\n",
        "# Threshold of pruning 0.01735198013484478\n",
        "# number of neruons being masked in layer 6: 976\n",
        "# Threshold of pruning 0.02053495645523071\n",
        "# number of neruons being masked in layer 7: 1152\n",
        "# Threshold of pruning 0.02456076890230179\n",
        "# number of neruons being masked in layer 8: 1307\n",
        "# Threshold of pruning 0.02888842597603798\n",
        "# number of neruons being masked in layer 9: 1439\n",
        "# Threshold of pruning 0.0281947486102581\n",
        "# number of neruons being masked in layer 10: 2681\n",
        "# Threshold of pruning 0.0297170989215374\n",
        "# number of neruons being masked in layer 11: 2491\n",
        "\n",
        "15224\n",
        "\n",
        "#0.15 layer 9,10,11\n",
        "# Threshold of pruning -0.011826851963996887\n",
        "# number of neruons being masked in layer 0: 0\n",
        "# Threshold of pruning 0.012110989540815353\n",
        "# number of neruons being masked in layer 1: 680\n",
        "# Threshold of pruning 0.013259940594434737\n",
        "# number of neruons being masked in layer 2: 806\n",
        "# Threshold of pruning 0.014797649905085563\n",
        "# number of neruons being masked in layer 3: 835\n",
        "# Threshold of pruning 0.016363783925771713\n",
        "# number of neruons being masked in layer 4: 1002\n",
        "# Threshold of pruning 0.01551689989864826\n",
        "# number of neruons being masked in layer 5: 905\n",
        "# Threshold of pruning 0.01735198013484478\n",
        "# number of neruons being masked in layer 6: 976\n",
        "# Threshold of pruning 0.02053495645523071\n",
        "# number of neruons being masked in layer 7: 1152\n",
        "# Threshold of pruning 0.02456076890230179\n",
        "# number of neruons being masked in layer 8: 1307\n",
        "# Threshold of pruning 0.02888842597603798\n",
        "# number of neruons being masked in layer 9: 2958\n",
        "# Threshold of pruning 0.0281947486102581\n",
        "# number of neruons being masked in layer 10: 2681\n",
        "# Threshold of pruning 0.0297170989215374\n",
        "# number of neruons being masked in layer 11: 2491\n",
        "\n",
        "13074\n",
        "\n",
        "#0.15 layer8,9,10,11\n",
        "# Threshold of pruning -0.011826851963996887\n",
        "# number of neruons being masked in layer 0: 0\n",
        "# Threshold of pruning 0.012110989540815353\n",
        "# number of neruons being masked in layer 1: 680\n",
        "# Threshold of pruning 0.013259940594434737\n",
        "# number of neruons being masked in layer 2: 806\n",
        "# Threshold of pruning 0.014797649905085563\n",
        "# number of neruons being masked in layer 3: 835\n",
        "# Threshold of pruning 0.016363783925771713\n",
        "# number of neruons being masked in layer 4: 1002\n",
        "# Threshold of pruning 0.01551689989864826\n",
        "# number of neruons being masked in layer 5: 905\n",
        "# Threshold of pruning 0.01735198013484478\n",
        "# number of neruons being masked in layer 6: 976\n",
        "# Threshold of pruning 0.02053495645523071\n",
        "# number of neruons being masked in layer 7: 1152\n",
        "# Threshold of pruning 0.02456076890230179\n",
        "# number of neruons being masked in layer 8: 2921\n",
        "# Threshold of pruning 0.02888842597603798\n",
        "# number of neruons being masked in layer 9: 2958\n",
        "# Threshold of pruning 0.0281947486102581\n",
        "# number of neruons being masked in layer 10: 2681\n",
        "# Threshold of pruning 0.0297170989215374\n",
        "# number of neruons being masked in layer 11: 2491\n",
        "\n",
        "17607\n",
        "\n",
        "#alpha 0.15, 7,8,9,10,11\n",
        "# Threshold of pruning -0.011826851963996887\n",
        "# number of neruons being masked in layer 0: 0\n",
        "# Threshold of pruning 0.012110989540815353\n",
        "# number of neruons being masked in layer 1: 680\n",
        "# Threshold of pruning 0.013259940594434737\n",
        "# number of neruons being masked in layer 2: 806\n",
        "# Threshold of pruning 0.014797649905085563\n",
        "# number of neruons being masked in layer 3: 835\n",
        "# Threshold of pruning 0.016363783925771713\n",
        "# number of neruons being masked in layer 4: 1002\n",
        "# Threshold of pruning 0.01551689989864826\n",
        "# number of neruons being masked in layer 5: 905\n",
        "# Threshold of pruning 0.01735198013484478\n",
        "# number of neruons being masked in layer 6: 976\n",
        "# Threshold of pruning 0.02053495645523071\n",
        "# number of neruons being masked in layer 7: 2859\n",
        "# Threshold of pruning 0.02456076890230179\n",
        "# number of neruons being masked in layer 8: 2921\n",
        "# Threshold of pruning 0.02888842597603798\n",
        "# number of neruons being masked in layer 9: 2958\n",
        "# Threshold of pruning 0.0281947486102581\n",
        "# number of neruons being masked in layer 10: 2681\n",
        "# Threshold of pruning 0.0297170989215374\n",
        "# number of neruons being masked in layer 11: 2491\n",
        "\n",
        "24388\n",
        "\n",
        "#0.15 layer 6~11\n",
        "# Threshold of pruning -0.011826851963996887\n",
        "# number of neruons being masked in layer 0: 0\n",
        "# Threshold of pruning 0.012110989540815353\n",
        "# number of neruons being masked in layer 1: 680\n",
        "# Threshold of pruning 0.013259940594434737\n",
        "# number of neruons being masked in layer 2: 806\n",
        "# Threshold of pruning 0.014797649905085563\n",
        "# number of neruons being masked in layer 3: 835\n",
        "# Threshold of pruning 0.016363783925771713\n",
        "# number of neruons being masked in layer 4: 1002\n",
        "# Threshold of pruning 0.01551689989864826\n",
        "# number of neruons being masked in layer 5: 905\n",
        "# Threshold of pruning 0.01735198013484478\n",
        "# number of neruons being masked in layer 6: 2730\n",
        "# Threshold of pruning 0.02053495645523071\n",
        "# number of neruons being masked in layer 7: 2859\n",
        "# Threshold of pruning 0.02456076890230179\n",
        "# number of neruons being masked in layer 8: 2921\n",
        "# Threshold of pruning 0.02888842597603798\n",
        "# number of neruons being masked in layer 9: 2958\n",
        "# Threshold of pruning 0.0281947486102581\n",
        "# number of neruons being masked in layer 10: 2681\n",
        "# Threshold of pruning 0.0297170989215374\n",
        "# number of neruons being masked in layer 11: 2491\n",
        "\n",
        "23715\n",
        "\n",
        "#layer 9,10,11 alpha = 0.1\n",
        "# Threshold of pruning -0.013572309911251069\n",
        "# number of neruons being masked in layer 0: 0\n",
        "# Threshold of pruning 0.00938405990600586\n",
        "# number of neruons being masked in layer 1: 519\n",
        "# Threshold of pruning 0.009955600649118424\n",
        "# number of neruons being masked in layer 2: 603\n",
        "# Threshold of pruning 0.011347671598196031\n",
        "# number of neruons being masked in layer 3: 667\n",
        "# Threshold of pruning 0.013960830867290497\n",
        "# number of neruons being masked in layer 4: 842\n",
        "# Threshold of pruning 0.011994074285030366\n",
        "# number of neruons being masked in layer 5: 715\n",
        "# Threshold of pruning 0.01488049626350403\n",
        "# number of neruons being masked in layer 6: 860\n",
        "# Threshold of pruning 0.01956499218940735\n",
        "# number of neruons being masked in layer 7: 1104\n",
        "# Threshold of pruning 0.02259863466024399\n",
        "# number of neruons being masked in layer 8: 1208\n",
        "# Threshold of pruning 0.021597903966903687\n",
        "# number of neruons being masked in layer 9: 2812\n",
        "# Threshold of pruning 0.02362041473388672\n",
        "# number of neruons being masked in layer 10: 2584\n",
        "# Threshold of pruning 0.028286200761795045\n",
        "# number of neruons being masked in layer 11: 2451\n",
        "\n",
        "#layer 9,10,11 alpha = 0.125\n",
        "# Threshold of pruning -0.016965387389063835\n",
        "# number of neruons being masked in layer 0: 0\n",
        "# Threshold of pruning 0.011730074882507324\n",
        "# number of neruons being masked in layer 1: 664\n",
        "# Threshold of pruning 0.01244450081139803\n",
        "# number of neruons being masked in layer 2: 752\n",
        "# Threshold of pruning 0.014184589497745037\n",
        "# number of neruons being masked in layer 3: 803\n",
        "# Threshold of pruning 0.01745103858411312\n",
        "# number of neruons being masked in layer 4: 1067\n",
        "# Threshold of pruning 0.014992592856287956\n",
        "# number of neruons being masked in layer 5: 879\n",
        "# Threshold of pruning 0.018600620329380035\n",
        "# number of neruons being masked in layer 6: 1029\n",
        "# Threshold of pruning 0.024456240236759186\n",
        "# number of neruons being masked in layer 7: 1329\n",
        "# Threshold of pruning 0.028248293325304985\n",
        "# number of neruons being masked in layer 8: 1489\n",
        "# Threshold of pruning 0.026997379958629608\n",
        "# number of neruons being masked in layer 9: 2932\n",
        "# Threshold of pruning 0.0295255184173584\n",
        "# number of neruons being masked in layer 10: 2712\n",
        "# Threshold of pruning 0.035357750952243805\n",
        "# number of neruons being masked in layer 11: 2642\n",
        "\n",
        "#layer 9,10,11 alpha = 0.15\n",
        "# Threshold of pruning -0.0203584648668766\n",
        "# number of neruons being masked in layer 0: 0\n",
        "# Threshold of pruning 0.014076089859008788\n",
        "# number of neruons being masked in layer 1: 780\n",
        "# Threshold of pruning 0.014933400973677634\n",
        "# number of neruons being masked in layer 2: 891\n",
        "# Threshold of pruning 0.017021507397294043\n",
        "# number of neruons being masked in layer 3: 951\n",
        "# Threshold of pruning 0.020941246300935745\n",
        "# number of neruons being masked in layer 4: 1259\n",
        "# Threshold of pruning 0.017991111427545548\n",
        "# number of neruons being masked in layer 5: 1035\n",
        "# Threshold of pruning 0.022320744395256043\n",
        "# number of neruons being masked in layer 6: 1239\n",
        "# Threshold of pruning 0.02934748828411102\n",
        "# number of neruons being masked in layer 7: 1566\n",
        "# Threshold of pruning 0.03389795199036598\n",
        "# number of neruons being masked in layer 8: 1718\n",
        "# Threshold of pruning 0.032396855950355526\n",
        "# number of neruons being masked in layer 9: 3003\n",
        "# Threshold of pruning 0.035430622100830075\n",
        "# number of neruons being masked in layer 10: 2800\n",
        "# Threshold of pruning 0.04242930114269256\n",
        "# number of neruons being masked in layer 11: 2761\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "u-fKYAvWwFoz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}